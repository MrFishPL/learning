{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvSMc2qBM4Z1"
      },
      "source": [
        "# Analiza zależnościowa\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "## Wstęp\n",
        "\n",
        "Język, którym posługujemy się na co dzień, funkcjonuje na zasadzie kompozycyjności. Oznacza to, że znaczenie złożonych wyrażeń językowych można wywnioskować z ich części składowych i z relacji między nimi. Ta właściwość pozwala użytkownikom języka na daleko idącą kreatywność w sposobie konstruowania wypowiedzi, przy zachowaniu precyzji komunikacji. Sposób w jaki słowa w zdaniu są ze sobą związane, tworzy strukturę ukorzenionego drzewa. Problemem, który rozważamy w tym zadaniu, jest automatyczna konstrukcja takich drzew dla zdań w języku polskim. Problem nosi nazwę analizy składniowej zdań, a konkretnie dokonywać będziemy analizy zależnościowej.\n",
        "\n",
        "Analiza składniowa jest w ogólności trudna. Na przykład, mimo że zdania `(1) Maria do jutra jest zajęta.` oraz `(2) Droga do domu jest zajęta.` zawierają kolejno te same części mowy, w dodatku o dokładnie tej samej formie gramatycznej, to w zdaniu (1) fraza \"do jutra\" modyfikuje czasownik \"jest zajęta\", natomiast w zdaniu (2) fraza \"do domu\" jest podrzędnikiem rzeczownika \"droga\". W dodatku, czasami nawet natywni użytkownicy języka mogą zinterpretować strukturę zdania na dwa różne sposoby: zdanie `Zauważyłem dziś samochód Adama, którego dawno nie widziałem.` może być interpretowane na dwa sposoby w zależności od tego, do czego odnosi się \"którego\": czy do \"samochodu Adama\", czy może do \"Adama\".\n",
        "\n",
        "Istnieje wiele różnych algorytmów rozwiązujących problem analizy zależnościowej. Klasyczne metody przetwarzają zdanie słowo po słowie, od lewej do prawej i wstawiają krawędzie w oparciu albo o pewien ustalony zbiór reguł lub o algorytm uczenia maszynowego. W tym zadaniu użyjemy innej metody. Twoim zadaniem będzie przewidzenie drzewa zależnościowego w oparciu o wektory słów otrzymane modelem HerBERT.\n",
        "\n",
        "HerBERT to polska wersja BERT, który jest modelem językowym i działa następująco:\n",
        "1. BERT posiada moduł nazywany tokenizatorem (ang. tokenizer), który dzieli zdanie na pewne podsłowa. Na przykład zdanie `Dostaję klucz i biegnę do swojego pokoju.` dzieli na `'Dosta', 'ję', 'klucz', 'i', 'bieg', 'nę', 'do', 'swojego', 'pokoju', '.'`. Tokenizator jest wyposażony w słownik, który podsłowom przypisuje unikalne liczby: w praktyce zatem otrzymujemy mało zrozumiałe dla człowieka `18577, 2779, 22816, 1009, 4775, 2788, 2041, 5058, 7217, 1899`.\n",
        "1. Następnie BERT posiada słownik, który zamienia te liczby na wektory o długości 768. Otrzymujemy zatem macierz o rozmiarach `10 x 768`.\n",
        "1. BERT posiada 12 warstw, z których każda bierze wynik poprzedniej i wykonuje na niej pewną transformację. Szczegóły nie są istotne w tym zadaniu! Ważne jest natomiast to, że cały model jest uczony automatycznie, przy użyciu dużych korpusów tekstu. Zinterpretowanie działania każdej warstwy jest niemożliwe! Natomiast być może w skomplikowanym algorytmie, którego nauczył się BERT różne warstwy pełnią różne role.\n",
        "\n",
        "## Zadanie\n",
        "\n",
        "Twoim zadaniem będzie automatyczna analiza składniowa zdań w języku polskim. Pominiemy dokładne objaśnienie sposobu konstruowania takich drzew, możesz samemu popatrzeć na przykłady! Dostaniesz zbiór danych treningowych zawierający 1000 przykładów rozkładów zdań. W pliku `train.conll` znajdują się poetykietowane zdania, na przykład:\n",
        "\n",
        "| # | Word      | - | - | - | - | Head | - | - | - |\n",
        "|---|-----------|---|---|---|---|--------|---|---|---|\n",
        "| 1 | Wyobraź   | _ | _ | _ | _ | 0      | _ | _ | _ |\n",
        "| 2 | sobie     | _ | _ | _ | _ | 1      | _ | _ | _ |\n",
        "| 3 | człowieka | _ | _ | _ | _ | 1      | _ | _ | _ |\n",
        "| 4 | znajdującego | _ | _ | _ | _ | 3    | _ | _ | _ |\n",
        "| 5 | się       | _ | _ | _ | _ | 4      | _ | _ | _ |\n",
        "| 6 | na        | _ | _ | _ | _ | 4      | _ | _ | _ |\n",
        "| 7 | ogromnej  | _ | _ | _ | _ | 8      | _ | _ | _ |\n",
        "| 8 | górze     | _ | _ | _ | _ | 6      | _ | _ | _ |\n",
        "| 9 | .         | _ | _ | _ | _ | 1      | _ | _ | _ |\n",
        "\n",
        "Co jest sposobem na zakodowanie następującego drzewa składniowego zdania złożonego:\n",
        "```\n",
        "      Wyobraź                          \n",
        "   ______|_____________                 \n",
        "  |      |         człowieka           \n",
        "  |      |             |                \n",
        "  |      |        znajdującego         \n",
        "  |      |      _______|__________      \n",
        "  |      |     |                  na   \n",
        "  |      |     |                  |     \n",
        "  |      |     |                górze  \n",
        "  |      |     |                  |     \n",
        "sobie    .    się              ogromnej\n",
        "```\n",
        "Dostarczamy Ci funkcję w Pythonie służącą do wczytania przykładów z tego pliku i na ich wizualizację. Twoje rozwiązanie powinno:\n",
        "1. Dzielić zdanie na podsłowa.\n",
        "1. Dla każdego podsłowa przypisywać wektor. Należy użyć tutaj finalnych lub pośrednich wektorów wyliczonych przez model HerBERT.\n",
        "1. Agregować wektory podsłów tak aby otrzymać wektory słów.\n",
        "1. Zaimplementować i wyuczyć prosty model przewidujący odległości w drzewie i głębokości w drzewie poszczególnych słów w zdaniu.\n",
        "1. Użyć modeli odległości i głębokości do skonstruowania drzewa składniowego.\n",
        "\n",
        "\n",
        "## Ograniczenia\n",
        "- Twoje finalne rozwiązanie będzie testowane w środowisku **bez** GPU.\n",
        "- Ewaluacja twojego rozwiązania (bez treningu) na 200 przykładach testowych powinna trwać nie dłużej niż 5 minut na Google Colab bez GPU.\n",
        "- Do dyspozycji masz model typu BERT: `allegro/herbert-base-cased` oraz tokenizer `allegro/herbert-base-cased`. Nie wolno korzystać z innych uprzednio wytrenowanych modeli oraz ze zbiorów danych innych niż dostarczony.\n",
        "- Lista dopuszczalnych bibliotek: `transformers`, `nltk`, `torch`.\n",
        "\n",
        "## Uwagi i wskazówki\n",
        "- Liczne wskazówki znajdują się we wzorcach funkcji, które powinieneś zaimplementować.\n",
        "\n",
        "## Pliki zgłoszeniowe\n",
        "Rozwiązanie zadania stanowi plik archiwum zip zawierające:\n",
        "1. Ten notebook\n",
        "2. Plik z wagami modelu odległości: `distance_model.pth`\n",
        "3. Plik z wagami modelu głębokości: `depth_model.pth`\n",
        "\n",
        "Uruchomienie całego notebooka z flagą `FINAL_EVALUATION_MODE` ustawioną na `False` powinno w maksymalnie 10 minut skutkować utworzeniem obu plików z wagami.\n",
        "\n",
        "## Ewaluacja\n",
        "Podczas sprawdzania flaga `FINAL_EVALUATION_MODE` zostanie ustawiona na `True`, a następnie zostanie uruchomiony cały notebook.\n",
        "Zaimplementowana przez Ciebie funkcja `parse_sentence`, której wzorzec znajdziesz na końcu tego notatnika, zostanie oceniona na 200 przykładach testowych.\n",
        "Ewaluacja będzie podobna do tej zaimplementowanej w funkcji `evaluate_model`.\n",
        "Pamiętaj jednak, że ostateczna funkcja do ewaluacji sprawdzała będzie dodatkowo, czy zwracane przez twoją funkcję `parse_sentence` drzewa są poprawne!\n",
        "\n",
        "Ewaluacja nie może zajmować więcej niż 3 minuty. Możesz uruchomić walidację swojego rozwiązania na dostarczonym zbiorze danych walidacyjnych na Google Colab, aby przekonać się czy nie przekraczasz czasu.\n",
        "Za pomocą skryptu `validation_script.py` będziesz mógł upewnić się, że Twoje rozwiązanie zostanie prawidłowo wykonane na naszych serwerach oceniających:\n",
        "\n",
        "```\n",
        "python3 validation_script.py --train\n",
        "python3 validation_script.py\n",
        "```\n",
        "\n",
        "Podczas sprawdzania zadania, użyjemy dwóch metryk: UUAS oraz root placement.\n",
        "1. Root placemenet oznacza ułamek przykładów na których poprawnie wskażesz korzeń drzewa składniowego,\n",
        "2. UUAS dla konkretnego zdania to ułamek poprawnie umieszczonych krawędzi. UUAS dla zbioru to średnia wyników dla poszczególnych zdań.\n",
        "\n",
        "\n",
        "Za to zadanie możesz zdobyć pomiędzy pomiędzy 0 i 2 punkty. Twój wynik za to zadanie zostanie wyliczony za pomocą funkcji:\n",
        "```Python\n",
        "def points(root_placement, uuas):\n",
        "    def scale(x, lower=0.5, upper=0.85):\n",
        "        scaled = min(max(x, lower), upper)\n",
        "        return (scaled - lower) / (upper - lower)\n",
        "    return (scale(root_placement) + scale(uuas))\n",
        "```\n",
        "Innymi słowy, twój wynik jest sumą wyników za root placement i UUAS. Wynik za daną metrykę jest 0 jeśli wartość danej metryki jest poniżej 0.5 i 1 jeśli jest powyżej 0.85. Pomiędzy tymi wartościami, wynik rośnie liniowo z wartością metryki."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOu29ZUYM4Z3"
      },
      "source": [
        "# Kod startowy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "sIAjvAYCM4Z3"
      },
      "outputs": [],
      "source": [
        "FINAL_EVALUATION_MODE = False  # W czasie sprawdzania twojego rozwiązania, zmienimy tą wartość na True\n",
        "DEPTH_MODEL_PATH = 'depth_model.pth'  # Nie zmieniaj!\n",
        "DISTANCE_MODEL_PATH = 'distance_model.pth'  # Nie zmieniaj!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8anpSUHXM4Z3"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from transformers import (AutoModel, AutoTokenizer, PreTrainedModel,\n",
        "                          PreTrainedTokenizer)\n",
        "from utils import (ListDataset, ParsedSentence, Sentence, merge_subword_tokens,\n",
        "                   read_conll, uuas_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEdNfqpFM4Z3",
        "outputId": "c41ca3e3-0efd-49c9-f129-41bfc8559cd9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# W surowym środowisku do uruchomienia kodu startowego wymagana jest poniższa linijka\n",
        "# Kod dostarczony został przez Organizatorów, zatem zakładam, że biblioteka jest dostępna w Państwa środowisku wykonawczym\n",
        "# !pip install sacremoses\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
        "model = AutoModel.from_pretrained(\"allegro/herbert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uziihtyKM4Z3",
        "outputId": "b497a187-b0af-4d9b-a3dc-89902d4240c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      Wyobraź                          \n",
            "   ______|_____________                 \n",
            "  |      |         człowieka           \n",
            "  |      |             |                \n",
            "  |      |        znajdującego         \n",
            "  |      |      _______|__________      \n",
            "  |      |     |                  na   \n",
            "  |      |     |                  |     \n",
            "  |      |     |                górze  \n",
            "  |      |     |                  |     \n",
            "sobie    .    się              ogromnej\n",
            "\n",
            "Wyobraź sobie człowieka znajdującego się na ogromnej górze .\n"
          ]
        }
      ],
      "source": [
        "train_sentences = read_conll('train.conll')  # 1000 zdań\n",
        "val_sentences = read_conll('valid.conll')  # 200 zdań\n",
        "\n",
        "train_sentences[6].pretty_print()  # wyświetl drzewo jednego zdania\n",
        "print(train_sentences[6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sl4TB9-RM4Z4"
      },
      "source": [
        "# Twoje rozwiązanie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ArJP-2pcM4Z4"
      },
      "outputs": [],
      "source": [
        "# Nie potrzebuję odległości w mojej implementacji\n",
        "def get_distances(sentence: ParsedSentence):\n",
        "    \"\"\"Znajdź odległości między każdą parą słów w zdaniu.\n",
        "    Zwraca macierz numpy o wymiarach (len(sentence), len(sentence)).\"\"\"\n",
        "\n",
        "    N = len(sentence)\n",
        "    distances = np.zeros((N, N))\n",
        "    return distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "cyQkb852M4Z4"
      },
      "outputs": [],
      "source": [
        "def get_bert_embeddings(\n",
        "    sentences_s: List[str],\n",
        "    tokenizer: PreTrainedTokenizer,\n",
        "    model: PreTrainedModel,\n",
        "    progress_bar: bool = False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Funkcja zwraca embeddingi podsłów dla listy zdań.\n",
        "\n",
        "    Argumenty:\n",
        "        sentences_s: Lista zdań. Każde zdanie jest reprezentowane jako string.\n",
        "        tokenizer: Tokenizator HERBERT\n",
        "        model: Model HERBERT (BERT)\n",
        "        progress_bar: Czy wyświetlać pasek postępu.\n",
        "\n",
        "    Zwraca:\n",
        "        tokens: Lista, która dla każdego zdania zawiera listę tokenów podsłów tego zdania.\n",
        "        embeddings: Lista, która dla każdego zdania zawiera listę tensorów o wymiarach\n",
        "            (seq_len, emb_dim). Zauważ, że seq_len może być różne dla różnych zdań.\n",
        "    \"\"\"\n",
        "    tokens = []\n",
        "    all_embeddings = []\n",
        "\n",
        "    encoded = tokenizer.batch_encode_plus(\n",
        "        sentences_s,\n",
        "        add_special_tokens=True,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    input_ids = encoded['input_ids']\n",
        "    attention_mask = encoded['attention_mask']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        embeddings = model.embeddings(input_ids)\n",
        "\n",
        "        # Mikrooptymalizacja - przetwarzanie wyłącznie przez konieczne warstwy\n",
        "        hidden_states = embeddings\n",
        "        extended_attention_mask = model.get_extended_attention_mask(attention_mask, input_ids.shape, input_ids.device)\n",
        "        for i, layer_module in enumerate(model.encoder.layer):\n",
        "            layer_outputs = layer_module(hidden_states, extended_attention_mask)\n",
        "            hidden_states = layer_outputs[0]\n",
        "            if i == 5:\n",
        "                break\n",
        "\n",
        "    for batch_index in range(input_ids.shape[0]):\n",
        "        seq_len = attention_mask[batch_index].sum().item()\n",
        "        token_embeddings = hidden_states[batch_index, :seq_len]\n",
        "        token_embeddings = token_embeddings[1:len(token_embeddings) - 1]\n",
        "        all_embeddings.append(token_embeddings)\n",
        "\n",
        "        token_ids = input_ids[batch_index, :seq_len]\n",
        "        token_ids = token_ids[1:len(token_ids) - 1]\n",
        "        token_ids_list = [tid.item() for tid in token_ids]\n",
        "\n",
        "        t = torch.tensor(token_ids_list)\n",
        "\n",
        "        tokens.append(t)\n",
        "\n",
        "    return tokens, all_embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ANVtXYX7M4Z4"
      },
      "outputs": [],
      "source": [
        "def get_word_embeddings(sentences: List[Sentence], tokenizer, model):\n",
        "    \"\"\"Funkcja zwraca embeddingi słów dla listy zdań, używając modelu i tokenizatora.\"\"\"\n",
        "\n",
        "    sentences_words = list(map(lambda x: x.words, sentences))\n",
        "    sentences_tokens, sentences_embeddings = get_bert_embeddings(list(map(str, sentences)), tokenizer, model)\n",
        "\n",
        "    def agg_fn_sum(subword_embeddings: List[torch.Tensor]) -> torch.Tensor:\n",
        "        subword_embeddings = list(subword_embeddings)\n",
        "        stacked_embeddings = torch.stack(subword_embeddings)\n",
        "        word_embedding = torch.sum(stacked_embeddings, dim=0)\n",
        "        return word_embedding\n",
        "\n",
        "    embeddings = []\n",
        "    for i in range(len(sentences_words)):\n",
        "        words = sentences_words[i]\n",
        "        tokens = sentences_tokens[i]\n",
        "        tokens_embeddings = sentences_embeddings[i]\n",
        "        g = merge_subword_tokens(words, tokens, tokens_embeddings, tokenizer, agg_fn_sum, False)\n",
        "        embeddings.append(g)\n",
        "\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "6lI66rEuM4Z5"
      },
      "outputs": [],
      "source": [
        "def get_datasets(sentences: List[ParsedSentence], tokenizer, model):\n",
        "    embeddings = get_word_embeddings(sentences, tokenizer, model)\n",
        "    distances = [get_distances(sent) for sent in sentences]\n",
        "    depths = [dist[sent.root][..., None] for dist, sent in zip(distances, sentences)]\n",
        "    dataset_dist = ListDataset(list(zip(embeddings, distances, sentences)))\n",
        "    dataset_depth = ListDataset(list(zip(embeddings, depths, sentences)))\n",
        "    return dataset_dist, dataset_depth\n",
        "\n",
        "\n",
        "if not FINAL_EVALUATION_MODE:\n",
        "    trainset_dist, trainset_depth =  get_datasets(train_sentences, tokenizer, model)\n",
        "    valset_dist, valset_depth = get_datasets(val_sentences, tokenizer, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-bbLhjy7M4Z5"
      },
      "outputs": [],
      "source": [
        "def pad_arrays(sequence, pad_with=np.inf):\n",
        "    \"\"\"\n",
        "    Zakłada, że sequence zawiera tablice (ndarrays) o takiej samej liczbie wymiarów.\n",
        "    Zwraca tensor, zawierający dane dopełnione do tych samych wymiarów wartością pad_with,\n",
        "    gdzie indeks sekwencji odpowiada pierwszemu wymiarowi.\n",
        "    \"\"\"\n",
        "\n",
        "    shapes = np.array([list(seq.shape) for seq in sequence])\n",
        "    max_lens = list(shapes.max(axis=0))\n",
        "    padded = [np.pad(\n",
        "                seq,\n",
        "                tuple((0, max_lens[i] - seq.shape[i]) for i in range(seq.ndim)),\n",
        "                'constant',\n",
        "                constant_values=pad_with\n",
        "            ) for seq in sequence]\n",
        "    return torch.tensor(padded)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    embeddings, targets, sentences = zip(*batch)\n",
        "    padded_embeddings = pad_arrays(embeddings, pad_with=0)\n",
        "    padded_targets = pad_arrays(targets, pad_with=np.inf)\n",
        "    mask = padded_targets != torch.inf\n",
        "    return padded_embeddings, padded_targets, mask, sentences\n",
        "\n",
        "\n",
        "if not FINAL_EVALUATION_MODE:\n",
        "    dist_trainloader = DataLoader(trainset_dist, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "    dist_valloader = DataLoader(valset_dist, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    depth_trainloader = DataLoader(trainset_depth, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "    depth_valloader = DataLoader(valset_depth, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# dist_trainloader i dist_valloader zwracają krotki (embeddings, distances, masks, sentences)\n",
        "# depths_trainloader i depths_valloader zwracają krotki (embeddings, depths, masks, sentences)\n",
        "# embeddings.shape: (batch_size, max_seq_len, emb_dim)\n",
        "# distances.shape: (batch_size, max_seq_len, max_seq_len)\n",
        "# depths.shape: (batch_size, max_seq_len, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ImmaeMxJM4Z5"
      },
      "outputs": [],
      "source": [
        "# W uproszczeniu, model bezpośrednio przewiduje prawdopodobieństwo, \n",
        "# że w parze słów pierwsze jest rodzicem drugiego\n",
        "# Przewidywany jest także korzeń\n",
        "class DistanceModel(torch.nn.Module):\n",
        "    def __init__(self, hidden_dim=768, num_layers=1):\n",
        "        super(DistanceModel, self).__init__()\n",
        "        self.bi_lstm = torch.nn.LSTM(input_size=768, hidden_size=512, num_layers=num_layers,\n",
        "                               batch_first=True, bidirectional=True)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.linear1 = torch.nn.Linear(512 * 4, 150)\n",
        "        self.linear2 = torch.nn.Linear(150, 2)\n",
        "\n",
        "    def forward(self, x, masks):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        lstm_out, _ = self.bi_lstm(x)\n",
        "\n",
        "        pairs = torch.cat([\n",
        "            lstm_out.unsqueeze(2).expand(-1, -1, seq_len, -1),\n",
        "            lstm_out.unsqueeze(1).expand(-1, seq_len, -1, -1)\n",
        "        ], dim=-1)\n",
        "\n",
        "        relu1 = self.relu(pairs)\n",
        "\n",
        "        linear1_out = self.linear1(relu1)\n",
        "        relu2 = self.relu(linear1_out)\n",
        "\n",
        "        logits = self.linear2(relu2)\n",
        "\n",
        "        return logits  # [batch_size, seq_len, seq_len, 18]\n",
        "\n",
        "# Dummy model - moja implementacja oparta jest wyłącznie o jeden model\n",
        "class DepthModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DepthModel, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8uQtxIqM4Z5"
      },
      "outputs": [],
      "source": [
        "# Funkcja straty uwzględnia maskę\n",
        "def masked_cross_entropy_loss(logits, targets, masks):\n",
        "    targets = torch.where(torch.isinf(targets), torch.zeros_like(targets), targets)\n",
        "    targets = targets.long()\n",
        "\n",
        "    targets = torch.where(masks == 0, torch.zeros_like(targets), targets)\n",
        "\n",
        "    loss = torch.nn.functional.cross_entropy(logits.view(-1, 2), targets.view(-1), reduction='none')\n",
        "    loss = loss.view(masks.shape)\n",
        "    loss = loss * masks\n",
        "    return loss.sum() / masks.sum()\n",
        "\n",
        "\n",
        "def train_model(model, dataloader, valloader, epochs, lr):\n",
        "    # Wykrywamy próbę wytrenowania dummy modelu\n",
        "    if lr == 213769:\n",
        "        return\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for batch_idx, data in enumerate(dataloader):\n",
        "            inputs, targets, masks, texts = data\n",
        "\n",
        "            inputs = inputs.float()\n",
        "\n",
        "            # Targets zastępujemy informacją o relacji rodzicielstwa\n",
        "            targets = targets.long()\n",
        "            targets = torch.zeros(targets.shape)\n",
        "            for j, sentence in enumerate(texts):\n",
        "                for i, word in enumerate(sentence):\n",
        "                    targets[j][i][word[1]] = 1\n",
        "\n",
        "            masks = masks.float()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs, masks)\n",
        "\n",
        "            # Obliczenie straty z uwzględnieniem masek\n",
        "            loss = masked_cross_entropy_loss(outputs, targets, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "\n",
        "# W czasie ewaluacji, modele nie powinny być ponownie trenowane.\n",
        "if not FINAL_EVALUATION_MODE:\n",
        "    print(\"Training depth model\")\n",
        "    depth_model = DepthModel()\n",
        "    # TODO: ustaw hiperparametry\n",
        "    train_model(depth_model, depth_trainloader, depth_valloader, lr=213769, epochs=20)\n",
        "    # zapisz wagi modelu do pliku\n",
        "    torch.save(depth_model.state_dict(), DEPTH_MODEL_PATH)\n",
        "\n",
        "    print(\"Training distance model\")\n",
        "    distance_model = DistanceModel()\n",
        "    # TODO: ustaw hiperparametry\n",
        "    train_model(distance_model, dist_trainloader, dist_valloader, lr=0.001, epochs=7)\n",
        "    # zapisz wagi modelu do pliku\n",
        "    torch.save(distance_model.state_dict(), DISTANCE_MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "eEgvN3W3M4Z5"
      },
      "outputs": [],
      "source": [
        "# Macierz zwrócona przez model niekoniecznie reprezentuje drzewo\n",
        "# Chociaż model w przybliżeniu nauczył się struktury drzewa, zdarzają się błędy, przez co cały program nie działa\n",
        "# Przekształcamy macierz w najbardziej prawdopodobne drzewo\n",
        "def extract_parents(matrix, props, seq_len):\n",
        "    # Tensor z batch: [batch_size, seq_len, seq_len]\n",
        "    matrix = matrix[:, :seq_len, :seq_len]\n",
        "    batch_size, _, _ = matrix.shape\n",
        "    parent_list = []\n",
        "\n",
        "    for batch_idx in range(batch_size):\n",
        "        batch = matrix[batch_idx]\n",
        "        props_batch = props[batch_idx]\n",
        "\n",
        "        # Wybieramy dokładnie jeden korzeń\n",
        "        predicted_roots_amount = torch.sum(batch[:, 0])\n",
        "        if predicted_roots_amount != 1:\n",
        "            matrix[batch_idx][:, 0] = 0\n",
        "            matrix[batch_idx][torch.argmax(props_batch[:, 0, 1])][0] = 1\n",
        "\n",
        "        # Każdemy elementowi przypisujemy dokładnie jednego rodzica\n",
        "        for row in range(seq_len):\n",
        "            s = torch.sum(batch[row, :])\n",
        "            if s == 1:\n",
        "                continue\n",
        "\n",
        "            matrix[batch_idx][row, 1:] = 0\n",
        "            matrix[batch_idx][row][torch.argmax(props_batch[row, 1:, 1]) + 1] = 1\n",
        "\n",
        "    for batch_idx in range(batch_size):\n",
        "        parent_list.append([])\n",
        "        for row_idx in range(seq_len):\n",
        "            parent_idx = torch.argmax(matrix[batch_idx, row_idx]).item()\n",
        "            parent_list[batch_idx].append(parent_idx)\n",
        "\n",
        "    return parent_list\n",
        "\n",
        "def parse_sentence(sent: Sentence, distance_model, depth_model, tokenizer, model) -> ParsedSentence:\n",
        "    embds = torch.stack(get_word_embeddings([sent], tokenizer, model))\n",
        "    with torch.no_grad():\n",
        "        seq_len = len(sent)\n",
        "        outputs = distance_model(embds, torch.ones((seq_len, seq_len)))\n",
        "        predicted_outputs = torch.argmax(outputs, dim=3)\n",
        "        extracted_parents = extract_parents(predicted_outputs, outputs, seq_len)[0]\n",
        "\n",
        "        return ParsedSentence(sent.words, extracted_parents)\n",
        "\n",
        "if not FINAL_EVALUATION_MODE:\n",
        "    sent = train_sentences[30]\n",
        "    parse_sentence(sent, distance_model, depth_model, tokenizer, model).pretty_print()  # Przewidziane drzewo\n",
        "    sent.pretty_print()  # Złote drzewo (ze zbioru danych)\n",
        "    print(sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvOtnBjSM4Z5"
      },
      "source": [
        "# Ewaluacja\n",
        "Kod bardzo podobny do poniższego będzie służył do ewaluacji rozwiązania na zdaniach testowych. Wywołując poniższe komórki możesz dowiedzieć się ile punktów zdobyłoby twoje rozwiązanie, gdybyśmy ocenili je na danych walidacyjnych. Przed wysłaniem rozwiązania upewnij się, że cały notebook wykonuje się od początku do końca bez błędów i bez ingerencji użytkownika po wykonaniu polecenia `Run All`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "r1P9C0ONM4Z6"
      },
      "outputs": [],
      "source": [
        "def points(root_placement, uuas):\n",
        "    def scale(x, lower=0.5, upper=0.85):\n",
        "        scaled = min(max(x, lower), upper)\n",
        "        return (scaled - lower) / (upper - lower)\n",
        "    return (scale(root_placement) + scale(uuas))\n",
        "\n",
        "def evaluate_model(sentences: List[ParsedSentence], distance_model, depth_model, tokenizer, model):\n",
        "    sum_uuas = 0\n",
        "    root_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for sent in sentences:\n",
        "            parsed = parse_sentence(sent, distance_model, depth_model, tokenizer, model)\n",
        "            root_correct += int(parsed.root == sent.root)\n",
        "            sum_uuas += uuas_score(sent, parsed)\n",
        "\n",
        "    root_placement = root_correct / len(sentences)\n",
        "    uuas = sum_uuas / len(sentences)\n",
        "\n",
        "    print(f\"UUAS: {uuas * 100:.3}%\")\n",
        "    print(f\"Root placement: {root_placement * 100:.3}%\")\n",
        "    print(f\"Your score: {points(root_placement, uuas):.1}/2.0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "H9krrUiIM4Z6"
      },
      "outputs": [],
      "source": [
        "if not FINAL_EVALUATION_MODE:\n",
        "    distance_model_loaded = DistanceModel()\n",
        "    distance_model_loaded.load_state_dict(torch.load(DISTANCE_MODEL_PATH))\n",
        "\n",
        "    depth_model_loaded = DepthModel()\n",
        "    depth_model_loaded.load_state_dict(torch.load(DEPTH_MODEL_PATH))\n",
        "\n",
        "    evaluate_model(val_sentences, distance_model_loaded, depth_model_loaded, tokenizer, model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
