{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWM_zMbq5Ni0",
        "outputId": "b53e5493-ebc4-4a31-9f20-fdb1989db3af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2024.5.15)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sacremoses\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "o-Yu7fO83puA",
        "outputId": "ae000f2d-1dae-4bb6-bdd0-7d7278aa9f49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at allegro/herbert-klej-cased-v1 and are newly initialized: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
            "        num_rows: 8960\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
            "        num_rows: 747\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
            "        num_rows: 1099\n",
            "    })\n",
            "})\n",
            "Epoch: 1/5; Step: 0/280; Loss: 10.054091453552246\n",
            "Epoch: 1/5; Step: 5/280; Loss: 0.938270092010498\n",
            "Epoch: 1/5; Step: 10/280; Loss: 0.783644437789917\n",
            "Epoch: 1/5; Step: 15/280; Loss: 0.6947688460350037\n",
            "Epoch: 1/5; Step: 20/280; Loss: 0.6816601753234863\n",
            "Epoch: 1/5; Step: 25/280; Loss: 0.6460989117622375\n",
            "Epoch: 1/5; Step: 30/280; Loss: 0.6351413130760193\n",
            "Epoch: 1/5; Step: 35/280; Loss: 0.6485887765884399\n",
            "Epoch: 1/5; Step: 40/280; Loss: 0.6476826071739197\n",
            "Epoch: 1/5; Step: 45/280; Loss: 0.5985161662101746\n",
            "Epoch: 1/5; Step: 50/280; Loss: 0.5839060544967651\n",
            "Epoch: 1/5; Step: 55/280; Loss: 0.5832929015159607\n",
            "Epoch: 1/5; Step: 60/280; Loss: 0.6196713447570801\n",
            "Epoch: 1/5; Step: 65/280; Loss: 0.6507148742675781\n",
            "Epoch: 1/5; Step: 70/280; Loss: 0.6145877242088318\n",
            "Epoch: 1/5; Step: 75/280; Loss: 0.6024031043052673\n",
            "Epoch: 1/5; Step: 80/280; Loss: 0.6253135800361633\n",
            "Epoch: 1/5; Step: 85/280; Loss: 0.5818758010864258\n",
            "Epoch: 1/5; Step: 90/280; Loss: 0.6214134097099304\n",
            "Epoch: 1/5; Step: 95/280; Loss: 0.5672994256019592\n",
            "Epoch: 1/5; Step: 100/280; Loss: 0.6008098125457764\n",
            "Epoch: 1/5; Step: 105/280; Loss: 0.7577144503593445\n",
            "Epoch: 1/5; Step: 110/280; Loss: 0.6380924582481384\n",
            "Epoch: 1/5; Step: 115/280; Loss: 0.6231889724731445\n",
            "Epoch: 1/5; Step: 120/280; Loss: 0.6615390181541443\n",
            "Epoch: 1/5; Step: 125/280; Loss: 0.6314327120780945\n",
            "Epoch: 1/5; Step: 130/280; Loss: 0.5525858998298645\n",
            "Epoch: 1/5; Step: 135/280; Loss: 0.5712383389472961\n",
            "Epoch: 1/5; Step: 140/280; Loss: 0.5840633511543274\n",
            "Epoch: 1/5; Step: 145/280; Loss: 0.7444381713867188\n",
            "Epoch: 1/5; Step: 150/280; Loss: 0.595299243927002\n",
            "Epoch: 1/5; Step: 155/280; Loss: 0.6284533143043518\n",
            "Epoch: 1/5; Step: 160/280; Loss: 0.6334326863288879\n",
            "Epoch: 1/5; Step: 165/280; Loss: 0.6384632587432861\n",
            "Epoch: 1/5; Step: 170/280; Loss: 0.5744333863258362\n",
            "Epoch: 1/5; Step: 175/280; Loss: 0.6441845893859863\n",
            "Epoch: 1/5; Step: 180/280; Loss: 0.576825737953186\n",
            "Epoch: 1/5; Step: 185/280; Loss: 0.6305036544799805\n",
            "Epoch: 1/5; Step: 190/280; Loss: 0.6058154702186584\n",
            "Epoch: 1/5; Step: 195/280; Loss: 0.5765970945358276\n",
            "Epoch: 1/5; Step: 200/280; Loss: 0.5298853516578674\n",
            "Epoch: 1/5; Step: 205/280; Loss: 0.6218010783195496\n",
            "Epoch: 1/5; Step: 210/280; Loss: 0.5995029807090759\n",
            "Epoch: 1/5; Step: 215/280; Loss: 0.6276577115058899\n",
            "Epoch: 1/5; Step: 220/280; Loss: 0.5447849035263062\n",
            "Epoch: 1/5; Step: 225/280; Loss: 0.5179703831672668\n",
            "Epoch: 1/5; Step: 230/280; Loss: 0.5927902460098267\n",
            "Epoch: 1/5; Step: 235/280; Loss: 0.584351658821106\n",
            "Epoch: 1/5; Step: 240/280; Loss: 0.5389331579208374\n",
            "Epoch: 1/5; Step: 245/280; Loss: 0.558089554309845\n",
            "Epoch: 1/5; Step: 250/280; Loss: 0.5757028460502625\n",
            "Epoch: 1/5; Step: 255/280; Loss: 0.608971893787384\n",
            "Epoch: 1/5; Step: 260/280; Loss: 0.5403152704238892\n",
            "Epoch: 1/5; Step: 265/280; Loss: 0.5072227120399475\n",
            "Epoch: 1/5; Step: 270/280; Loss: 0.5181264281272888\n",
            "Epoch: 1/5; Step: 275/280; Loss: 0.5846473574638367\n",
            "Epoch 1, Training Loss: 0.5855355262756348\n",
            "Epoch 1, Perplexity: 1.7738198041915894, Avg loss: 0.5731353438203753, Generated Text: Jam jest Jacek, że że co w z,\n",
            "Epoch: 2/5; Step: 0/280; Loss: 0.48938852548599243\n",
            "Epoch: 2/5; Step: 5/280; Loss: 0.6112006306648254\n",
            "Epoch: 2/5; Step: 10/280; Loss: 0.5699514746665955\n",
            "Epoch: 2/5; Step: 15/280; Loss: 0.5830240845680237\n",
            "Epoch: 2/5; Step: 20/280; Loss: 0.5593667030334473\n",
            "Epoch: 2/5; Step: 25/280; Loss: 0.5308524966239929\n",
            "Epoch: 2/5; Step: 30/280; Loss: 0.5548895001411438\n",
            "Epoch: 2/5; Step: 35/280; Loss: 0.5555319786071777\n",
            "Epoch: 2/5; Step: 40/280; Loss: 0.5123792290687561\n",
            "Epoch: 2/5; Step: 45/280; Loss: 0.5279650092124939\n",
            "Epoch: 2/5; Step: 50/280; Loss: 0.510356068611145\n",
            "Epoch: 2/5; Step: 55/280; Loss: 0.5465027689933777\n",
            "Epoch: 2/5; Step: 60/280; Loss: 0.560133159160614\n",
            "Epoch: 2/5; Step: 65/280; Loss: 0.5833867788314819\n",
            "Epoch: 2/5; Step: 70/280; Loss: 0.5059981346130371\n",
            "Epoch: 2/5; Step: 75/280; Loss: 0.5120466351509094\n",
            "Epoch: 2/5; Step: 80/280; Loss: 0.5281667113304138\n",
            "Epoch: 2/5; Step: 85/280; Loss: 0.5093669295310974\n",
            "Epoch: 2/5; Step: 90/280; Loss: 0.5768287181854248\n",
            "Epoch: 2/5; Step: 95/280; Loss: 0.7423138618469238\n",
            "Epoch: 2/5; Step: 100/280; Loss: 0.5147921442985535\n",
            "Epoch: 2/5; Step: 105/280; Loss: 0.5508946180343628\n",
            "Epoch: 2/5; Step: 110/280; Loss: 0.7069213390350342\n",
            "Epoch: 2/5; Step: 115/280; Loss: 0.7347269058227539\n",
            "Epoch: 2/5; Step: 120/280; Loss: 0.5513167381286621\n",
            "Epoch: 2/5; Step: 125/280; Loss: 0.5281276702880859\n",
            "Epoch: 2/5; Step: 130/280; Loss: 0.5426980257034302\n",
            "Epoch: 2/5; Step: 135/280; Loss: 0.5635154843330383\n",
            "Epoch: 2/5; Step: 140/280; Loss: 0.5648378729820251\n",
            "Epoch: 2/5; Step: 145/280; Loss: 0.5505121350288391\n",
            "Epoch: 2/5; Step: 150/280; Loss: 0.554766833782196\n",
            "Epoch: 2/5; Step: 155/280; Loss: 0.5483101010322571\n",
            "Epoch: 2/5; Step: 160/280; Loss: 0.5524364709854126\n",
            "Epoch: 2/5; Step: 165/280; Loss: 0.5375273823738098\n",
            "Epoch: 2/5; Step: 170/280; Loss: 0.5747583508491516\n",
            "Epoch: 2/5; Step: 175/280; Loss: 0.5438529253005981\n",
            "Epoch: 2/5; Step: 180/280; Loss: 0.5702694654464722\n",
            "Epoch: 2/5; Step: 185/280; Loss: 0.5084998607635498\n",
            "Epoch: 2/5; Step: 190/280; Loss: 0.5734152793884277\n",
            "Epoch: 2/5; Step: 195/280; Loss: 0.49296894669532776\n",
            "Epoch: 2/5; Step: 200/280; Loss: 0.5404192805290222\n",
            "Epoch: 2/5; Step: 205/280; Loss: 0.5340302586555481\n",
            "Epoch: 2/5; Step: 210/280; Loss: 0.5137606263160706\n",
            "Epoch: 2/5; Step: 215/280; Loss: 0.5181509256362915\n",
            "Epoch: 2/5; Step: 220/280; Loss: 0.5745148658752441\n",
            "Epoch: 2/5; Step: 225/280; Loss: 0.5431132316589355\n",
            "Epoch: 2/5; Step: 230/280; Loss: 0.5616327524185181\n",
            "Epoch: 2/5; Step: 235/280; Loss: 0.527212381362915\n",
            "Epoch: 2/5; Step: 240/280; Loss: 0.5968531966209412\n",
            "Epoch: 2/5; Step: 245/280; Loss: 0.5510166883468628\n",
            "Epoch: 2/5; Step: 250/280; Loss: 0.5344749689102173\n",
            "Epoch: 2/5; Step: 255/280; Loss: 0.5410619974136353\n",
            "Epoch: 2/5; Step: 260/280; Loss: 0.5323415398597717\n",
            "Epoch: 2/5; Step: 265/280; Loss: 0.5457517504692078\n",
            "Epoch: 2/5; Step: 270/280; Loss: 0.5253507494926453\n",
            "Epoch: 2/5; Step: 275/280; Loss: 0.4597513973712921\n",
            "Epoch 2, Training Loss: 0.5032726526260376\n",
            "Epoch 2, Perplexity: 1.7410045862197876, Avg loss: 0.5544622913900629, Generated Text: Jam jest Jacek, a w domu, a w nim,\n",
            "Epoch: 3/5; Step: 0/280; Loss: 0.5439063310623169\n",
            "Epoch: 3/5; Step: 5/280; Loss: 0.5177651643753052\n",
            "Epoch: 3/5; Step: 10/280; Loss: 0.5677517652511597\n",
            "Epoch: 3/5; Step: 15/280; Loss: 0.5049473643302917\n",
            "Epoch: 3/5; Step: 20/280; Loss: 0.5899438858032227\n",
            "Epoch: 3/5; Step: 25/280; Loss: 0.5658435225486755\n",
            "Epoch: 3/5; Step: 30/280; Loss: 0.5528057813644409\n",
            "Epoch: 3/5; Step: 35/280; Loss: 0.4883832335472107\n",
            "Epoch: 3/5; Step: 40/280; Loss: 0.5515831708908081\n",
            "Epoch: 3/5; Step: 45/280; Loss: 0.5207798480987549\n",
            "Epoch: 3/5; Step: 50/280; Loss: 0.5338950753211975\n",
            "Epoch: 3/5; Step: 55/280; Loss: 0.4946976900100708\n",
            "Epoch: 3/5; Step: 60/280; Loss: 0.5060586929321289\n",
            "Epoch: 3/5; Step: 65/280; Loss: 0.5246251225471497\n",
            "Epoch: 3/5; Step: 70/280; Loss: 0.5160632729530334\n",
            "Epoch: 3/5; Step: 75/280; Loss: 0.5346918702125549\n",
            "Epoch: 3/5; Step: 80/280; Loss: 0.5291404128074646\n",
            "Epoch: 3/5; Step: 85/280; Loss: 0.548614501953125\n",
            "Epoch: 3/5; Step: 90/280; Loss: 0.5226773023605347\n",
            "Epoch: 3/5; Step: 95/280; Loss: 0.5850909948348999\n",
            "Epoch: 3/5; Step: 100/280; Loss: 0.5159013867378235\n",
            "Epoch: 3/5; Step: 105/280; Loss: 0.5635480880737305\n",
            "Epoch: 3/5; Step: 110/280; Loss: 0.5175957679748535\n",
            "Epoch: 3/5; Step: 115/280; Loss: 0.4574293792247772\n",
            "Epoch: 3/5; Step: 120/280; Loss: 0.4773422181606293\n",
            "Epoch: 3/5; Step: 125/280; Loss: 0.5196642279624939\n",
            "Epoch: 3/5; Step: 130/280; Loss: 0.5737085342407227\n",
            "Epoch: 3/5; Step: 135/280; Loss: 0.5306223630905151\n",
            "Epoch: 3/5; Step: 140/280; Loss: 0.49193254113197327\n",
            "Epoch: 3/5; Step: 145/280; Loss: 0.5153142213821411\n",
            "Epoch: 3/5; Step: 150/280; Loss: 0.45869895815849304\n",
            "Epoch: 3/5; Step: 155/280; Loss: 0.4612605571746826\n",
            "Epoch: 3/5; Step: 160/280; Loss: 0.5694188475608826\n",
            "Epoch: 3/5; Step: 165/280; Loss: 0.5132691264152527\n",
            "Epoch: 3/5; Step: 170/280; Loss: 0.5108247995376587\n",
            "Epoch: 3/5; Step: 175/280; Loss: 0.5464410781860352\n",
            "Epoch: 3/5; Step: 180/280; Loss: 0.5296992063522339\n",
            "Epoch: 3/5; Step: 185/280; Loss: 0.5282883644104004\n",
            "Epoch: 3/5; Step: 190/280; Loss: 0.5191359519958496\n",
            "Epoch: 3/5; Step: 195/280; Loss: 0.5539879202842712\n",
            "Epoch: 3/5; Step: 200/280; Loss: 0.5296707153320312\n",
            "Epoch: 3/5; Step: 205/280; Loss: 0.5524548888206482\n",
            "Epoch: 3/5; Step: 210/280; Loss: 0.5588613152503967\n",
            "Epoch: 3/5; Step: 215/280; Loss: 0.48786377906799316\n",
            "Epoch: 3/5; Step: 220/280; Loss: 0.5430803298950195\n",
            "Epoch: 3/5; Step: 225/280; Loss: 0.5550293922424316\n",
            "Epoch: 3/5; Step: 230/280; Loss: 0.5205140709877014\n",
            "Epoch: 3/5; Step: 235/280; Loss: 0.5132330060005188\n",
            "Epoch: 3/5; Step: 240/280; Loss: 0.5611073970794678\n",
            "Epoch: 3/5; Step: 245/280; Loss: 0.5140260457992554\n",
            "Epoch: 3/5; Step: 250/280; Loss: 0.532233715057373\n",
            "Epoch: 3/5; Step: 255/280; Loss: 0.5326342582702637\n",
            "Epoch: 3/5; Step: 260/280; Loss: 0.4824686050415039\n",
            "Epoch: 3/5; Step: 265/280; Loss: 0.5176790952682495\n",
            "Epoch: 3/5; Step: 270/280; Loss: 0.4938149154186249\n",
            "Epoch: 3/5; Step: 275/280; Loss: 0.5170692205429077\n",
            "Epoch 3, Training Loss: 0.5440346002578735\n",
            "Epoch 3, Perplexity: 1.7166318893432617, Avg loss: 0.5403641171883069, Generated Text: Jam jest Jacek, że nie jest nie jest,\n",
            "Epoch: 4/5; Step: 0/280; Loss: 0.506392776966095\n",
            "Epoch: 4/5; Step: 5/280; Loss: 0.5303610563278198\n",
            "Epoch: 4/5; Step: 10/280; Loss: 0.6807243227958679\n",
            "Epoch: 4/5; Step: 15/280; Loss: 0.4964387118816376\n",
            "Epoch: 4/5; Step: 20/280; Loss: 0.5277443528175354\n",
            "Epoch: 4/5; Step: 25/280; Loss: 0.5068835616111755\n",
            "Epoch: 4/5; Step: 30/280; Loss: 0.5418258309364319\n",
            "Epoch: 4/5; Step: 35/280; Loss: 0.47657179832458496\n",
            "Epoch: 4/5; Step: 40/280; Loss: 0.5326507091522217\n",
            "Epoch: 4/5; Step: 45/280; Loss: 0.5274887084960938\n",
            "Epoch: 4/5; Step: 50/280; Loss: 0.4747049808502197\n",
            "Epoch: 4/5; Step: 55/280; Loss: 0.4798411726951599\n",
            "Epoch: 4/5; Step: 60/280; Loss: 0.5410430431365967\n",
            "Epoch: 4/5; Step: 65/280; Loss: 0.49471282958984375\n",
            "Epoch: 4/5; Step: 70/280; Loss: 0.5250234603881836\n",
            "Epoch: 4/5; Step: 75/280; Loss: 0.4333762228488922\n",
            "Epoch: 4/5; Step: 80/280; Loss: 0.48859336972236633\n",
            "Epoch: 4/5; Step: 85/280; Loss: 0.4819546341896057\n",
            "Epoch: 4/5; Step: 90/280; Loss: 0.5463910102844238\n",
            "Epoch: 4/5; Step: 95/280; Loss: 0.4845113754272461\n",
            "Epoch: 4/5; Step: 100/280; Loss: 0.5422158241271973\n",
            "Epoch: 4/5; Step: 105/280; Loss: 0.47208836674690247\n",
            "Epoch: 4/5; Step: 110/280; Loss: 0.48373597860336304\n",
            "Epoch: 4/5; Step: 115/280; Loss: 0.5210554599761963\n",
            "Epoch: 4/5; Step: 120/280; Loss: 0.4833178222179413\n",
            "Epoch: 4/5; Step: 125/280; Loss: 0.4840569794178009\n",
            "Epoch: 4/5; Step: 130/280; Loss: 0.4930293560028076\n",
            "Epoch: 4/5; Step: 135/280; Loss: 0.5131816267967224\n",
            "Epoch: 4/5; Step: 140/280; Loss: 0.5079730153083801\n",
            "Epoch: 4/5; Step: 145/280; Loss: 0.5380253791809082\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-065bc3ad7379>\u001b[0m in \u001b[0;36m<cell line: 63>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from transformers import RobertaForCausalLM, AutoTokenizer\n",
        "import datasets\n",
        "import torch\n",
        "\n",
        "model = RobertaForCausalLM.from_pretrained(\"allegro/herbert-klej-cased-v1\", is_decoder=True).to(\"cuda\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-klej-cased-tokenizer-v1\")\n",
        "\n",
        "ds = datasets.load_dataset(\"text\", data_files={\n",
        "   \"train\": \"pan_tadeusz_1_10.txt\",\n",
        "   \"validation\": \"pan_tadeusz_11.txt\",\n",
        "   \"test\": \"pan_tadeusz_12.txt\",\n",
        "})\n",
        "\n",
        "def tokenize_function(examples):\n",
        "   return tokenizer(examples[\"text\"], padding=\"max_length\", max_length=137)\n",
        "\n",
        "tokenized_datasets = ds.map(tokenize_function, batched=True, num_proc=1, remove_columns=[\"text\"])\n",
        "print(tokenized_datasets)\n",
        "\n",
        "init_text = \"Jam jest Jacek\"\n",
        "\n",
        "class MLMDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokenized_dataset, tokenizer, mlm_probability=0.15):\n",
        "        self.input_ids = tokenized_dataset[\"input_ids\"]\n",
        "        self.attention_mask = tokenized_dataset[\"attention_mask\"]\n",
        "        self.tokenizer = tokenizer\n",
        "        self.mlm_probability = mlm_probability\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_ids = torch.tensor(self.input_ids[idx])\n",
        "        attention_mask = torch.tensor(self.attention_mask[idx])\n",
        "\n",
        "        # Klonujemy oryginalne tokeny i traktujemy jako labele\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        # Tworzymy maskę z prawdopodobieństwem 15%, nie maskujemy tokenów specjalnych i tokenów paddignu\n",
        "        rand = torch.rand(input_ids.shape)\n",
        "        mask_arr = (rand < self.mlm_probability) * (input_ids != self.tokenizer.cls_token_id) * (input_ids != self.tokenizer.sep_token_id) * (input_ids != self.tokenizer.pad_token_id)\n",
        "        selection = torch.flatten(mask_arr.nonzero()).tolist()\n",
        "\n",
        "        # Maskujemy oryginalne\n",
        "        input_ids[selection] = self.tokenizer.mask_token_id\n",
        "\n",
        "        return [\n",
        "            input_ids,\n",
        "            attention_mask,\n",
        "            labels\n",
        "        ]\n",
        "\n",
        "train_dataset = MLMDataset(tokenized_datasets[\"train\"], tokenizer)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "valid_dataset = MLMDataset(tokenized_datasets[\"validation\"], tokenizer)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "model.train()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "epoch_num = 5\n",
        "for epoch in range(epoch_num):\n",
        "    # Trening\n",
        "    model.train()\n",
        "    for i, (input_ids, attention_mask, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = input_ids.to(\"cuda\")\n",
        "        attention_mask = attention_mask.to(\"cuda\")\n",
        "        labels = labels.to(\"cuda\")\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 5 == 0:\n",
        "            print(f\"Epoch: {epoch + 1}/{epoch_num}; Step: {i}/{len(train_loader)}; Loss: {loss.item()}\")\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Training Loss: {loss.item()}\")\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    num_batches = 0\n",
        "    num_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attention_mask, labels in valid_loader:\n",
        "            input_ids = input_ids.to(\"cuda\")\n",
        "            attention_mask = attention_mask.to(\"cuda\")\n",
        "            labels = labels.to(\"cuda\")\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item() * input_ids.size(0)  # sum the loss for each batch\n",
        "            num_samples += input_ids.size(0)\n",
        "            num_batches += 1\n",
        "\n",
        "    avg_val_loss = val_loss / num_samples\n",
        "\n",
        "    # Generowanie przykładowego tekstu\n",
        "    input_ids = tokenizer.encode(init_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    temperature = 0.2\n",
        "    top_p = 0.9\n",
        "    generated_ids = model.generate(\n",
        "        input_ids,\n",
        "        max_length=150,\n",
        "        num_return_sequences=1,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        do_sample=True,\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    print(f\"Epoch {epoch + 1}, Avg loss: {avg_val_loss}, Generated Text: {generated_text}\")\n",
        "\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# Wydrukowanie kształtu tensorów wejściowych\n",
        "ids = torch.tensor(tokenized_datasets[\"train\"][\"input_ids\"])\n",
        "print(ids.shape)  # torch.Size([8960, 137])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}