{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWM_zMbq5Ni0",
        "outputId": "b47b6890-8f19-42b4-f43a-787f3fa25055"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2024.5.15)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sacremoses\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaForCausalLM, AutoTokenizer\n",
        "import datasets\n",
        "import torch\n",
        "\n",
        "model = RobertaForCausalLM.from_pretrained(\"allegro/herbert-klej-cased-v1\", is_decoder=True).to(\"cuda\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-klej-cased-tokenizer-v1\")\n",
        "\n",
        "ds = datasets.load_dataset(\"text\", data_files={\n",
        "   \"train\": \"pan_tadeusz_1_10.txt\",\n",
        "   \"validation\": \"pan_tadeusz_11.txt\",\n",
        "   \"test\": \"pan_tadeusz_12.txt\",\n",
        "})\n",
        "\n",
        "def flatten(xss):\n",
        "    return [x for xs in xss for x in xs]\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"])\n",
        "\n",
        "tokenized_datasets = ds.map(tokenize_function, batched=True, num_proc=1, remove_columns=[\"text\"])\n",
        "print(tokenized_datasets)\n",
        "\n",
        "init_text = \"Jam jest Jacek\"\n",
        "\n",
        "class MLMDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokenized_dataset, tokenizer, mlm_probability=0.15, window_size=50):\n",
        "        self.input_ids = flatten(tokenized_dataset[\"input_ids\"])\n",
        "        self.attention_mask = flatten(tokenized_dataset[\"attention_mask\"])\n",
        "        self.tokenizer = tokenizer\n",
        "        self.mlm_probability = mlm_probability\n",
        "        self.window_size = window_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.input_ids)//self.window_size)*self.window_size - self.window_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_ids = torch.tensor(self.input_ids[idx:idx+self.window_size])\n",
        "\n",
        "        # Klonujemy oryginalne tokeny i traktujemy jako labele\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        # Tworzymy maskę z prawdopodobieństwem 15%\n",
        "        rand = torch.rand(input_ids.shape)\n",
        "        mask_arr = (rand < self.mlm_probability)\n",
        "        selection = torch.flatten(mask_arr.nonzero()).tolist()\n",
        "\n",
        "        # Maskujemy oryginalne\n",
        "        input_ids[selection] = self.tokenizer.mask_token_id\n",
        "\n",
        "        return [\n",
        "            input_ids,\n",
        "            labels\n",
        "        ]\n",
        "\n",
        "train_dataset = MLMDataset(tokenized_datasets[\"train\"], tokenizer)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "valid_dataset = MLMDataset(tokenized_datasets[\"validation\"], tokenizer)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(len(train_dataset))\n",
        "masked, labeled = next(iter(train_dataset))\n",
        "\n",
        "masked_text = tokenizer.decode(masked, skip_special_tokens=False)\n",
        "labeled_text = tokenizer.decode(labeled, skip_special_tokens=False)\n",
        "\n",
        "print(\"Masked: \", masked_text)\n",
        "print(\"Labeled: \", labeled_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_a8yjUM4hmZ",
        "outputId": "d93f9b76-1299-415d-9214-392cd7dcec15"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at allegro/herbert-klej-cased-v1 and are newly initialized: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
            "        num_rows: 8960\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
            "        num_rows: 747\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
            "        num_rows: 1099\n",
            "    })\n",
            "})\n",
            "113800\n",
            "Masked:  <s>Księga pierwsza </s><s></s><s><mask><s><mask><s>Gospodarstwo </s><s></s><s>Powrót panicza - <mask>się pierwsze w pokoiku <mask>drugie u stołu - Ważna Sędziego nauka o grzeczności - Podkomo<mask>go uwagi polityczne nad mod\n",
            "Labeled:  <s>Księga pierwsza </s><s></s><s></s><s></s><s>Gospodarstwo </s><s></s><s>Powrót panicza - Spotkanie się pierwsze w pokoiku, drugie u stołu - Ważna Sędziego nauka o grzeczności - Podkomorzego uwagi polityczne nad mod\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "o-Yu7fO83puA",
        "outputId": "fdd27a76-ae4f-4bef-b923-4f9271f4ee96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/5; Step: 0/3557; Loss: 11.826803207397461\n",
            "Epoch: 1/5; Step: 5/3557; Loss: 7.672152996063232\n",
            "Epoch: 1/5; Step: 10/3557; Loss: 7.27687406539917\n",
            "Epoch: 1/5; Step: 15/3557; Loss: 7.043516635894775\n",
            "Epoch: 1/5; Step: 20/3557; Loss: 6.806664943695068\n",
            "Epoch: 1/5; Step: 25/3557; Loss: 6.6434454917907715\n",
            "Epoch: 1/5; Step: 30/3557; Loss: 6.557663440704346\n",
            "Epoch: 1/5; Step: 35/3557; Loss: 6.407878875732422\n",
            "Epoch: 1/5; Step: 40/3557; Loss: 6.436079978942871\n",
            "Epoch: 1/5; Step: 45/3557; Loss: 6.350093841552734\n",
            "Epoch: 1/5; Step: 50/3557; Loss: 6.223112106323242\n",
            "Epoch: 1/5; Step: 55/3557; Loss: 6.154550552368164\n",
            "Epoch: 1/5; Step: 60/3557; Loss: 6.053224086761475\n",
            "Epoch: 1/5; Step: 65/3557; Loss: 6.08909273147583\n",
            "Epoch: 1/5; Step: 70/3557; Loss: 5.983164310455322\n",
            "Epoch: 1/5; Step: 75/3557; Loss: 5.960041046142578\n",
            "Epoch: 1/5; Step: 80/3557; Loss: 6.0462470054626465\n",
            "Epoch: 1/5; Step: 85/3557; Loss: 5.914473056793213\n",
            "Epoch: 1/5; Step: 90/3557; Loss: 5.943591117858887\n",
            "Epoch: 1/5; Step: 95/3557; Loss: 5.894397735595703\n",
            "Epoch: 1/5; Step: 100/3557; Loss: 5.820369720458984\n",
            "Epoch: 1/5; Step: 105/3557; Loss: 5.787489414215088\n",
            "Epoch: 1/5; Step: 110/3557; Loss: 5.791923522949219\n",
            "Epoch: 1/5; Step: 115/3557; Loss: 5.8399457931518555\n",
            "Epoch: 1/5; Step: 120/3557; Loss: 5.741306304931641\n",
            "Epoch: 1/5; Step: 125/3557; Loss: 5.719851016998291\n",
            "Epoch: 1/5; Step: 130/3557; Loss: 5.625303268432617\n",
            "Epoch: 1/5; Step: 135/3557; Loss: 5.744330406188965\n",
            "Epoch: 1/5; Step: 140/3557; Loss: 5.580838680267334\n",
            "Epoch: 1/5; Step: 145/3557; Loss: 5.71163272857666\n",
            "Epoch: 1/5; Step: 150/3557; Loss: 5.709297180175781\n",
            "Epoch: 1/5; Step: 155/3557; Loss: 5.633721828460693\n",
            "Epoch: 1/5; Step: 160/3557; Loss: 5.521831512451172\n",
            "Epoch: 1/5; Step: 165/3557; Loss: 5.434770107269287\n",
            "Epoch: 1/5; Step: 170/3557; Loss: 5.511236190795898\n",
            "Epoch: 1/5; Step: 175/3557; Loss: 5.57516622543335\n",
            "Epoch: 1/5; Step: 180/3557; Loss: 5.419723987579346\n",
            "Epoch: 1/5; Step: 185/3557; Loss: 5.44101619720459\n",
            "Epoch: 1/5; Step: 190/3557; Loss: 5.525426864624023\n",
            "Epoch: 1/5; Step: 195/3557; Loss: 5.438930988311768\n",
            "Epoch: 1/5; Step: 200/3557; Loss: 5.352518081665039\n",
            "Epoch: 1/5; Step: 205/3557; Loss: 5.401352405548096\n",
            "Epoch: 1/5; Step: 210/3557; Loss: 5.277298450469971\n",
            "Epoch: 1/5; Step: 215/3557; Loss: 5.36508846282959\n",
            "Epoch: 1/5; Step: 220/3557; Loss: 5.345587730407715\n",
            "Epoch: 1/5; Step: 225/3557; Loss: 5.3769145011901855\n",
            "Epoch: 1/5; Step: 230/3557; Loss: 5.223540782928467\n",
            "Epoch: 1/5; Step: 235/3557; Loss: 5.285271644592285\n",
            "Epoch: 1/5; Step: 240/3557; Loss: 5.300107479095459\n",
            "Epoch: 1/5; Step: 245/3557; Loss: 5.133518695831299\n",
            "Epoch: 1/5; Step: 250/3557; Loss: 5.176853656768799\n",
            "Epoch: 1/5; Step: 255/3557; Loss: 5.211503982543945\n",
            "Epoch: 1/5; Step: 260/3557; Loss: 5.037386417388916\n",
            "Epoch: 1/5; Step: 265/3557; Loss: 5.231337547302246\n",
            "Epoch: 1/5; Step: 270/3557; Loss: 5.26425313949585\n",
            "Epoch: 1/5; Step: 275/3557; Loss: 5.1403398513793945\n",
            "Epoch: 1/5; Step: 280/3557; Loss: 5.113351821899414\n",
            "Epoch: 1/5; Step: 285/3557; Loss: 5.0170769691467285\n",
            "Epoch: 1/5; Step: 290/3557; Loss: 5.108234405517578\n",
            "Epoch: 1/5; Step: 295/3557; Loss: 5.134955406188965\n",
            "Epoch: 1/5; Step: 300/3557; Loss: 4.998502254486084\n",
            "Epoch: 1/5; Step: 305/3557; Loss: 5.048345565795898\n",
            "Epoch: 1/5; Step: 310/3557; Loss: 5.15871000289917\n",
            "Epoch: 1/5; Step: 315/3557; Loss: 5.06623649597168\n",
            "Epoch: 1/5; Step: 320/3557; Loss: 4.962588787078857\n",
            "Epoch: 1/5; Step: 325/3557; Loss: 4.982832908630371\n",
            "Epoch: 1/5; Step: 330/3557; Loss: 4.882407188415527\n",
            "Epoch: 1/5; Step: 335/3557; Loss: 4.887604236602783\n",
            "Epoch: 1/5; Step: 340/3557; Loss: 5.106988906860352\n",
            "Epoch: 1/5; Step: 345/3557; Loss: 4.865318298339844\n",
            "Epoch: 1/5; Step: 350/3557; Loss: 4.943622589111328\n",
            "Epoch: 1/5; Step: 355/3557; Loss: 4.862115859985352\n",
            "Epoch: 1/5; Step: 360/3557; Loss: 4.960606098175049\n",
            "Epoch: 1/5; Step: 365/3557; Loss: 4.653607368469238\n",
            "Epoch: 1/5; Step: 370/3557; Loss: 4.873608112335205\n",
            "Epoch: 1/5; Step: 375/3557; Loss: 4.791054725646973\n",
            "Epoch: 1/5; Step: 380/3557; Loss: 4.842075347900391\n",
            "Epoch: 1/5; Step: 385/3557; Loss: 4.80719518661499\n",
            "Epoch: 1/5; Step: 390/3557; Loss: 4.795119762420654\n",
            "Epoch: 1/5; Step: 395/3557; Loss: 4.835097789764404\n",
            "Epoch: 1/5; Step: 400/3557; Loss: 4.858518123626709\n",
            "Epoch: 1/5; Step: 405/3557; Loss: 4.868475437164307\n",
            "Epoch: 1/5; Step: 410/3557; Loss: 4.731353282928467\n",
            "Epoch: 1/5; Step: 415/3557; Loss: 4.684656143188477\n",
            "Epoch: 1/5; Step: 420/3557; Loss: 4.791290760040283\n",
            "Epoch: 1/5; Step: 425/3557; Loss: 4.809120178222656\n",
            "Epoch: 1/5; Step: 430/3557; Loss: 4.783010959625244\n",
            "Epoch: 1/5; Step: 435/3557; Loss: 4.475997447967529\n",
            "Epoch: 1/5; Step: 440/3557; Loss: 4.767885684967041\n",
            "Epoch: 1/5; Step: 445/3557; Loss: 4.793154239654541\n",
            "Epoch: 1/5; Step: 450/3557; Loss: 4.724373817443848\n",
            "Epoch: 1/5; Step: 455/3557; Loss: 4.557780742645264\n",
            "Epoch: 1/5; Step: 460/3557; Loss: 4.710540771484375\n",
            "Epoch: 1/5; Step: 465/3557; Loss: 4.446030616760254\n",
            "Epoch: 1/5; Step: 470/3557; Loss: 4.609170436859131\n",
            "Epoch: 1/5; Step: 475/3557; Loss: 4.498246669769287\n",
            "Epoch: 1/5; Step: 480/3557; Loss: 4.588287830352783\n",
            "Epoch: 1/5; Step: 485/3557; Loss: 4.6200761795043945\n",
            "Epoch: 1/5; Step: 490/3557; Loss: 4.584345817565918\n",
            "Epoch: 1/5; Step: 495/3557; Loss: 4.595458030700684\n",
            "Epoch: 1/5; Step: 500/3557; Loss: 4.685971736907959\n",
            "Epoch: 1/5; Step: 505/3557; Loss: 4.5036940574646\n",
            "Epoch: 1/5; Step: 510/3557; Loss: 4.558437824249268\n",
            "Epoch: 1/5; Step: 515/3557; Loss: 4.508897304534912\n",
            "Epoch: 1/5; Step: 520/3557; Loss: 4.326883316040039\n",
            "Epoch: 1/5; Step: 525/3557; Loss: 4.569624423980713\n",
            "Epoch: 1/5; Step: 530/3557; Loss: 4.467204570770264\n",
            "Epoch: 1/5; Step: 535/3557; Loss: 4.491922378540039\n",
            "Epoch: 1/5; Step: 540/3557; Loss: 4.389155387878418\n",
            "Epoch: 1/5; Step: 545/3557; Loss: 4.430972576141357\n",
            "Epoch: 1/5; Step: 550/3557; Loss: 4.184655666351318\n",
            "Epoch: 1/5; Step: 555/3557; Loss: 4.456009864807129\n",
            "Epoch: 1/5; Step: 560/3557; Loss: 4.396910190582275\n",
            "Epoch: 1/5; Step: 565/3557; Loss: 4.479935169219971\n",
            "Epoch: 1/5; Step: 570/3557; Loss: 4.299509525299072\n",
            "Epoch: 1/5; Step: 575/3557; Loss: 4.209413528442383\n",
            "Epoch: 1/5; Step: 580/3557; Loss: 4.388580799102783\n",
            "Epoch: 1/5; Step: 585/3557; Loss: 4.394468307495117\n",
            "Epoch: 1/5; Step: 590/3557; Loss: 4.305121421813965\n",
            "Epoch: 1/5; Step: 595/3557; Loss: 4.233020782470703\n",
            "Epoch: 1/5; Step: 600/3557; Loss: 4.343491554260254\n",
            "Epoch: 1/5; Step: 605/3557; Loss: 4.418437957763672\n",
            "Epoch: 1/5; Step: 610/3557; Loss: 4.353623390197754\n",
            "Epoch: 1/5; Step: 615/3557; Loss: 4.1573896408081055\n",
            "Epoch: 1/5; Step: 620/3557; Loss: 4.210628986358643\n",
            "Epoch: 1/5; Step: 625/3557; Loss: 4.37741756439209\n",
            "Epoch: 1/5; Step: 630/3557; Loss: 4.311518669128418\n",
            "Epoch: 1/5; Step: 635/3557; Loss: 4.062192916870117\n",
            "Epoch: 1/5; Step: 640/3557; Loss: 4.226175308227539\n",
            "Epoch: 1/5; Step: 645/3557; Loss: 4.236332893371582\n",
            "Epoch: 1/5; Step: 650/3557; Loss: 4.123214244842529\n",
            "Epoch: 1/5; Step: 655/3557; Loss: 4.13534688949585\n",
            "Epoch: 1/5; Step: 660/3557; Loss: 4.113699913024902\n",
            "Epoch: 1/5; Step: 665/3557; Loss: 4.184942245483398\n",
            "Epoch: 1/5; Step: 670/3557; Loss: 4.295844554901123\n",
            "Epoch: 1/5; Step: 675/3557; Loss: 4.313190460205078\n",
            "Epoch: 1/5; Step: 680/3557; Loss: 3.9763457775115967\n",
            "Epoch: 1/5; Step: 685/3557; Loss: 4.205255508422852\n",
            "Epoch: 1/5; Step: 690/3557; Loss: 4.150750637054443\n",
            "Epoch: 1/5; Step: 695/3557; Loss: 4.077399253845215\n",
            "Epoch: 1/5; Step: 700/3557; Loss: 4.086331367492676\n",
            "Epoch: 1/5; Step: 705/3557; Loss: 4.102110862731934\n",
            "Epoch: 1/5; Step: 710/3557; Loss: 4.143863677978516\n",
            "Epoch: 1/5; Step: 715/3557; Loss: 4.068388938903809\n",
            "Epoch: 1/5; Step: 720/3557; Loss: 3.9178519248962402\n",
            "Epoch: 1/5; Step: 725/3557; Loss: 3.9207913875579834\n",
            "Epoch: 1/5; Step: 730/3557; Loss: 4.296790599822998\n",
            "Epoch: 1/5; Step: 735/3557; Loss: 4.0152692794799805\n",
            "Epoch: 1/5; Step: 740/3557; Loss: 4.032047271728516\n",
            "Epoch: 1/5; Step: 745/3557; Loss: 3.917410373687744\n",
            "Epoch: 1/5; Step: 750/3557; Loss: 3.9926271438598633\n",
            "Epoch: 1/5; Step: 755/3557; Loss: 3.957394599914551\n",
            "Epoch: 1/5; Step: 760/3557; Loss: 4.0171709060668945\n",
            "Epoch: 1/5; Step: 765/3557; Loss: 3.980647087097168\n",
            "Epoch: 1/5; Step: 770/3557; Loss: 3.9585013389587402\n",
            "Epoch: 1/5; Step: 775/3557; Loss: 3.9359915256500244\n",
            "Epoch: 1/5; Step: 780/3557; Loss: 3.903162956237793\n",
            "Epoch: 1/5; Step: 785/3557; Loss: 3.819462299346924\n",
            "Epoch: 1/5; Step: 790/3557; Loss: 3.696337938308716\n",
            "Epoch: 1/5; Step: 795/3557; Loss: 3.7928333282470703\n",
            "Epoch: 1/5; Step: 800/3557; Loss: 4.117975234985352\n",
            "Epoch: 1/5; Step: 805/3557; Loss: 3.853193521499634\n",
            "Epoch: 1/5; Step: 810/3557; Loss: 3.9126405715942383\n",
            "Epoch: 1/5; Step: 815/3557; Loss: 3.7962253093719482\n",
            "Epoch: 1/5; Step: 820/3557; Loss: 3.800278663635254\n",
            "Epoch: 1/5; Step: 825/3557; Loss: 3.812716484069824\n",
            "Epoch: 1/5; Step: 830/3557; Loss: 3.771146774291992\n",
            "Epoch: 1/5; Step: 835/3557; Loss: 3.7437143325805664\n",
            "Epoch: 1/5; Step: 840/3557; Loss: 3.9190590381622314\n",
            "Epoch: 1/5; Step: 845/3557; Loss: 3.7981009483337402\n",
            "Epoch: 1/5; Step: 850/3557; Loss: 3.810080051422119\n",
            "Epoch: 1/5; Step: 855/3557; Loss: 3.7181031703948975\n",
            "Epoch: 1/5; Step: 860/3557; Loss: 3.5249998569488525\n",
            "Epoch: 1/5; Step: 865/3557; Loss: 3.9491729736328125\n",
            "Epoch: 1/5; Step: 870/3557; Loss: 3.86329984664917\n",
            "Epoch: 1/5; Step: 875/3557; Loss: 3.6277763843536377\n",
            "Epoch: 1/5; Step: 880/3557; Loss: 3.620786666870117\n",
            "Epoch: 1/5; Step: 885/3557; Loss: 3.8827626705169678\n",
            "Epoch: 1/5; Step: 890/3557; Loss: 3.6891531944274902\n",
            "Epoch: 1/5; Step: 895/3557; Loss: 3.6585030555725098\n",
            "Epoch: 1/5; Step: 900/3557; Loss: 3.478076457977295\n",
            "Epoch: 1/5; Step: 905/3557; Loss: 3.695192813873291\n",
            "Epoch: 1/5; Step: 910/3557; Loss: 3.652951717376709\n",
            "Epoch: 1/5; Step: 915/3557; Loss: 3.5998311042785645\n",
            "Epoch: 1/5; Step: 920/3557; Loss: 3.643556594848633\n",
            "Epoch: 1/5; Step: 925/3557; Loss: 3.5288779735565186\n",
            "Epoch: 1/5; Step: 930/3557; Loss: 3.629359722137451\n",
            "Epoch: 1/5; Step: 935/3557; Loss: 3.711663007736206\n",
            "Epoch: 1/5; Step: 940/3557; Loss: 3.585921287536621\n",
            "Epoch: 1/5; Step: 945/3557; Loss: 3.5058581829071045\n",
            "Epoch: 1/5; Step: 950/3557; Loss: 3.6471056938171387\n",
            "Epoch: 1/5; Step: 955/3557; Loss: 3.47810435295105\n",
            "Epoch: 1/5; Step: 960/3557; Loss: 3.6075081825256348\n",
            "Epoch: 1/5; Step: 965/3557; Loss: 3.8150250911712646\n",
            "Epoch: 1/5; Step: 970/3557; Loss: 3.5812344551086426\n",
            "Epoch: 1/5; Step: 975/3557; Loss: 3.4420247077941895\n",
            "Epoch: 1/5; Step: 980/3557; Loss: 3.441105842590332\n",
            "Epoch: 1/5; Step: 985/3557; Loss: 3.441343307495117\n",
            "Epoch: 1/5; Step: 990/3557; Loss: 3.5280892848968506\n",
            "Epoch: 1/5; Step: 995/3557; Loss: 3.487839937210083\n",
            "Epoch: 1/5; Step: 1000/3557; Loss: 3.597344160079956\n",
            "Epoch: 1/5; Step: 1005/3557; Loss: 3.531952142715454\n",
            "Epoch: 1/5; Step: 1010/3557; Loss: 3.5146851539611816\n",
            "Epoch: 1/5; Step: 1015/3557; Loss: 3.2458395957946777\n",
            "Epoch: 1/5; Step: 1020/3557; Loss: 3.2261087894439697\n",
            "Epoch: 1/5; Step: 1025/3557; Loss: 3.357844829559326\n",
            "Epoch: 1/5; Step: 1030/3557; Loss: 3.3343567848205566\n",
            "Epoch: 1/5; Step: 1035/3557; Loss: 3.394454002380371\n",
            "Epoch: 1/5; Step: 1040/3557; Loss: 3.318178653717041\n",
            "Epoch: 1/5; Step: 1045/3557; Loss: 3.325152635574341\n",
            "Epoch: 1/5; Step: 1050/3557; Loss: 3.2759530544281006\n",
            "Epoch: 1/5; Step: 1055/3557; Loss: 3.2986111640930176\n",
            "Epoch: 1/5; Step: 1060/3557; Loss: 3.3807952404022217\n",
            "Epoch: 1/5; Step: 1065/3557; Loss: 3.074113130569458\n",
            "Epoch: 1/5; Step: 1070/3557; Loss: 3.40311598777771\n",
            "Epoch: 1/5; Step: 1075/3557; Loss: 3.3149445056915283\n",
            "Epoch: 1/5; Step: 1080/3557; Loss: 3.3669931888580322\n",
            "Epoch: 1/5; Step: 1085/3557; Loss: 3.319765090942383\n",
            "Epoch: 1/5; Step: 1090/3557; Loss: 3.26298451423645\n",
            "Epoch: 1/5; Step: 1095/3557; Loss: 3.3064849376678467\n",
            "Epoch: 1/5; Step: 1100/3557; Loss: 3.2612357139587402\n",
            "Epoch: 1/5; Step: 1105/3557; Loss: 3.120448589324951\n",
            "Epoch: 1/5; Step: 1110/3557; Loss: 3.110186815261841\n",
            "Epoch: 1/5; Step: 1115/3557; Loss: 3.1748206615448\n",
            "Epoch: 1/5; Step: 1120/3557; Loss: 3.4141907691955566\n",
            "Epoch: 1/5; Step: 1125/3557; Loss: 3.356081485748291\n",
            "Epoch: 1/5; Step: 1130/3557; Loss: 3.2577450275421143\n",
            "Epoch: 1/5; Step: 1135/3557; Loss: 3.131837844848633\n",
            "Epoch: 1/5; Step: 1140/3557; Loss: 3.066242218017578\n",
            "Epoch: 1/5; Step: 1145/3557; Loss: 3.1787381172180176\n",
            "Epoch: 1/5; Step: 1150/3557; Loss: 3.1530861854553223\n",
            "Epoch: 1/5; Step: 1155/3557; Loss: 3.179734230041504\n",
            "Epoch: 1/5; Step: 1160/3557; Loss: 2.987241506576538\n",
            "Epoch: 1/5; Step: 1165/3557; Loss: 3.254251003265381\n",
            "Epoch: 1/5; Step: 1170/3557; Loss: 3.2378058433532715\n",
            "Epoch: 1/5; Step: 1175/3557; Loss: 3.2005527019500732\n",
            "Epoch: 1/5; Step: 1180/3557; Loss: 3.053865432739258\n",
            "Epoch: 1/5; Step: 1185/3557; Loss: 2.866194009780884\n",
            "Epoch: 1/5; Step: 1190/3557; Loss: 3.2080302238464355\n",
            "Epoch: 1/5; Step: 1195/3557; Loss: 3.079359769821167\n",
            "Epoch: 1/5; Step: 1200/3557; Loss: 2.9069323539733887\n",
            "Epoch: 1/5; Step: 1205/3557; Loss: 3.0583810806274414\n",
            "Epoch: 1/5; Step: 1210/3557; Loss: 2.9675955772399902\n",
            "Epoch: 1/5; Step: 1215/3557; Loss: 3.0045411586761475\n",
            "Epoch: 1/5; Step: 1220/3557; Loss: 3.100625991821289\n",
            "Epoch: 1/5; Step: 1225/3557; Loss: 3.106580972671509\n",
            "Epoch: 1/5; Step: 1230/3557; Loss: 2.9753267765045166\n",
            "Epoch: 1/5; Step: 1235/3557; Loss: 2.9462242126464844\n",
            "Epoch: 1/5; Step: 1240/3557; Loss: 2.8337910175323486\n",
            "Epoch: 1/5; Step: 1245/3557; Loss: 2.952279806137085\n",
            "Epoch: 1/5; Step: 1250/3557; Loss: 2.921963691711426\n",
            "Epoch: 1/5; Step: 1255/3557; Loss: 2.874755620956421\n",
            "Epoch: 1/5; Step: 1260/3557; Loss: 2.748234987258911\n",
            "Epoch: 1/5; Step: 1265/3557; Loss: 3.0053067207336426\n",
            "Epoch: 1/5; Step: 1270/3557; Loss: 2.820641279220581\n",
            "Epoch: 1/5; Step: 1275/3557; Loss: 2.976992607116699\n",
            "Epoch: 1/5; Step: 1280/3557; Loss: 2.8093085289001465\n",
            "Epoch: 1/5; Step: 1285/3557; Loss: 3.1131787300109863\n",
            "Epoch: 1/5; Step: 1290/3557; Loss: 2.962132453918457\n",
            "Epoch: 1/5; Step: 1295/3557; Loss: 2.9285988807678223\n",
            "Epoch: 1/5; Step: 1300/3557; Loss: 3.0813498497009277\n",
            "Epoch: 1/5; Step: 1305/3557; Loss: 2.871157169342041\n",
            "Epoch: 1/5; Step: 1310/3557; Loss: 2.988811492919922\n",
            "Epoch: 1/5; Step: 1315/3557; Loss: 2.7803547382354736\n",
            "Epoch: 1/5; Step: 1320/3557; Loss: 2.7894949913024902\n",
            "Epoch: 1/5; Step: 1325/3557; Loss: 2.8305675983428955\n",
            "Epoch: 1/5; Step: 1330/3557; Loss: 2.737137794494629\n",
            "Epoch: 1/5; Step: 1335/3557; Loss: 2.7875566482543945\n",
            "Epoch: 1/5; Step: 1340/3557; Loss: 2.8218138217926025\n",
            "Epoch: 1/5; Step: 1345/3557; Loss: 2.866297721862793\n",
            "Epoch: 1/5; Step: 1350/3557; Loss: 2.7709388732910156\n",
            "Epoch: 1/5; Step: 1355/3557; Loss: 2.843712091445923\n",
            "Epoch: 1/5; Step: 1360/3557; Loss: 2.6857283115386963\n",
            "Epoch: 1/5; Step: 1365/3557; Loss: 2.8516886234283447\n",
            "Epoch: 1/5; Step: 1370/3557; Loss: 2.869555950164795\n",
            "Epoch: 1/5; Step: 1375/3557; Loss: 2.478402853012085\n",
            "Epoch: 1/5; Step: 1380/3557; Loss: 2.747565746307373\n",
            "Epoch: 1/5; Step: 1385/3557; Loss: 2.5504848957061768\n",
            "Epoch: 1/5; Step: 1390/3557; Loss: 2.8340282440185547\n",
            "Epoch: 1/5; Step: 1395/3557; Loss: 2.6288022994995117\n",
            "Epoch: 1/5; Step: 1400/3557; Loss: 2.5355451107025146\n",
            "Epoch: 1/5; Step: 1405/3557; Loss: 2.7341227531433105\n",
            "Epoch: 1/5; Step: 1410/3557; Loss: 2.730576276779175\n",
            "Epoch: 1/5; Step: 1415/3557; Loss: 2.7370808124542236\n",
            "Epoch: 1/5; Step: 1420/3557; Loss: 2.837639093399048\n",
            "Epoch: 1/5; Step: 1425/3557; Loss: 2.4176487922668457\n",
            "Epoch: 1/5; Step: 1430/3557; Loss: 2.6631314754486084\n",
            "Epoch: 1/5; Step: 1435/3557; Loss: 2.4609367847442627\n",
            "Epoch: 1/5; Step: 1440/3557; Loss: 2.4290382862091064\n",
            "Epoch: 1/5; Step: 1445/3557; Loss: 2.51470947265625\n",
            "Epoch: 1/5; Step: 1450/3557; Loss: 2.4641008377075195\n",
            "Epoch: 1/5; Step: 1455/3557; Loss: 2.6268815994262695\n",
            "Epoch: 1/5; Step: 1460/3557; Loss: 2.5427985191345215\n",
            "Epoch: 1/5; Step: 1465/3557; Loss: 2.5369937419891357\n",
            "Epoch: 1/5; Step: 1470/3557; Loss: 2.682365655899048\n",
            "Epoch: 1/5; Step: 1475/3557; Loss: 2.5847861766815186\n",
            "Epoch: 1/5; Step: 1480/3557; Loss: 2.5232861042022705\n",
            "Epoch: 1/5; Step: 1485/3557; Loss: 2.5902459621429443\n",
            "Epoch: 1/5; Step: 1490/3557; Loss: 2.6533749103546143\n",
            "Epoch: 1/5; Step: 1495/3557; Loss: 2.385554790496826\n",
            "Epoch: 1/5; Step: 1500/3557; Loss: 2.3790969848632812\n",
            "Epoch: 1/5; Step: 1505/3557; Loss: 2.4944698810577393\n",
            "Epoch: 1/5; Step: 1510/3557; Loss: 2.6169493198394775\n",
            "Epoch: 1/5; Step: 1515/3557; Loss: 2.4952425956726074\n",
            "Epoch: 1/5; Step: 1520/3557; Loss: 2.4941823482513428\n",
            "Epoch: 1/5; Step: 1525/3557; Loss: 2.6709530353546143\n",
            "Epoch: 1/5; Step: 1530/3557; Loss: 2.6057186126708984\n",
            "Epoch: 1/5; Step: 1535/3557; Loss: 2.527233600616455\n",
            "Epoch: 1/5; Step: 1540/3557; Loss: 2.544516086578369\n",
            "Epoch: 1/5; Step: 1545/3557; Loss: 2.4376132488250732\n",
            "Epoch: 1/5; Step: 1550/3557; Loss: 2.327631950378418\n",
            "Epoch: 1/5; Step: 1555/3557; Loss: 2.4857468605041504\n",
            "Epoch: 1/5; Step: 1560/3557; Loss: 2.432793140411377\n",
            "Epoch: 1/5; Step: 1565/3557; Loss: 2.490415573120117\n",
            "Epoch: 1/5; Step: 1570/3557; Loss: 2.308736801147461\n",
            "Epoch: 1/5; Step: 1575/3557; Loss: 2.430068016052246\n",
            "Epoch: 1/5; Step: 1580/3557; Loss: 2.304197311401367\n",
            "Epoch: 1/5; Step: 1585/3557; Loss: 2.457033157348633\n",
            "Epoch: 1/5; Step: 1590/3557; Loss: 2.4411675930023193\n",
            "Epoch: 1/5; Step: 1595/3557; Loss: 2.3472933769226074\n",
            "Epoch: 1/5; Step: 1600/3557; Loss: 2.43369460105896\n",
            "Epoch: 1/5; Step: 1605/3557; Loss: 2.4557478427886963\n",
            "Epoch: 1/5; Step: 1610/3557; Loss: 2.332036256790161\n",
            "Epoch: 1/5; Step: 1615/3557; Loss: 2.3430633544921875\n",
            "Epoch: 1/5; Step: 1620/3557; Loss: 2.354776382446289\n",
            "Epoch: 1/5; Step: 1625/3557; Loss: 2.4183859825134277\n",
            "Epoch: 1/5; Step: 1630/3557; Loss: 2.0792036056518555\n",
            "Epoch: 1/5; Step: 1635/3557; Loss: 2.3078818321228027\n",
            "Epoch: 1/5; Step: 1640/3557; Loss: 2.385725736618042\n",
            "Epoch: 1/5; Step: 1645/3557; Loss: 2.2042105197906494\n",
            "Epoch: 1/5; Step: 1650/3557; Loss: 2.2233998775482178\n",
            "Epoch: 1/5; Step: 1655/3557; Loss: 2.2651498317718506\n",
            "Epoch: 1/5; Step: 1660/3557; Loss: 2.2562003135681152\n",
            "Epoch: 1/5; Step: 1665/3557; Loss: 2.1837830543518066\n",
            "Epoch: 1/5; Step: 1670/3557; Loss: 2.159959554672241\n",
            "Epoch: 1/5; Step: 1675/3557; Loss: 2.141169786453247\n",
            "Epoch: 1/5; Step: 1680/3557; Loss: 2.3487257957458496\n",
            "Epoch: 1/5; Step: 1685/3557; Loss: 2.1727309226989746\n",
            "Epoch: 1/5; Step: 1690/3557; Loss: 2.2082738876342773\n",
            "Epoch: 1/5; Step: 1695/3557; Loss: 2.2223217487335205\n",
            "Epoch: 1/5; Step: 1700/3557; Loss: 2.273033857345581\n",
            "Epoch: 1/5; Step: 1705/3557; Loss: 2.229154586791992\n",
            "Epoch: 1/5; Step: 1710/3557; Loss: 2.1068427562713623\n",
            "Epoch: 1/5; Step: 1715/3557; Loss: 1.9608206748962402\n",
            "Epoch: 1/5; Step: 1720/3557; Loss: 2.2460877895355225\n",
            "Epoch: 1/5; Step: 1725/3557; Loss: 2.1981892585754395\n",
            "Epoch: 1/5; Step: 1730/3557; Loss: 2.0509448051452637\n",
            "Epoch: 1/5; Step: 1735/3557; Loss: 2.0882139205932617\n",
            "Epoch: 1/5; Step: 1740/3557; Loss: 2.0848922729492188\n",
            "Epoch: 1/5; Step: 1745/3557; Loss: 2.1106116771698\n",
            "Epoch: 1/5; Step: 1750/3557; Loss: 2.060749053955078\n",
            "Epoch: 1/5; Step: 1755/3557; Loss: 2.217888593673706\n",
            "Epoch: 1/5; Step: 1760/3557; Loss: 2.068464994430542\n",
            "Epoch: 1/5; Step: 1765/3557; Loss: 2.0491578578948975\n",
            "Epoch: 1/5; Step: 1770/3557; Loss: 1.971879482269287\n",
            "Epoch: 1/5; Step: 1775/3557; Loss: 2.0266120433807373\n",
            "Epoch: 1/5; Step: 1780/3557; Loss: 2.150007486343384\n",
            "Epoch: 1/5; Step: 1785/3557; Loss: 2.0397589206695557\n",
            "Epoch: 1/5; Step: 1790/3557; Loss: 2.037106990814209\n",
            "Epoch: 1/5; Step: 1795/3557; Loss: 2.079054355621338\n",
            "Epoch: 1/5; Step: 1800/3557; Loss: 2.0703964233398438\n",
            "Epoch: 1/5; Step: 1805/3557; Loss: 1.9563161134719849\n",
            "Epoch: 1/5; Step: 1810/3557; Loss: 2.08894419670105\n",
            "Epoch: 1/5; Step: 1815/3557; Loss: 1.9168705940246582\n",
            "Epoch: 1/5; Step: 1820/3557; Loss: 1.9286693334579468\n",
            "Epoch: 1/5; Step: 1825/3557; Loss: 2.0916552543640137\n",
            "Epoch: 1/5; Step: 1830/3557; Loss: 2.070685625076294\n",
            "Epoch: 1/5; Step: 1835/3557; Loss: 1.9293997287750244\n",
            "Epoch: 1/5; Step: 1840/3557; Loss: 2.049267292022705\n",
            "Epoch: 1/5; Step: 1845/3557; Loss: 1.911721110343933\n",
            "Epoch: 1/5; Step: 1850/3557; Loss: 2.0493171215057373\n",
            "Epoch: 1/5; Step: 1855/3557; Loss: 1.9912501573562622\n",
            "Epoch: 1/5; Step: 1860/3557; Loss: 2.0357913970947266\n",
            "Epoch: 1/5; Step: 1865/3557; Loss: 1.811206579208374\n",
            "Epoch: 1/5; Step: 1870/3557; Loss: 1.8521060943603516\n",
            "Epoch: 1/5; Step: 1875/3557; Loss: 1.9513098001480103\n",
            "Epoch: 1/5; Step: 1880/3557; Loss: 1.8404844999313354\n",
            "Epoch: 1/5; Step: 1885/3557; Loss: 1.8045215606689453\n",
            "Epoch: 1/5; Step: 1890/3557; Loss: 1.8271769285202026\n",
            "Epoch: 1/5; Step: 1895/3557; Loss: 1.8763480186462402\n",
            "Epoch: 1/5; Step: 1900/3557; Loss: 1.8015272617340088\n",
            "Epoch: 1/5; Step: 1905/3557; Loss: 1.890019416809082\n",
            "Epoch: 1/5; Step: 1910/3557; Loss: 1.8662827014923096\n",
            "Epoch: 1/5; Step: 1915/3557; Loss: 1.8394144773483276\n",
            "Epoch: 1/5; Step: 1920/3557; Loss: 1.8728761672973633\n",
            "Epoch: 1/5; Step: 1925/3557; Loss: 1.8214356899261475\n",
            "Epoch: 1/5; Step: 1930/3557; Loss: 1.8963555097579956\n",
            "Epoch: 1/5; Step: 1935/3557; Loss: 1.7618488073349\n",
            "Epoch: 1/5; Step: 1940/3557; Loss: 1.8539073467254639\n",
            "Epoch: 1/5; Step: 1945/3557; Loss: 1.9042390584945679\n",
            "Epoch: 1/5; Step: 1950/3557; Loss: 1.6735172271728516\n",
            "Epoch: 1/5; Step: 1955/3557; Loss: 1.8712748289108276\n",
            "Epoch: 1/5; Step: 1960/3557; Loss: 1.881719946861267\n",
            "Epoch: 1/5; Step: 1965/3557; Loss: 1.782163381576538\n",
            "Epoch: 1/5; Step: 1970/3557; Loss: 1.870867371559143\n",
            "Epoch: 1/5; Step: 1975/3557; Loss: 1.8214682340621948\n",
            "Epoch: 1/5; Step: 1980/3557; Loss: 1.8529692888259888\n",
            "Epoch: 1/5; Step: 1985/3557; Loss: 1.8160064220428467\n",
            "Epoch: 1/5; Step: 1990/3557; Loss: 1.7387624979019165\n",
            "Epoch: 1/5; Step: 1995/3557; Loss: 1.7635122537612915\n",
            "Epoch: 1/5; Step: 2000/3557; Loss: 1.7585943937301636\n",
            "Epoch: 1/5; Step: 2005/3557; Loss: 1.7401854991912842\n",
            "Epoch: 1/5; Step: 2010/3557; Loss: 1.7979331016540527\n",
            "Epoch: 1/5; Step: 2015/3557; Loss: 1.7376506328582764\n",
            "Epoch: 1/5; Step: 2020/3557; Loss: 1.8272069692611694\n",
            "Epoch: 1/5; Step: 2025/3557; Loss: 1.7971532344818115\n",
            "Epoch: 1/5; Step: 2030/3557; Loss: 1.6203787326812744\n",
            "Epoch: 1/5; Step: 2035/3557; Loss: 1.7034002542495728\n",
            "Epoch: 1/5; Step: 2040/3557; Loss: 1.730020523071289\n",
            "Epoch: 1/5; Step: 2045/3557; Loss: 1.703357458114624\n",
            "Epoch: 1/5; Step: 2050/3557; Loss: 1.7676095962524414\n",
            "Epoch: 1/5; Step: 2055/3557; Loss: 1.7385305166244507\n",
            "Epoch: 1/5; Step: 2060/3557; Loss: 1.6663366556167603\n",
            "Epoch: 1/5; Step: 2065/3557; Loss: 1.8126118183135986\n",
            "Epoch: 1/5; Step: 2070/3557; Loss: 1.7831332683563232\n",
            "Epoch: 1/5; Step: 2075/3557; Loss: 1.8337419033050537\n",
            "Epoch: 1/5; Step: 2080/3557; Loss: 1.5869109630584717\n",
            "Epoch: 1/5; Step: 2085/3557; Loss: 1.769411563873291\n",
            "Epoch: 1/5; Step: 2090/3557; Loss: 1.7075684070587158\n",
            "Epoch: 1/5; Step: 2095/3557; Loss: 1.7512710094451904\n",
            "Epoch: 1/5; Step: 2100/3557; Loss: 1.708471655845642\n",
            "Epoch: 1/5; Step: 2105/3557; Loss: 1.7180207967758179\n",
            "Epoch: 1/5; Step: 2110/3557; Loss: 1.7100638151168823\n",
            "Epoch: 1/5; Step: 2115/3557; Loss: 1.5773895978927612\n",
            "Epoch: 1/5; Step: 2120/3557; Loss: 1.599776029586792\n",
            "Epoch: 1/5; Step: 2125/3557; Loss: 1.709238052368164\n",
            "Epoch: 1/5; Step: 2130/3557; Loss: 1.5483968257904053\n",
            "Epoch: 1/5; Step: 2135/3557; Loss: 1.4756728410720825\n",
            "Epoch: 1/5; Step: 2140/3557; Loss: 1.5993691682815552\n",
            "Epoch: 1/5; Step: 2145/3557; Loss: 1.612544298171997\n",
            "Epoch: 1/5; Step: 2150/3557; Loss: 1.6156812906265259\n",
            "Epoch: 1/5; Step: 2155/3557; Loss: 1.7930973768234253\n",
            "Epoch: 1/5; Step: 2160/3557; Loss: 1.5955535173416138\n",
            "Epoch: 1/5; Step: 2165/3557; Loss: 1.587459683418274\n",
            "Epoch: 1/5; Step: 2170/3557; Loss: 1.6145694255828857\n",
            "Epoch: 1/5; Step: 2175/3557; Loss: 1.6098617315292358\n",
            "Epoch: 1/5; Step: 2180/3557; Loss: 1.557134985923767\n",
            "Epoch: 1/5; Step: 2185/3557; Loss: 1.6070735454559326\n",
            "Epoch: 1/5; Step: 2190/3557; Loss: 1.6610749959945679\n",
            "Epoch: 1/5; Step: 2195/3557; Loss: 1.5979318618774414\n",
            "Epoch: 1/5; Step: 2200/3557; Loss: 1.5408296585083008\n",
            "Epoch: 1/5; Step: 2205/3557; Loss: 1.6030079126358032\n",
            "Epoch: 1/5; Step: 2210/3557; Loss: 1.633686900138855\n",
            "Epoch: 1/5; Step: 2215/3557; Loss: 1.5745503902435303\n",
            "Epoch: 1/5; Step: 2220/3557; Loss: 1.681734323501587\n",
            "Epoch: 1/5; Step: 2225/3557; Loss: 1.591946005821228\n",
            "Epoch: 1/5; Step: 2230/3557; Loss: 1.5184078216552734\n",
            "Epoch: 1/5; Step: 2235/3557; Loss: 1.5822551250457764\n",
            "Epoch: 1/5; Step: 2240/3557; Loss: 1.6039178371429443\n",
            "Epoch: 1/5; Step: 2245/3557; Loss: 1.488889217376709\n",
            "Epoch: 1/5; Step: 2250/3557; Loss: 1.5958023071289062\n",
            "Epoch: 1/5; Step: 2255/3557; Loss: 1.5675764083862305\n",
            "Epoch: 1/5; Step: 2260/3557; Loss: 1.495315432548523\n",
            "Epoch: 1/5; Step: 2265/3557; Loss: 1.4750715494155884\n",
            "Epoch: 1/5; Step: 2270/3557; Loss: 1.473578929901123\n",
            "Epoch: 1/5; Step: 2275/3557; Loss: 1.409744143486023\n",
            "Epoch: 1/5; Step: 2280/3557; Loss: 1.4535777568817139\n",
            "Epoch: 1/5; Step: 2285/3557; Loss: 1.5226833820343018\n",
            "Epoch: 1/5; Step: 2290/3557; Loss: 1.657783031463623\n",
            "Epoch: 1/5; Step: 2295/3557; Loss: 1.6031413078308105\n",
            "Epoch: 1/5; Step: 2300/3557; Loss: 1.6437413692474365\n",
            "Epoch: 1/5; Step: 2305/3557; Loss: 1.4969732761383057\n",
            "Epoch: 1/5; Step: 2310/3557; Loss: 1.496898889541626\n",
            "Epoch: 1/5; Step: 2315/3557; Loss: 1.4479008913040161\n",
            "Epoch: 1/5; Step: 2320/3557; Loss: 1.4850656986236572\n",
            "Epoch: 1/5; Step: 2325/3557; Loss: 1.6123236417770386\n",
            "Epoch: 1/5; Step: 2330/3557; Loss: 1.4502753019332886\n",
            "Epoch: 1/5; Step: 2335/3557; Loss: 1.4074019193649292\n",
            "Epoch: 1/5; Step: 2340/3557; Loss: 1.4097213745117188\n",
            "Epoch: 1/5; Step: 2345/3557; Loss: 1.4300974607467651\n",
            "Epoch: 1/5; Step: 2350/3557; Loss: 1.4731882810592651\n",
            "Epoch: 1/5; Step: 2355/3557; Loss: 1.4956384897232056\n",
            "Epoch: 1/5; Step: 2360/3557; Loss: 1.4068024158477783\n",
            "Epoch: 1/5; Step: 2365/3557; Loss: 1.4386998414993286\n",
            "Epoch: 1/5; Step: 2370/3557; Loss: 1.519376277923584\n",
            "Epoch: 1/5; Step: 2375/3557; Loss: 1.4327460527420044\n",
            "Epoch: 1/5; Step: 2380/3557; Loss: 1.4292504787445068\n",
            "Epoch: 1/5; Step: 2385/3557; Loss: 1.4264646768569946\n",
            "Epoch: 1/5; Step: 2390/3557; Loss: 1.372758388519287\n",
            "Epoch: 1/5; Step: 2395/3557; Loss: 1.4127053022384644\n",
            "Epoch: 1/5; Step: 2400/3557; Loss: 1.4206575155258179\n",
            "Epoch: 1/5; Step: 2405/3557; Loss: 1.3902790546417236\n",
            "Epoch: 1/5; Step: 2410/3557; Loss: 1.4026153087615967\n",
            "Epoch: 1/5; Step: 2415/3557; Loss: 1.4956079721450806\n",
            "Epoch: 1/5; Step: 2420/3557; Loss: 1.5395612716674805\n",
            "Epoch: 1/5; Step: 2425/3557; Loss: 1.3971240520477295\n",
            "Epoch: 1/5; Step: 2430/3557; Loss: 1.4161337614059448\n",
            "Epoch: 1/5; Step: 2435/3557; Loss: 1.48032546043396\n",
            "Epoch: 1/5; Step: 2440/3557; Loss: 1.4846795797348022\n",
            "Epoch: 1/5; Step: 2445/3557; Loss: 1.461891531944275\n",
            "Epoch: 1/5; Step: 2450/3557; Loss: 1.487057089805603\n",
            "Epoch: 1/5; Step: 2455/3557; Loss: 1.3721113204956055\n",
            "Epoch: 1/5; Step: 2460/3557; Loss: 1.385380744934082\n",
            "Epoch: 1/5; Step: 2465/3557; Loss: 1.3145252466201782\n",
            "Epoch: 1/5; Step: 2470/3557; Loss: 1.3931971788406372\n",
            "Epoch: 1/5; Step: 2475/3557; Loss: 1.2834255695343018\n",
            "Epoch: 1/5; Step: 2480/3557; Loss: 1.3240946531295776\n",
            "Epoch: 1/5; Step: 2485/3557; Loss: 1.363242745399475\n",
            "Epoch: 1/5; Step: 2490/3557; Loss: 1.3350019454956055\n",
            "Epoch: 1/5; Step: 2495/3557; Loss: 1.3493961095809937\n",
            "Epoch: 1/5; Step: 2500/3557; Loss: 1.3975820541381836\n",
            "Epoch: 1/5; Step: 2505/3557; Loss: 1.3468122482299805\n",
            "Epoch: 1/5; Step: 2510/3557; Loss: 1.3927414417266846\n",
            "Epoch: 1/5; Step: 2515/3557; Loss: 1.4357422590255737\n",
            "Epoch: 1/5; Step: 2520/3557; Loss: 1.3345826864242554\n",
            "Epoch: 1/5; Step: 2525/3557; Loss: 1.1956040859222412\n",
            "Epoch: 1/5; Step: 2530/3557; Loss: 1.3146412372589111\n",
            "Epoch: 1/5; Step: 2535/3557; Loss: 1.3635822534561157\n",
            "Epoch: 1/5; Step: 2540/3557; Loss: 1.2942543029785156\n",
            "Epoch: 1/5; Step: 2545/3557; Loss: 1.325069785118103\n",
            "Epoch: 1/5; Step: 2550/3557; Loss: 1.4332575798034668\n",
            "Epoch: 1/5; Step: 2555/3557; Loss: 1.4091252088546753\n",
            "Epoch: 1/5; Step: 2560/3557; Loss: 1.4269323348999023\n",
            "Epoch: 1/5; Step: 2565/3557; Loss: 1.285872459411621\n",
            "Epoch: 1/5; Step: 2570/3557; Loss: 1.264754056930542\n",
            "Epoch: 1/5; Step: 2575/3557; Loss: 1.3460444211959839\n",
            "Epoch: 1/5; Step: 2580/3557; Loss: 1.1592317819595337\n",
            "Epoch: 1/5; Step: 2585/3557; Loss: 1.2822480201721191\n",
            "Epoch: 1/5; Step: 2590/3557; Loss: 1.318265438079834\n",
            "Epoch: 1/5; Step: 2595/3557; Loss: 1.3880375623703003\n",
            "Epoch: 1/5; Step: 2600/3557; Loss: 1.399506688117981\n",
            "Epoch: 1/5; Step: 2605/3557; Loss: 1.3102432489395142\n",
            "Epoch: 1/5; Step: 2610/3557; Loss: 1.3094183206558228\n",
            "Epoch: 1/5; Step: 2615/3557; Loss: 1.180965781211853\n",
            "Epoch: 1/5; Step: 2620/3557; Loss: 1.343907356262207\n",
            "Epoch: 1/5; Step: 2625/3557; Loss: 1.2397305965423584\n",
            "Epoch: 1/5; Step: 2630/3557; Loss: 1.2858880758285522\n",
            "Epoch: 1/5; Step: 2635/3557; Loss: 1.342769742012024\n",
            "Epoch: 1/5; Step: 2640/3557; Loss: 1.2077065706253052\n",
            "Epoch: 1/5; Step: 2645/3557; Loss: 1.3254475593566895\n",
            "Epoch: 1/5; Step: 2650/3557; Loss: 1.2827144861221313\n",
            "Epoch: 1/5; Step: 2655/3557; Loss: 1.2711125612258911\n",
            "Epoch: 1/5; Step: 2660/3557; Loss: 1.325452208518982\n",
            "Epoch: 1/5; Step: 2665/3557; Loss: 1.2326503992080688\n",
            "Epoch: 1/5; Step: 2670/3557; Loss: 1.3424869775772095\n",
            "Epoch: 1/5; Step: 2675/3557; Loss: 1.2206109762191772\n",
            "Epoch: 1/5; Step: 2680/3557; Loss: 1.241762638092041\n",
            "Epoch: 1/5; Step: 2685/3557; Loss: 1.2813102006912231\n",
            "Epoch: 1/5; Step: 2690/3557; Loss: 1.3411896228790283\n",
            "Epoch: 1/5; Step: 2695/3557; Loss: 1.2932424545288086\n",
            "Epoch: 1/5; Step: 2700/3557; Loss: 1.2634481191635132\n",
            "Epoch: 1/5; Step: 2705/3557; Loss: 1.1713100671768188\n",
            "Epoch: 1/5; Step: 2710/3557; Loss: 1.2193326950073242\n",
            "Epoch: 1/5; Step: 2715/3557; Loss: 1.3274692296981812\n",
            "Epoch: 1/5; Step: 2720/3557; Loss: 1.3798487186431885\n",
            "Epoch: 1/5; Step: 2725/3557; Loss: 1.3893588781356812\n",
            "Epoch: 1/5; Step: 2730/3557; Loss: 1.3453623056411743\n",
            "Epoch: 1/5; Step: 2735/3557; Loss: 1.1545261144638062\n",
            "Epoch: 1/5; Step: 2740/3557; Loss: 1.1496942043304443\n",
            "Epoch: 1/5; Step: 2745/3557; Loss: 1.1537739038467407\n",
            "Epoch: 1/5; Step: 2750/3557; Loss: 1.217495083808899\n",
            "Epoch: 1/5; Step: 2755/3557; Loss: 1.3264415264129639\n",
            "Epoch: 1/5; Step: 2760/3557; Loss: 1.2410920858383179\n",
            "Epoch: 1/5; Step: 2765/3557; Loss: 1.1482418775558472\n",
            "Epoch: 1/5; Step: 2770/3557; Loss: 1.2150665521621704\n",
            "Epoch: 1/5; Step: 2775/3557; Loss: 1.195730447769165\n",
            "Epoch: 1/5; Step: 2780/3557; Loss: 1.074426293373108\n",
            "Epoch: 1/5; Step: 2785/3557; Loss: 1.1516389846801758\n",
            "Epoch: 1/5; Step: 2790/3557; Loss: 1.125733494758606\n",
            "Epoch: 1/5; Step: 2795/3557; Loss: 1.135326623916626\n",
            "Epoch: 1/5; Step: 2800/3557; Loss: 1.1149821281433105\n",
            "Epoch: 1/5; Step: 2805/3557; Loss: 1.1069086790084839\n",
            "Epoch: 1/5; Step: 2810/3557; Loss: 1.203710913658142\n",
            "Epoch: 1/5; Step: 2815/3557; Loss: 1.2929185628890991\n",
            "Epoch: 1/5; Step: 2820/3557; Loss: 1.2346956729888916\n",
            "Epoch: 1/5; Step: 2825/3557; Loss: 1.2277871370315552\n",
            "Epoch: 1/5; Step: 2830/3557; Loss: 1.182918906211853\n",
            "Epoch: 1/5; Step: 2835/3557; Loss: 1.1940219402313232\n",
            "Epoch: 1/5; Step: 2840/3557; Loss: 1.0952773094177246\n",
            "Epoch: 1/5; Step: 2845/3557; Loss: 1.2527179718017578\n",
            "Epoch: 1/5; Step: 2850/3557; Loss: 1.245469093322754\n",
            "Epoch: 1/5; Step: 2855/3557; Loss: 1.2172532081604004\n",
            "Epoch: 1/5; Step: 2860/3557; Loss: 1.208145260810852\n",
            "Epoch: 1/5; Step: 2865/3557; Loss: 1.214680552482605\n",
            "Epoch: 1/5; Step: 2870/3557; Loss: 1.192270040512085\n",
            "Epoch: 1/5; Step: 2875/3557; Loss: 1.2791320085525513\n",
            "Epoch: 1/5; Step: 2880/3557; Loss: 1.2057849168777466\n",
            "Epoch: 1/5; Step: 2885/3557; Loss: 1.3069993257522583\n",
            "Epoch: 1/5; Step: 2890/3557; Loss: 1.0670274496078491\n",
            "Epoch: 1/5; Step: 2895/3557; Loss: 1.1925889253616333\n",
            "Epoch: 1/5; Step: 2900/3557; Loss: 1.0634377002716064\n",
            "Epoch: 1/5; Step: 2905/3557; Loss: 1.2489500045776367\n",
            "Epoch: 1/5; Step: 2910/3557; Loss: 1.108609676361084\n",
            "Epoch: 1/5; Step: 2915/3557; Loss: 1.139764666557312\n",
            "Epoch: 1/5; Step: 2920/3557; Loss: 1.1966753005981445\n",
            "Epoch: 1/5; Step: 2925/3557; Loss: 1.0768468379974365\n",
            "Epoch: 1/5; Step: 2930/3557; Loss: 1.1441890001296997\n",
            "Epoch: 1/5; Step: 2935/3557; Loss: 1.1722012758255005\n",
            "Epoch: 1/5; Step: 2940/3557; Loss: 1.0458780527114868\n",
            "Epoch: 1/5; Step: 2945/3557; Loss: 1.187747836112976\n",
            "Epoch: 1/5; Step: 2950/3557; Loss: 1.1308213472366333\n",
            "Epoch: 1/5; Step: 2955/3557; Loss: 1.2183256149291992\n",
            "Epoch: 1/5; Step: 2960/3557; Loss: 1.1539863348007202\n",
            "Epoch: 1/5; Step: 2965/3557; Loss: 1.155813217163086\n",
            "Epoch: 1/5; Step: 2970/3557; Loss: 1.142194390296936\n",
            "Epoch: 1/5; Step: 2975/3557; Loss: 1.1107197999954224\n",
            "Epoch: 1/5; Step: 2980/3557; Loss: 1.2160539627075195\n",
            "Epoch: 1/5; Step: 2985/3557; Loss: 1.0480066537857056\n",
            "Epoch: 1/5; Step: 2990/3557; Loss: 1.1277045011520386\n",
            "Epoch: 1/5; Step: 2995/3557; Loss: 1.1774266958236694\n",
            "Epoch: 1/5; Step: 3000/3557; Loss: 1.1618942022323608\n",
            "Epoch: 1/5; Step: 3005/3557; Loss: 1.1160341501235962\n",
            "Epoch: 1/5; Step: 3010/3557; Loss: 1.323439598083496\n",
            "Epoch: 1/5; Step: 3015/3557; Loss: 1.208176851272583\n",
            "Epoch: 1/5; Step: 3020/3557; Loss: 1.1613152027130127\n",
            "Epoch: 1/5; Step: 3025/3557; Loss: 1.06989586353302\n",
            "Epoch: 1/5; Step: 3030/3557; Loss: 1.1480261087417603\n",
            "Epoch: 1/5; Step: 3035/3557; Loss: 1.1123380661010742\n",
            "Epoch: 1/5; Step: 3040/3557; Loss: 1.147078037261963\n",
            "Epoch: 1/5; Step: 3045/3557; Loss: 1.0717098712921143\n",
            "Epoch: 1/5; Step: 3050/3557; Loss: 1.1121606826782227\n",
            "Epoch: 1/5; Step: 3055/3557; Loss: 1.1513868570327759\n",
            "Epoch: 1/5; Step: 3060/3557; Loss: 1.2365144491195679\n",
            "Epoch: 1/5; Step: 3065/3557; Loss: 1.031380295753479\n",
            "Epoch: 1/5; Step: 3070/3557; Loss: 1.1214903593063354\n",
            "Epoch: 1/5; Step: 3075/3557; Loss: 1.3148635625839233\n",
            "Epoch: 1/5; Step: 3080/3557; Loss: 1.0900384187698364\n",
            "Epoch: 1/5; Step: 3085/3557; Loss: 0.9660703539848328\n",
            "Epoch: 1/5; Step: 3090/3557; Loss: 1.0603591203689575\n",
            "Epoch: 1/5; Step: 3095/3557; Loss: 0.9887745976448059\n",
            "Epoch: 1/5; Step: 3100/3557; Loss: 1.0621205568313599\n",
            "Epoch: 1/5; Step: 3105/3557; Loss: 1.1696275472640991\n",
            "Epoch: 1/5; Step: 3110/3557; Loss: 1.1028428077697754\n",
            "Epoch: 1/5; Step: 3115/3557; Loss: 1.1192708015441895\n",
            "Epoch: 1/5; Step: 3120/3557; Loss: 1.1279664039611816\n",
            "Epoch: 1/5; Step: 3125/3557; Loss: 1.0972769260406494\n",
            "Epoch: 1/5; Step: 3130/3557; Loss: 1.029476284980774\n",
            "Epoch: 1/5; Step: 3135/3557; Loss: 1.0111513137817383\n",
            "Epoch: 1/5; Step: 3140/3557; Loss: 1.17790687084198\n",
            "Epoch: 1/5; Step: 3145/3557; Loss: 0.9769009947776794\n",
            "Epoch: 1/5; Step: 3150/3557; Loss: 1.1833072900772095\n",
            "Epoch: 1/5; Step: 3155/3557; Loss: 0.9914403557777405\n",
            "Epoch: 1/5; Step: 3160/3557; Loss: 0.9876102805137634\n",
            "Epoch: 1/5; Step: 3165/3557; Loss: 1.048276662826538\n",
            "Epoch: 1/5; Step: 3170/3557; Loss: 1.2605279684066772\n",
            "Epoch: 1/5; Step: 3175/3557; Loss: 1.0065537691116333\n",
            "Epoch: 1/5; Step: 3180/3557; Loss: 1.0158919095993042\n",
            "Epoch: 1/5; Step: 3185/3557; Loss: 1.0661569833755493\n",
            "Epoch: 1/5; Step: 3190/3557; Loss: 1.1921610832214355\n",
            "Epoch: 1/5; Step: 3195/3557; Loss: 1.1231261491775513\n",
            "Epoch: 1/5; Step: 3200/3557; Loss: 1.1203523874282837\n",
            "Epoch: 1/5; Step: 3205/3557; Loss: 1.1127500534057617\n",
            "Epoch: 1/5; Step: 3210/3557; Loss: 1.101723074913025\n",
            "Epoch: 1/5; Step: 3215/3557; Loss: 1.0480698347091675\n",
            "Epoch: 1/5; Step: 3220/3557; Loss: 1.0184741020202637\n",
            "Epoch: 1/5; Step: 3225/3557; Loss: 0.9905199408531189\n",
            "Epoch: 1/5; Step: 3230/3557; Loss: 1.0690919160842896\n",
            "Epoch: 1/5; Step: 3235/3557; Loss: 0.9599922895431519\n",
            "Epoch: 1/5; Step: 3240/3557; Loss: 1.0166683197021484\n",
            "Epoch: 1/5; Step: 3245/3557; Loss: 0.8663976192474365\n",
            "Epoch: 1/5; Step: 3250/3557; Loss: 1.0851373672485352\n",
            "Epoch: 1/5; Step: 3255/3557; Loss: 1.0374714136123657\n",
            "Epoch: 1/5; Step: 3260/3557; Loss: 0.9549283385276794\n",
            "Epoch: 1/5; Step: 3265/3557; Loss: 0.9131657481193542\n",
            "Epoch: 1/5; Step: 3270/3557; Loss: 0.987462043762207\n",
            "Epoch: 1/5; Step: 3275/3557; Loss: 1.0263164043426514\n",
            "Epoch: 1/5; Step: 3280/3557; Loss: 1.0461231470108032\n",
            "Epoch: 1/5; Step: 3285/3557; Loss: 0.9777597784996033\n",
            "Epoch: 1/5; Step: 3290/3557; Loss: 1.1265901327133179\n",
            "Epoch: 1/5; Step: 3295/3557; Loss: 0.9272629022598267\n",
            "Epoch: 1/5; Step: 3300/3557; Loss: 1.0532876253128052\n",
            "Epoch: 1/5; Step: 3305/3557; Loss: 0.9820594191551208\n",
            "Epoch: 1/5; Step: 3310/3557; Loss: 1.0707329511642456\n",
            "Epoch: 1/5; Step: 3315/3557; Loss: 1.0107121467590332\n",
            "Epoch: 1/5; Step: 3320/3557; Loss: 1.035688877105713\n",
            "Epoch: 1/5; Step: 3325/3557; Loss: 0.9645949602127075\n",
            "Epoch: 1/5; Step: 3330/3557; Loss: 1.0640599727630615\n",
            "Epoch: 1/5; Step: 3335/3557; Loss: 1.010792851448059\n",
            "Epoch: 1/5; Step: 3340/3557; Loss: 0.9870386719703674\n",
            "Epoch: 1/5; Step: 3345/3557; Loss: 0.9916626214981079\n",
            "Epoch: 1/5; Step: 3350/3557; Loss: 0.9331341981887817\n",
            "Epoch: 1/5; Step: 3355/3557; Loss: 1.0681694746017456\n",
            "Epoch: 1/5; Step: 3360/3557; Loss: 0.959972083568573\n",
            "Epoch: 1/5; Step: 3365/3557; Loss: 1.0321959257125854\n",
            "Epoch: 1/5; Step: 3370/3557; Loss: 1.0208308696746826\n",
            "Epoch: 1/5; Step: 3375/3557; Loss: 0.9739319682121277\n",
            "Epoch: 1/5; Step: 3380/3557; Loss: 0.8863346576690674\n",
            "Epoch: 1/5; Step: 3385/3557; Loss: 1.045628547668457\n",
            "Epoch: 1/5; Step: 3390/3557; Loss: 0.8872683048248291\n",
            "Epoch: 1/5; Step: 3395/3557; Loss: 1.0303078889846802\n",
            "Epoch: 1/5; Step: 3400/3557; Loss: 1.0717955827713013\n",
            "Epoch: 1/5; Step: 3405/3557; Loss: 0.9929158687591553\n",
            "Epoch: 1/5; Step: 3410/3557; Loss: 0.9790334105491638\n",
            "Epoch: 1/5; Step: 3415/3557; Loss: 1.037704586982727\n",
            "Epoch: 1/5; Step: 3420/3557; Loss: 1.0128024816513062\n",
            "Epoch: 1/5; Step: 3425/3557; Loss: 1.024144172668457\n",
            "Epoch: 1/5; Step: 3430/3557; Loss: 0.9471936225891113\n",
            "Epoch: 1/5; Step: 3435/3557; Loss: 0.9515756368637085\n",
            "Epoch: 1/5; Step: 3440/3557; Loss: 1.1302247047424316\n",
            "Epoch: 1/5; Step: 3445/3557; Loss: 0.966606855392456\n",
            "Epoch: 1/5; Step: 3450/3557; Loss: 0.9505412578582764\n",
            "Epoch: 1/5; Step: 3455/3557; Loss: 0.9636531472206116\n",
            "Epoch: 1/5; Step: 3460/3557; Loss: 0.929350733757019\n",
            "Epoch: 1/5; Step: 3465/3557; Loss: 0.9484708905220032\n",
            "Epoch: 1/5; Step: 3470/3557; Loss: 1.0054506063461304\n",
            "Epoch: 1/5; Step: 3475/3557; Loss: 0.9417594075202942\n",
            "Epoch: 1/5; Step: 3480/3557; Loss: 1.0081332921981812\n",
            "Epoch: 1/5; Step: 3485/3557; Loss: 0.888649046421051\n",
            "Epoch: 1/5; Step: 3490/3557; Loss: 0.9483730792999268\n",
            "Epoch: 1/5; Step: 3495/3557; Loss: 0.8846408128738403\n",
            "Epoch: 1/5; Step: 3500/3557; Loss: 0.9849868416786194\n",
            "Epoch: 1/5; Step: 3505/3557; Loss: 1.0191476345062256\n",
            "Epoch: 1/5; Step: 3510/3557; Loss: 0.9716218709945679\n",
            "Epoch: 1/5; Step: 3515/3557; Loss: 0.9222279787063599\n",
            "Epoch: 1/5; Step: 3520/3557; Loss: 0.8599972724914551\n",
            "Epoch: 1/5; Step: 3525/3557; Loss: 0.9146748781204224\n",
            "Epoch: 1/5; Step: 3530/3557; Loss: 1.0747376680374146\n",
            "Epoch: 1/5; Step: 3535/3557; Loss: 0.9456117749214172\n",
            "Epoch: 1/5; Step: 3540/3557; Loss: 1.041006088256836\n",
            "Epoch: 1/5; Step: 3545/3557; Loss: 1.0290658473968506\n",
            "Epoch: 1/5; Step: 3550/3557; Loss: 0.8950452208518982\n",
            "Epoch: 1/5; Step: 3555/3557; Loss: 0.9713549613952637\n",
            "Epoch 1, Training Loss: 1.0444327592849731\n",
            "Epoch 1, Avg loss: 6.890966326291444, Generated Text: Jam jest Jacek - \" Mów pan Sędzia - żeby to uszło na sucho, Za każdą głowę tysiąc rubelków gotówką : Tysiąc rubelków Sędzio, to ostatnie słówko. \" Sędzia chciał targować się ; lecz Major nie przebudził dzwonek z Robak, dobywa rapier w ustach, to ostatnie słówko. \" Sędzia chciał targować się ; lecz Major nie słuchał, strasznie zadziwiony. \" \" Oj, jegry w milczenie miał słuch bardzo czuły, Gerwazy wołał, lecz zagłuszył. \" Oj, ale kapitanie - Przerwał Konewka -\n",
            "Epoch: 2/5; Step: 0/3557; Loss: 0.9230621457099915\n",
            "Epoch: 2/5; Step: 5/3557; Loss: 0.938972532749176\n",
            "Epoch: 2/5; Step: 10/3557; Loss: 0.9485201239585876\n",
            "Epoch: 2/5; Step: 15/3557; Loss: 0.9054571390151978\n",
            "Epoch: 2/5; Step: 20/3557; Loss: 0.9493902325630188\n",
            "Epoch: 2/5; Step: 25/3557; Loss: 0.9930658340454102\n",
            "Epoch: 2/5; Step: 30/3557; Loss: 0.9496932029724121\n",
            "Epoch: 2/5; Step: 35/3557; Loss: 0.9432973861694336\n",
            "Epoch: 2/5; Step: 40/3557; Loss: 0.8939918875694275\n",
            "Epoch: 2/5; Step: 45/3557; Loss: 0.9513023495674133\n",
            "Epoch: 2/5; Step: 50/3557; Loss: 0.8909850716590881\n",
            "Epoch: 2/5; Step: 55/3557; Loss: 0.96291583776474\n",
            "Epoch: 2/5; Step: 60/3557; Loss: 0.8783301711082458\n",
            "Epoch: 2/5; Step: 65/3557; Loss: 0.8258366584777832\n",
            "Epoch: 2/5; Step: 70/3557; Loss: 0.920743465423584\n",
            "Epoch: 2/5; Step: 75/3557; Loss: 0.9778804183006287\n",
            "Epoch: 2/5; Step: 80/3557; Loss: 0.9096113443374634\n",
            "Epoch: 2/5; Step: 85/3557; Loss: 0.8708383440971375\n",
            "Epoch: 2/5; Step: 90/3557; Loss: 0.9768307209014893\n",
            "Epoch: 2/5; Step: 95/3557; Loss: 0.890404999256134\n",
            "Epoch: 2/5; Step: 100/3557; Loss: 0.8388644456863403\n",
            "Epoch: 2/5; Step: 105/3557; Loss: 0.8859260678291321\n",
            "Epoch: 2/5; Step: 110/3557; Loss: 1.001108169555664\n",
            "Epoch: 2/5; Step: 115/3557; Loss: 0.9415448904037476\n",
            "Epoch: 2/5; Step: 120/3557; Loss: 0.9401575326919556\n",
            "Epoch: 2/5; Step: 125/3557; Loss: 0.851445734500885\n",
            "Epoch: 2/5; Step: 130/3557; Loss: 0.9296181201934814\n",
            "Epoch: 2/5; Step: 135/3557; Loss: 0.9874098896980286\n",
            "Epoch: 2/5; Step: 140/3557; Loss: 0.8389137387275696\n",
            "Epoch: 2/5; Step: 145/3557; Loss: 0.9306304454803467\n",
            "Epoch: 2/5; Step: 150/3557; Loss: 0.8371133804321289\n",
            "Epoch: 2/5; Step: 155/3557; Loss: 0.897548496723175\n",
            "Epoch: 2/5; Step: 160/3557; Loss: 0.9110417366027832\n",
            "Epoch: 2/5; Step: 165/3557; Loss: 0.8451184034347534\n",
            "Epoch: 2/5; Step: 170/3557; Loss: 0.8753461837768555\n",
            "Epoch: 2/5; Step: 175/3557; Loss: 0.8334785103797913\n",
            "Epoch: 2/5; Step: 180/3557; Loss: 0.8754765391349792\n",
            "Epoch: 2/5; Step: 185/3557; Loss: 0.9930934309959412\n",
            "Epoch: 2/5; Step: 190/3557; Loss: 0.8717933297157288\n",
            "Epoch: 2/5; Step: 195/3557; Loss: 0.9940620064735413\n",
            "Epoch: 2/5; Step: 200/3557; Loss: 0.866028368473053\n",
            "Epoch: 2/5; Step: 205/3557; Loss: 0.8872329592704773\n",
            "Epoch: 2/5; Step: 210/3557; Loss: 0.8779354691505432\n",
            "Epoch: 2/5; Step: 215/3557; Loss: 0.928626537322998\n",
            "Epoch: 2/5; Step: 220/3557; Loss: 0.8953242301940918\n",
            "Epoch: 2/5; Step: 225/3557; Loss: 0.9097269177436829\n",
            "Epoch: 2/5; Step: 230/3557; Loss: 0.991046667098999\n",
            "Epoch: 2/5; Step: 235/3557; Loss: 0.9337030649185181\n",
            "Epoch: 2/5; Step: 240/3557; Loss: 0.866276741027832\n",
            "Epoch: 2/5; Step: 245/3557; Loss: 0.8722633719444275\n",
            "Epoch: 2/5; Step: 250/3557; Loss: 0.9599419236183167\n",
            "Epoch: 2/5; Step: 255/3557; Loss: 0.8657411336898804\n",
            "Epoch: 2/5; Step: 260/3557; Loss: 0.9303080439567566\n",
            "Epoch: 2/5; Step: 265/3557; Loss: 0.8168343305587769\n",
            "Epoch: 2/5; Step: 270/3557; Loss: 0.7704134583473206\n",
            "Epoch: 2/5; Step: 275/3557; Loss: 0.8770802617073059\n",
            "Epoch: 2/5; Step: 280/3557; Loss: 0.8115191459655762\n",
            "Epoch: 2/5; Step: 285/3557; Loss: 0.8698440790176392\n",
            "Epoch: 2/5; Step: 290/3557; Loss: 0.9053473472595215\n",
            "Epoch: 2/5; Step: 295/3557; Loss: 0.8758328557014465\n",
            "Epoch: 2/5; Step: 300/3557; Loss: 0.8676226735115051\n",
            "Epoch: 2/5; Step: 305/3557; Loss: 0.8552493453025818\n",
            "Epoch: 2/5; Step: 310/3557; Loss: 0.8482887148857117\n",
            "Epoch: 2/5; Step: 315/3557; Loss: 0.8648473620414734\n",
            "Epoch: 2/5; Step: 320/3557; Loss: 0.9050855040550232\n",
            "Epoch: 2/5; Step: 325/3557; Loss: 0.8776159286499023\n",
            "Epoch: 2/5; Step: 330/3557; Loss: 0.8579567670822144\n",
            "Epoch: 2/5; Step: 335/3557; Loss: 0.8729054927825928\n",
            "Epoch: 2/5; Step: 340/3557; Loss: 0.8140760064125061\n",
            "Epoch: 2/5; Step: 345/3557; Loss: 0.8726232051849365\n",
            "Epoch: 2/5; Step: 350/3557; Loss: 0.8124014139175415\n",
            "Epoch: 2/5; Step: 355/3557; Loss: 0.8957306742668152\n",
            "Epoch: 2/5; Step: 360/3557; Loss: 0.8281447887420654\n",
            "Epoch: 2/5; Step: 365/3557; Loss: 0.9099578857421875\n",
            "Epoch: 2/5; Step: 370/3557; Loss: 0.9868608713150024\n",
            "Epoch: 2/5; Step: 375/3557; Loss: 0.8314695358276367\n",
            "Epoch: 2/5; Step: 380/3557; Loss: 0.8701711893081665\n",
            "Epoch: 2/5; Step: 385/3557; Loss: 0.8417516350746155\n",
            "Epoch: 2/5; Step: 390/3557; Loss: 0.8679877519607544\n",
            "Epoch: 2/5; Step: 395/3557; Loss: 0.8295012712478638\n",
            "Epoch: 2/5; Step: 400/3557; Loss: 0.9087105393409729\n",
            "Epoch: 2/5; Step: 405/3557; Loss: 0.9358747005462646\n",
            "Epoch: 2/5; Step: 410/3557; Loss: 0.8854197263717651\n",
            "Epoch: 2/5; Step: 415/3557; Loss: 0.861201286315918\n",
            "Epoch: 2/5; Step: 420/3557; Loss: 0.8142955303192139\n",
            "Epoch: 2/5; Step: 425/3557; Loss: 0.8354275226593018\n",
            "Epoch: 2/5; Step: 430/3557; Loss: 0.8610608577728271\n",
            "Epoch: 2/5; Step: 435/3557; Loss: 0.7928068041801453\n",
            "Epoch: 2/5; Step: 440/3557; Loss: 0.9454784989356995\n",
            "Epoch: 2/5; Step: 445/3557; Loss: 0.8776003122329712\n",
            "Epoch: 2/5; Step: 450/3557; Loss: 0.7951359152793884\n",
            "Epoch: 2/5; Step: 455/3557; Loss: 0.8719466328620911\n",
            "Epoch: 2/5; Step: 460/3557; Loss: 0.8790166974067688\n",
            "Epoch: 2/5; Step: 465/3557; Loss: 0.917705237865448\n",
            "Epoch: 2/5; Step: 470/3557; Loss: 0.8348193764686584\n",
            "Epoch: 2/5; Step: 475/3557; Loss: 0.7965202331542969\n",
            "Epoch: 2/5; Step: 480/3557; Loss: 0.7592349648475647\n",
            "Epoch: 2/5; Step: 485/3557; Loss: 0.9154823422431946\n",
            "Epoch: 2/5; Step: 490/3557; Loss: 0.8538332581520081\n",
            "Epoch: 2/5; Step: 495/3557; Loss: 0.8990630507469177\n",
            "Epoch: 2/5; Step: 500/3557; Loss: 0.9015921950340271\n",
            "Epoch: 2/5; Step: 505/3557; Loss: 0.8287793397903442\n",
            "Epoch: 2/5; Step: 510/3557; Loss: 0.905552089214325\n",
            "Epoch: 2/5; Step: 515/3557; Loss: 0.873817503452301\n",
            "Epoch: 2/5; Step: 520/3557; Loss: 0.837664783000946\n",
            "Epoch: 2/5; Step: 525/3557; Loss: 0.9062449336051941\n",
            "Epoch: 2/5; Step: 530/3557; Loss: 0.833024799823761\n",
            "Epoch: 2/5; Step: 535/3557; Loss: 0.8043723106384277\n",
            "Epoch: 2/5; Step: 540/3557; Loss: 0.7998237609863281\n",
            "Epoch: 2/5; Step: 545/3557; Loss: 0.8639019727706909\n",
            "Epoch: 2/5; Step: 550/3557; Loss: 0.8778771162033081\n",
            "Epoch: 2/5; Step: 555/3557; Loss: 0.8068172931671143\n",
            "Epoch: 2/5; Step: 560/3557; Loss: 0.83079993724823\n",
            "Epoch: 2/5; Step: 565/3557; Loss: 0.8329010605812073\n",
            "Epoch: 2/5; Step: 570/3557; Loss: 0.8359993696212769\n",
            "Epoch: 2/5; Step: 575/3557; Loss: 0.7738654613494873\n",
            "Epoch: 2/5; Step: 580/3557; Loss: 0.8578178882598877\n",
            "Epoch: 2/5; Step: 585/3557; Loss: 0.8352275490760803\n",
            "Epoch: 2/5; Step: 590/3557; Loss: 0.7970031499862671\n",
            "Epoch: 2/5; Step: 595/3557; Loss: 0.8709424138069153\n",
            "Epoch: 2/5; Step: 600/3557; Loss: 0.827396810054779\n",
            "Epoch: 2/5; Step: 605/3557; Loss: 0.7935567498207092\n",
            "Epoch: 2/5; Step: 610/3557; Loss: 0.8310549259185791\n",
            "Epoch: 2/5; Step: 615/3557; Loss: 0.8299181461334229\n",
            "Epoch: 2/5; Step: 620/3557; Loss: 0.7511686086654663\n",
            "Epoch: 2/5; Step: 625/3557; Loss: 0.8419747352600098\n",
            "Epoch: 2/5; Step: 630/3557; Loss: 0.774297833442688\n",
            "Epoch: 2/5; Step: 635/3557; Loss: 0.849132776260376\n",
            "Epoch: 2/5; Step: 640/3557; Loss: 0.8316314220428467\n",
            "Epoch: 2/5; Step: 645/3557; Loss: 0.8547635078430176\n",
            "Epoch: 2/5; Step: 650/3557; Loss: 0.8985018730163574\n",
            "Epoch: 2/5; Step: 655/3557; Loss: 0.8514760136604309\n",
            "Epoch: 2/5; Step: 660/3557; Loss: 0.8927849531173706\n",
            "Epoch: 2/5; Step: 665/3557; Loss: 0.7610162496566772\n",
            "Epoch: 2/5; Step: 670/3557; Loss: 0.8348886966705322\n",
            "Epoch: 2/5; Step: 675/3557; Loss: 0.7444640398025513\n",
            "Epoch: 2/5; Step: 680/3557; Loss: 0.8844680786132812\n",
            "Epoch: 2/5; Step: 685/3557; Loss: 0.7420799136161804\n",
            "Epoch: 2/5; Step: 690/3557; Loss: 0.8444675803184509\n",
            "Epoch: 2/5; Step: 695/3557; Loss: 0.8228716850280762\n",
            "Epoch: 2/5; Step: 700/3557; Loss: 0.7279512882232666\n",
            "Epoch: 2/5; Step: 705/3557; Loss: 0.8156566023826599\n",
            "Epoch: 2/5; Step: 710/3557; Loss: 0.7763936519622803\n",
            "Epoch: 2/5; Step: 715/3557; Loss: 0.8832827210426331\n",
            "Epoch: 2/5; Step: 720/3557; Loss: 0.7388184070587158\n",
            "Epoch: 2/5; Step: 725/3557; Loss: 0.7454589605331421\n",
            "Epoch: 2/5; Step: 730/3557; Loss: 0.723369836807251\n",
            "Epoch: 2/5; Step: 735/3557; Loss: 0.7814901471138\n",
            "Epoch: 2/5; Step: 740/3557; Loss: 0.926604688167572\n",
            "Epoch: 2/5; Step: 745/3557; Loss: 0.7540138363838196\n",
            "Epoch: 2/5; Step: 750/3557; Loss: 0.8106045126914978\n",
            "Epoch: 2/5; Step: 755/3557; Loss: 0.7987862229347229\n",
            "Epoch: 2/5; Step: 760/3557; Loss: 0.861676812171936\n",
            "Epoch: 2/5; Step: 765/3557; Loss: 0.8149405717849731\n",
            "Epoch: 2/5; Step: 770/3557; Loss: 0.9068540334701538\n",
            "Epoch: 2/5; Step: 775/3557; Loss: 0.821094274520874\n",
            "Epoch: 2/5; Step: 780/3557; Loss: 0.7389721870422363\n",
            "Epoch: 2/5; Step: 785/3557; Loss: 0.8764139413833618\n",
            "Epoch: 2/5; Step: 790/3557; Loss: 0.8295830488204956\n",
            "Epoch: 2/5; Step: 795/3557; Loss: 0.7741439938545227\n",
            "Epoch: 2/5; Step: 800/3557; Loss: 0.8230547904968262\n",
            "Epoch: 2/5; Step: 805/3557; Loss: 0.6839237809181213\n",
            "Epoch: 2/5; Step: 810/3557; Loss: 0.801731526851654\n",
            "Epoch: 2/5; Step: 815/3557; Loss: 0.8805152773857117\n",
            "Epoch: 2/5; Step: 820/3557; Loss: 0.8359338641166687\n",
            "Epoch: 2/5; Step: 825/3557; Loss: 0.7532610893249512\n",
            "Epoch: 2/5; Step: 830/3557; Loss: 0.7958594560623169\n",
            "Epoch: 2/5; Step: 835/3557; Loss: 0.8265324831008911\n",
            "Epoch: 2/5; Step: 840/3557; Loss: 0.8359670639038086\n",
            "Epoch: 2/5; Step: 845/3557; Loss: 0.7996447086334229\n",
            "Epoch: 2/5; Step: 850/3557; Loss: 0.7990316152572632\n",
            "Epoch: 2/5; Step: 855/3557; Loss: 0.7493563890457153\n",
            "Epoch: 2/5; Step: 860/3557; Loss: 0.756186842918396\n",
            "Epoch: 2/5; Step: 865/3557; Loss: 0.7384251356124878\n",
            "Epoch: 2/5; Step: 870/3557; Loss: 0.8680275082588196\n",
            "Epoch: 2/5; Step: 875/3557; Loss: 0.831269383430481\n",
            "Epoch: 2/5; Step: 880/3557; Loss: 0.8049601912498474\n",
            "Epoch: 2/5; Step: 885/3557; Loss: 0.8827064037322998\n",
            "Epoch: 2/5; Step: 890/3557; Loss: 0.8312793374061584\n",
            "Epoch: 2/5; Step: 895/3557; Loss: 0.782304048538208\n",
            "Epoch: 2/5; Step: 900/3557; Loss: 0.8010097146034241\n",
            "Epoch: 2/5; Step: 905/3557; Loss: 0.7664563059806824\n",
            "Epoch: 2/5; Step: 910/3557; Loss: 0.7802687883377075\n",
            "Epoch: 2/5; Step: 915/3557; Loss: 0.7356507182121277\n",
            "Epoch: 2/5; Step: 920/3557; Loss: 0.8009018301963806\n",
            "Epoch: 2/5; Step: 925/3557; Loss: 0.7820162773132324\n",
            "Epoch: 2/5; Step: 930/3557; Loss: 0.9027563333511353\n",
            "Epoch: 2/5; Step: 935/3557; Loss: 0.7568352222442627\n",
            "Epoch: 2/5; Step: 940/3557; Loss: 0.7921425104141235\n",
            "Epoch: 2/5; Step: 945/3557; Loss: 0.87578946352005\n",
            "Epoch: 2/5; Step: 950/3557; Loss: 0.7832552790641785\n",
            "Epoch: 2/5; Step: 955/3557; Loss: 0.8295441269874573\n",
            "Epoch: 2/5; Step: 960/3557; Loss: 0.7159739136695862\n",
            "Epoch: 2/5; Step: 965/3557; Loss: 0.8384465575218201\n",
            "Epoch: 2/5; Step: 970/3557; Loss: 0.7804487347602844\n",
            "Epoch: 2/5; Step: 975/3557; Loss: 0.7834789752960205\n",
            "Epoch: 2/5; Step: 980/3557; Loss: 0.692255437374115\n",
            "Epoch: 2/5; Step: 985/3557; Loss: 0.7061631083488464\n",
            "Epoch: 2/5; Step: 990/3557; Loss: 0.814824104309082\n",
            "Epoch: 2/5; Step: 995/3557; Loss: 0.7503324747085571\n",
            "Epoch: 2/5; Step: 1000/3557; Loss: 0.8053192496299744\n",
            "Epoch: 2/5; Step: 1005/3557; Loss: 0.8376240730285645\n",
            "Epoch: 2/5; Step: 1010/3557; Loss: 0.8518037796020508\n",
            "Epoch: 2/5; Step: 1015/3557; Loss: 0.7460747361183167\n",
            "Epoch: 2/5; Step: 1020/3557; Loss: 0.7384099364280701\n",
            "Epoch: 2/5; Step: 1025/3557; Loss: 0.8520947694778442\n",
            "Epoch: 2/5; Step: 1030/3557; Loss: 0.7767136096954346\n",
            "Epoch: 2/5; Step: 1035/3557; Loss: 0.8255389332771301\n",
            "Epoch: 2/5; Step: 1040/3557; Loss: 0.6985855102539062\n",
            "Epoch: 2/5; Step: 1045/3557; Loss: 0.7836408019065857\n",
            "Epoch: 2/5; Step: 1050/3557; Loss: 0.8293901681900024\n",
            "Epoch: 2/5; Step: 1055/3557; Loss: 0.7689869403839111\n",
            "Epoch: 2/5; Step: 1060/3557; Loss: 0.7838091850280762\n",
            "Epoch: 2/5; Step: 1065/3557; Loss: 0.8219479322433472\n",
            "Epoch: 2/5; Step: 1070/3557; Loss: 0.7397515177726746\n",
            "Epoch: 2/5; Step: 1075/3557; Loss: 0.6954950094223022\n",
            "Epoch: 2/5; Step: 1080/3557; Loss: 0.6764973998069763\n",
            "Epoch: 2/5; Step: 1085/3557; Loss: 0.7911267876625061\n",
            "Epoch: 2/5; Step: 1090/3557; Loss: 0.7589825987815857\n",
            "Epoch: 2/5; Step: 1095/3557; Loss: 0.9008113145828247\n",
            "Epoch: 2/5; Step: 1100/3557; Loss: 0.8250174522399902\n",
            "Epoch: 2/5; Step: 1105/3557; Loss: 0.7875762581825256\n",
            "Epoch: 2/5; Step: 1110/3557; Loss: 0.7502596974372864\n",
            "Epoch: 2/5; Step: 1115/3557; Loss: 0.6567110419273376\n",
            "Epoch: 2/5; Step: 1120/3557; Loss: 0.7519867420196533\n",
            "Epoch: 2/5; Step: 1125/3557; Loss: 0.7295061945915222\n",
            "Epoch: 2/5; Step: 1130/3557; Loss: 0.7411792278289795\n",
            "Epoch: 2/5; Step: 1135/3557; Loss: 0.7703720927238464\n",
            "Epoch: 2/5; Step: 1140/3557; Loss: 0.7271837592124939\n",
            "Epoch: 2/5; Step: 1145/3557; Loss: 0.8404104113578796\n",
            "Epoch: 2/5; Step: 1150/3557; Loss: 0.7751129269599915\n",
            "Epoch: 2/5; Step: 1155/3557; Loss: 0.6980432868003845\n",
            "Epoch: 2/5; Step: 1160/3557; Loss: 0.7874143719673157\n",
            "Epoch: 2/5; Step: 1165/3557; Loss: 0.8062787652015686\n",
            "Epoch: 2/5; Step: 1170/3557; Loss: 0.8053863644599915\n",
            "Epoch: 2/5; Step: 1175/3557; Loss: 0.7759655714035034\n",
            "Epoch: 2/5; Step: 1180/3557; Loss: 0.8192931413650513\n",
            "Epoch: 2/5; Step: 1185/3557; Loss: 0.820848286151886\n",
            "Epoch: 2/5; Step: 1190/3557; Loss: 0.7553102374076843\n",
            "Epoch: 2/5; Step: 1195/3557; Loss: 0.695087194442749\n",
            "Epoch: 2/5; Step: 1200/3557; Loss: 0.7171465754508972\n",
            "Epoch: 2/5; Step: 1205/3557; Loss: 0.6898317337036133\n",
            "Epoch: 2/5; Step: 1210/3557; Loss: 0.6377347707748413\n",
            "Epoch: 2/5; Step: 1215/3557; Loss: 0.7932651042938232\n",
            "Epoch: 2/5; Step: 1220/3557; Loss: 0.7923873066902161\n",
            "Epoch: 2/5; Step: 1225/3557; Loss: 0.7483301758766174\n",
            "Epoch: 2/5; Step: 1230/3557; Loss: 0.7547218799591064\n",
            "Epoch: 2/5; Step: 1235/3557; Loss: 0.7676063776016235\n",
            "Epoch: 2/5; Step: 1240/3557; Loss: 0.8070856332778931\n",
            "Epoch: 2/5; Step: 1245/3557; Loss: 0.7084515690803528\n",
            "Epoch: 2/5; Step: 1250/3557; Loss: 0.7790740132331848\n",
            "Epoch: 2/5; Step: 1255/3557; Loss: 0.6839719414710999\n",
            "Epoch: 2/5; Step: 1260/3557; Loss: 0.8255593180656433\n",
            "Epoch: 2/5; Step: 1265/3557; Loss: 0.8067148923873901\n",
            "Epoch: 2/5; Step: 1270/3557; Loss: 0.8195615410804749\n",
            "Epoch: 2/5; Step: 1275/3557; Loss: 0.6939224004745483\n",
            "Epoch: 2/5; Step: 1280/3557; Loss: 0.7269452214241028\n",
            "Epoch: 2/5; Step: 1285/3557; Loss: 0.8276277780532837\n",
            "Epoch: 2/5; Step: 1290/3557; Loss: 0.7193290591239929\n",
            "Epoch: 2/5; Step: 1295/3557; Loss: 0.7495776414871216\n",
            "Epoch: 2/5; Step: 1300/3557; Loss: 0.7538538575172424\n",
            "Epoch: 2/5; Step: 1305/3557; Loss: 0.6619903445243835\n",
            "Epoch: 2/5; Step: 1310/3557; Loss: 0.8429747819900513\n",
            "Epoch: 2/5; Step: 1315/3557; Loss: 0.7273941040039062\n",
            "Epoch: 2/5; Step: 1320/3557; Loss: 0.7327102422714233\n",
            "Epoch: 2/5; Step: 1325/3557; Loss: 0.6693246960639954\n",
            "Epoch: 2/5; Step: 1330/3557; Loss: 0.7492484450340271\n",
            "Epoch: 2/5; Step: 1335/3557; Loss: 0.7123230695724487\n",
            "Epoch: 2/5; Step: 1340/3557; Loss: 0.7763073444366455\n",
            "Epoch: 2/5; Step: 1345/3557; Loss: 0.6420772075653076\n",
            "Epoch: 2/5; Step: 1350/3557; Loss: 0.7423346638679504\n",
            "Epoch: 2/5; Step: 1355/3557; Loss: 0.6692586541175842\n",
            "Epoch: 2/5; Step: 1360/3557; Loss: 0.7464972734451294\n",
            "Epoch: 2/5; Step: 1365/3557; Loss: 0.6816835403442383\n",
            "Epoch: 2/5; Step: 1370/3557; Loss: 0.9588549137115479\n",
            "Epoch: 2/5; Step: 1375/3557; Loss: 0.6772368550300598\n",
            "Epoch: 2/5; Step: 1380/3557; Loss: 0.6348488926887512\n",
            "Epoch: 2/5; Step: 1385/3557; Loss: 0.7315137982368469\n",
            "Epoch: 2/5; Step: 1390/3557; Loss: 0.7224395871162415\n",
            "Epoch: 2/5; Step: 1395/3557; Loss: 0.8501271605491638\n",
            "Epoch: 2/5; Step: 1400/3557; Loss: 0.7580746412277222\n",
            "Epoch: 2/5; Step: 1405/3557; Loss: 0.7151954770088196\n",
            "Epoch: 2/5; Step: 1410/3557; Loss: 0.6969637870788574\n",
            "Epoch: 2/5; Step: 1415/3557; Loss: 0.8028936386108398\n",
            "Epoch: 2/5; Step: 1420/3557; Loss: 0.6882532835006714\n",
            "Epoch: 2/5; Step: 1425/3557; Loss: 0.6858242750167847\n",
            "Epoch: 2/5; Step: 1430/3557; Loss: 0.6645232439041138\n",
            "Epoch: 2/5; Step: 1435/3557; Loss: 0.7381045818328857\n",
            "Epoch: 2/5; Step: 1440/3557; Loss: 0.8596102595329285\n",
            "Epoch: 2/5; Step: 1445/3557; Loss: 0.7234374284744263\n",
            "Epoch: 2/5; Step: 1450/3557; Loss: 0.7639633417129517\n",
            "Epoch: 2/5; Step: 1455/3557; Loss: 0.6726540327072144\n",
            "Epoch: 2/5; Step: 1460/3557; Loss: 0.7223619222640991\n",
            "Epoch: 2/5; Step: 1465/3557; Loss: 0.6551274061203003\n",
            "Epoch: 2/5; Step: 1470/3557; Loss: 0.761115550994873\n",
            "Epoch: 2/5; Step: 1475/3557; Loss: 0.721345841884613\n",
            "Epoch: 2/5; Step: 1480/3557; Loss: 0.6902883648872375\n",
            "Epoch: 2/5; Step: 1485/3557; Loss: 0.7259523868560791\n",
            "Epoch: 2/5; Step: 1490/3557; Loss: 0.7327975630760193\n",
            "Epoch: 2/5; Step: 1495/3557; Loss: 0.7720504999160767\n",
            "Epoch: 2/5; Step: 1500/3557; Loss: 0.765547513961792\n",
            "Epoch: 2/5; Step: 1505/3557; Loss: 0.7494799494743347\n",
            "Epoch: 2/5; Step: 1510/3557; Loss: 0.7739644050598145\n",
            "Epoch: 2/5; Step: 1515/3557; Loss: 0.7206056714057922\n",
            "Epoch: 2/5; Step: 1520/3557; Loss: 0.6838889718055725\n",
            "Epoch: 2/5; Step: 1525/3557; Loss: 0.7282569408416748\n",
            "Epoch: 2/5; Step: 1530/3557; Loss: 0.6467980146408081\n",
            "Epoch: 2/5; Step: 1535/3557; Loss: 0.721007227897644\n",
            "Epoch: 2/5; Step: 1540/3557; Loss: 0.7832456827163696\n",
            "Epoch: 2/5; Step: 1545/3557; Loss: 0.7398850321769714\n",
            "Epoch: 2/5; Step: 1550/3557; Loss: 0.7309181690216064\n",
            "Epoch: 2/5; Step: 1555/3557; Loss: 0.7631642818450928\n",
            "Epoch: 2/5; Step: 1560/3557; Loss: 0.7212052345275879\n",
            "Epoch: 2/5; Step: 1565/3557; Loss: 0.7803627252578735\n",
            "Epoch: 2/5; Step: 1570/3557; Loss: 0.714867889881134\n",
            "Epoch: 2/5; Step: 1575/3557; Loss: 0.6289670467376709\n",
            "Epoch: 2/5; Step: 1580/3557; Loss: 0.6685604453086853\n",
            "Epoch: 2/5; Step: 1585/3557; Loss: 0.6474748849868774\n",
            "Epoch: 2/5; Step: 1590/3557; Loss: 0.7531799077987671\n",
            "Epoch: 2/5; Step: 1595/3557; Loss: 0.7283039689064026\n",
            "Epoch: 2/5; Step: 1600/3557; Loss: 0.7154286503791809\n",
            "Epoch: 2/5; Step: 1605/3557; Loss: 0.8112195730209351\n",
            "Epoch: 2/5; Step: 1610/3557; Loss: 0.7916914224624634\n",
            "Epoch: 2/5; Step: 1615/3557; Loss: 0.7609080076217651\n",
            "Epoch: 2/5; Step: 1620/3557; Loss: 0.6949467658996582\n",
            "Epoch: 2/5; Step: 1625/3557; Loss: 0.7899433970451355\n",
            "Epoch: 2/5; Step: 1630/3557; Loss: 0.6995813250541687\n",
            "Epoch: 2/5; Step: 1635/3557; Loss: 0.5489984750747681\n",
            "Epoch: 2/5; Step: 1640/3557; Loss: 0.6291504502296448\n",
            "Epoch: 2/5; Step: 1645/3557; Loss: 0.688452959060669\n",
            "Epoch: 2/5; Step: 1650/3557; Loss: 0.6731393933296204\n",
            "Epoch: 2/5; Step: 1655/3557; Loss: 0.8746287226676941\n",
            "Epoch: 2/5; Step: 1660/3557; Loss: 0.7041904926300049\n",
            "Epoch: 2/5; Step: 1665/3557; Loss: 0.676293134689331\n",
            "Epoch: 2/5; Step: 1670/3557; Loss: 0.7677136063575745\n",
            "Epoch: 2/5; Step: 1675/3557; Loss: 0.7203149795532227\n",
            "Epoch: 2/5; Step: 1680/3557; Loss: 0.6242530941963196\n",
            "Epoch: 2/5; Step: 1685/3557; Loss: 0.7088868021965027\n",
            "Epoch: 2/5; Step: 1690/3557; Loss: 0.7764577269554138\n",
            "Epoch: 2/5; Step: 1695/3557; Loss: 0.6691159605979919\n",
            "Epoch: 2/5; Step: 1700/3557; Loss: 0.6690201759338379\n",
            "Epoch: 2/5; Step: 1705/3557; Loss: 0.6933242678642273\n",
            "Epoch: 2/5; Step: 1710/3557; Loss: 0.7522212266921997\n",
            "Epoch: 2/5; Step: 1715/3557; Loss: 0.6565901041030884\n",
            "Epoch: 2/5; Step: 1720/3557; Loss: 0.6863776445388794\n",
            "Epoch: 2/5; Step: 1725/3557; Loss: 0.7051275372505188\n",
            "Epoch: 2/5; Step: 1730/3557; Loss: 0.6326726675033569\n",
            "Epoch: 2/5; Step: 1735/3557; Loss: 0.702512800693512\n",
            "Epoch: 2/5; Step: 1740/3557; Loss: 0.6456918716430664\n",
            "Epoch: 2/5; Step: 1745/3557; Loss: 0.7068880796432495\n",
            "Epoch: 2/5; Step: 1750/3557; Loss: 0.6437668800354004\n",
            "Epoch: 2/5; Step: 1755/3557; Loss: 0.7013401389122009\n",
            "Epoch: 2/5; Step: 1760/3557; Loss: 0.6290470361709595\n",
            "Epoch: 2/5; Step: 1765/3557; Loss: 0.7397648692131042\n",
            "Epoch: 2/5; Step: 1770/3557; Loss: 0.7338080406188965\n",
            "Epoch: 2/5; Step: 1775/3557; Loss: 0.664537250995636\n",
            "Epoch: 2/5; Step: 1780/3557; Loss: 0.7038670182228088\n",
            "Epoch: 2/5; Step: 1785/3557; Loss: 0.7396433353424072\n",
            "Epoch: 2/5; Step: 1790/3557; Loss: 0.8571021556854248\n",
            "Epoch: 2/5; Step: 1795/3557; Loss: 0.6910781264305115\n",
            "Epoch: 2/5; Step: 1800/3557; Loss: 0.656654417514801\n",
            "Epoch: 2/5; Step: 1805/3557; Loss: 0.7340616583824158\n",
            "Epoch: 2/5; Step: 1810/3557; Loss: 0.6922370195388794\n",
            "Epoch: 2/5; Step: 1815/3557; Loss: 0.6606273055076599\n",
            "Epoch: 2/5; Step: 1820/3557; Loss: 0.656823992729187\n",
            "Epoch: 2/5; Step: 1825/3557; Loss: 0.6515399813652039\n",
            "Epoch: 2/5; Step: 1830/3557; Loss: 0.6239988803863525\n",
            "Epoch: 2/5; Step: 1835/3557; Loss: 0.7339611649513245\n",
            "Epoch: 2/5; Step: 1840/3557; Loss: 0.6709864735603333\n",
            "Epoch: 2/5; Step: 1845/3557; Loss: 0.6345838308334351\n",
            "Epoch: 2/5; Step: 1850/3557; Loss: 0.6733694076538086\n",
            "Epoch: 2/5; Step: 1855/3557; Loss: 0.6936999559402466\n",
            "Epoch: 2/5; Step: 1860/3557; Loss: 0.6918550729751587\n",
            "Epoch: 2/5; Step: 1865/3557; Loss: 0.6087799072265625\n",
            "Epoch: 2/5; Step: 1870/3557; Loss: 0.6725628972053528\n",
            "Epoch: 2/5; Step: 1875/3557; Loss: 0.6566716432571411\n",
            "Epoch: 2/5; Step: 1880/3557; Loss: 0.6744260787963867\n",
            "Epoch: 2/5; Step: 1885/3557; Loss: 0.6679022908210754\n",
            "Epoch: 2/5; Step: 1890/3557; Loss: 0.6190440058708191\n",
            "Epoch: 2/5; Step: 1895/3557; Loss: 0.7410288453102112\n",
            "Epoch: 2/5; Step: 1900/3557; Loss: 0.7596849203109741\n",
            "Epoch: 2/5; Step: 1905/3557; Loss: 0.6411316394805908\n",
            "Epoch: 2/5; Step: 1910/3557; Loss: 0.6512004137039185\n",
            "Epoch: 2/5; Step: 1915/3557; Loss: 0.7683418989181519\n",
            "Epoch: 2/5; Step: 1920/3557; Loss: 0.6542408466339111\n",
            "Epoch: 2/5; Step: 1925/3557; Loss: 0.6453951597213745\n",
            "Epoch: 2/5; Step: 1930/3557; Loss: 0.7086331248283386\n",
            "Epoch: 2/5; Step: 1935/3557; Loss: 0.6605934500694275\n",
            "Epoch: 2/5; Step: 1940/3557; Loss: 0.6343011260032654\n",
            "Epoch: 2/5; Step: 1945/3557; Loss: 0.695551872253418\n",
            "Epoch: 2/5; Step: 1950/3557; Loss: 0.6864361763000488\n",
            "Epoch: 2/5; Step: 1955/3557; Loss: 0.6145667433738708\n",
            "Epoch: 2/5; Step: 1960/3557; Loss: 0.6104850172996521\n",
            "Epoch: 2/5; Step: 1965/3557; Loss: 0.6784366965293884\n",
            "Epoch: 2/5; Step: 1970/3557; Loss: 0.6338600516319275\n",
            "Epoch: 2/5; Step: 1975/3557; Loss: 0.6605814099311829\n",
            "Epoch: 2/5; Step: 1980/3557; Loss: 0.6796269416809082\n",
            "Epoch: 2/5; Step: 1985/3557; Loss: 0.7120320796966553\n",
            "Epoch: 2/5; Step: 1990/3557; Loss: 0.6816264390945435\n",
            "Epoch: 2/5; Step: 1995/3557; Loss: 0.6182346940040588\n",
            "Epoch: 2/5; Step: 2000/3557; Loss: 0.6965730786323547\n",
            "Epoch: 2/5; Step: 2005/3557; Loss: 0.7157585024833679\n",
            "Epoch: 2/5; Step: 2010/3557; Loss: 0.6696228384971619\n",
            "Epoch: 2/5; Step: 2015/3557; Loss: 0.6688441038131714\n",
            "Epoch: 2/5; Step: 2020/3557; Loss: 0.6794002056121826\n",
            "Epoch: 2/5; Step: 2025/3557; Loss: 0.7474013566970825\n",
            "Epoch: 2/5; Step: 2030/3557; Loss: 0.7483083009719849\n",
            "Epoch: 2/5; Step: 2035/3557; Loss: 0.6821681261062622\n",
            "Epoch: 2/5; Step: 2040/3557; Loss: 0.648293673992157\n",
            "Epoch: 2/5; Step: 2045/3557; Loss: 0.6296598315238953\n",
            "Epoch: 2/5; Step: 2050/3557; Loss: 0.7225779891014099\n",
            "Epoch: 2/5; Step: 2055/3557; Loss: 0.6214919686317444\n",
            "Epoch: 2/5; Step: 2060/3557; Loss: 0.6435945630073547\n",
            "Epoch: 2/5; Step: 2065/3557; Loss: 0.6754745841026306\n",
            "Epoch: 2/5; Step: 2070/3557; Loss: 0.6749739646911621\n",
            "Epoch: 2/5; Step: 2075/3557; Loss: 0.6710266470909119\n",
            "Epoch: 2/5; Step: 2080/3557; Loss: 0.7354576587677002\n",
            "Epoch: 2/5; Step: 2085/3557; Loss: 0.6042487025260925\n",
            "Epoch: 2/5; Step: 2090/3557; Loss: 0.7105786800384521\n",
            "Epoch: 2/5; Step: 2095/3557; Loss: 0.7194931507110596\n",
            "Epoch: 2/5; Step: 2100/3557; Loss: 0.6575452089309692\n",
            "Epoch: 2/5; Step: 2105/3557; Loss: 0.7008896470069885\n",
            "Epoch: 2/5; Step: 2110/3557; Loss: 0.6309570074081421\n",
            "Epoch: 2/5; Step: 2115/3557; Loss: 0.6905674934387207\n",
            "Epoch: 2/5; Step: 2120/3557; Loss: 0.7282254695892334\n",
            "Epoch: 2/5; Step: 2125/3557; Loss: 0.6226330399513245\n",
            "Epoch: 2/5; Step: 2130/3557; Loss: 0.6564264297485352\n",
            "Epoch: 2/5; Step: 2135/3557; Loss: 0.6432399153709412\n",
            "Epoch: 2/5; Step: 2140/3557; Loss: 0.6905350089073181\n",
            "Epoch: 2/5; Step: 2145/3557; Loss: 0.696540117263794\n",
            "Epoch: 2/5; Step: 2150/3557; Loss: 0.8086754679679871\n",
            "Epoch: 2/5; Step: 2155/3557; Loss: 0.6033961772918701\n",
            "Epoch: 2/5; Step: 2160/3557; Loss: 0.6545573472976685\n",
            "Epoch: 2/5; Step: 2165/3557; Loss: 0.7117072939872742\n",
            "Epoch: 2/5; Step: 2170/3557; Loss: 0.6178308129310608\n",
            "Epoch: 2/5; Step: 2175/3557; Loss: 0.617739737033844\n",
            "Epoch: 2/5; Step: 2180/3557; Loss: 0.6930849552154541\n",
            "Epoch: 2/5; Step: 2185/3557; Loss: 0.739915132522583\n",
            "Epoch: 2/5; Step: 2190/3557; Loss: 0.5931352972984314\n",
            "Epoch: 2/5; Step: 2195/3557; Loss: 0.5849851369857788\n",
            "Epoch: 2/5; Step: 2200/3557; Loss: 0.6759294271469116\n",
            "Epoch: 2/5; Step: 2205/3557; Loss: 0.6609810590744019\n",
            "Epoch: 2/5; Step: 2210/3557; Loss: 0.6616243124008179\n",
            "Epoch: 2/5; Step: 2215/3557; Loss: 0.6749018430709839\n",
            "Epoch: 2/5; Step: 2220/3557; Loss: 0.6149230599403381\n",
            "Epoch: 2/5; Step: 2225/3557; Loss: 0.6792150139808655\n",
            "Epoch: 2/5; Step: 2230/3557; Loss: 0.669251024723053\n",
            "Epoch: 2/5; Step: 2235/3557; Loss: 0.6615612506866455\n",
            "Epoch: 2/5; Step: 2240/3557; Loss: 0.5607183575630188\n",
            "Epoch: 2/5; Step: 2245/3557; Loss: 0.6616038084030151\n",
            "Epoch: 2/5; Step: 2250/3557; Loss: 0.6459199786186218\n",
            "Epoch: 2/5; Step: 2255/3557; Loss: 0.6658363342285156\n",
            "Epoch: 2/5; Step: 2260/3557; Loss: 0.6712946891784668\n",
            "Epoch: 2/5; Step: 2265/3557; Loss: 0.6681243777275085\n",
            "Epoch: 2/5; Step: 2270/3557; Loss: 0.6023882031440735\n",
            "Epoch: 2/5; Step: 2275/3557; Loss: 0.6608899831771851\n",
            "Epoch: 2/5; Step: 2280/3557; Loss: 0.6658055186271667\n",
            "Epoch: 2/5; Step: 2285/3557; Loss: 0.635344922542572\n",
            "Epoch: 2/5; Step: 2290/3557; Loss: 0.589608371257782\n",
            "Epoch: 2/5; Step: 2295/3557; Loss: 0.716844916343689\n",
            "Epoch: 2/5; Step: 2300/3557; Loss: 0.6062493324279785\n",
            "Epoch: 2/5; Step: 2305/3557; Loss: 0.6744065284729004\n",
            "Epoch: 2/5; Step: 2310/3557; Loss: 0.6333183646202087\n",
            "Epoch: 2/5; Step: 2315/3557; Loss: 0.6967898607254028\n",
            "Epoch: 2/5; Step: 2320/3557; Loss: 0.5725575685501099\n",
            "Epoch: 2/5; Step: 2325/3557; Loss: 0.690227746963501\n",
            "Epoch: 2/5; Step: 2330/3557; Loss: 0.6052900552749634\n",
            "Epoch: 2/5; Step: 2335/3557; Loss: 0.6245953440666199\n",
            "Epoch: 2/5; Step: 2340/3557; Loss: 0.6219049692153931\n",
            "Epoch: 2/5; Step: 2345/3557; Loss: 0.7063544392585754\n",
            "Epoch: 2/5; Step: 2350/3557; Loss: 0.6266006827354431\n",
            "Epoch: 2/5; Step: 2355/3557; Loss: 0.6059715151786804\n",
            "Epoch: 2/5; Step: 2360/3557; Loss: 0.6689282655715942\n",
            "Epoch: 2/5; Step: 2365/3557; Loss: 0.7024581432342529\n",
            "Epoch: 2/5; Step: 2370/3557; Loss: 0.7182322144508362\n",
            "Epoch: 2/5; Step: 2375/3557; Loss: 0.6830703616142273\n",
            "Epoch: 2/5; Step: 2380/3557; Loss: 0.6914674639701843\n",
            "Epoch: 2/5; Step: 2385/3557; Loss: 0.6172566413879395\n",
            "Epoch: 2/5; Step: 2390/3557; Loss: 0.5611589550971985\n",
            "Epoch: 2/5; Step: 2395/3557; Loss: 0.6452454924583435\n",
            "Epoch: 2/5; Step: 2400/3557; Loss: 0.596102237701416\n",
            "Epoch: 2/5; Step: 2405/3557; Loss: 0.6172902584075928\n",
            "Epoch: 2/5; Step: 2410/3557; Loss: 0.59127277135849\n",
            "Epoch: 2/5; Step: 2415/3557; Loss: 0.6198557019233704\n",
            "Epoch: 2/5; Step: 2420/3557; Loss: 0.6109008193016052\n",
            "Epoch: 2/5; Step: 2425/3557; Loss: 0.6622374653816223\n",
            "Epoch: 2/5; Step: 2430/3557; Loss: 0.590289294719696\n",
            "Epoch: 2/5; Step: 2435/3557; Loss: 0.6669788956642151\n",
            "Epoch: 2/5; Step: 2440/3557; Loss: 0.6361365914344788\n",
            "Epoch: 2/5; Step: 2445/3557; Loss: 0.5953405499458313\n",
            "Epoch: 2/5; Step: 2450/3557; Loss: 0.5460551381111145\n",
            "Epoch: 2/5; Step: 2455/3557; Loss: 0.561985969543457\n",
            "Epoch: 2/5; Step: 2460/3557; Loss: 0.7115169167518616\n",
            "Epoch: 2/5; Step: 2465/3557; Loss: 0.535147488117218\n",
            "Epoch: 2/5; Step: 2470/3557; Loss: 0.6479038596153259\n",
            "Epoch: 2/5; Step: 2475/3557; Loss: 0.6582112312316895\n",
            "Epoch: 2/5; Step: 2480/3557; Loss: 0.61417156457901\n",
            "Epoch: 2/5; Step: 2485/3557; Loss: 0.694703221321106\n",
            "Epoch: 2/5; Step: 2490/3557; Loss: 0.5824719667434692\n",
            "Epoch: 2/5; Step: 2495/3557; Loss: 0.599398136138916\n",
            "Epoch: 2/5; Step: 2500/3557; Loss: 0.6134697198867798\n",
            "Epoch: 2/5; Step: 2505/3557; Loss: 0.6617039442062378\n",
            "Epoch: 2/5; Step: 2510/3557; Loss: 0.622346818447113\n",
            "Epoch: 2/5; Step: 2515/3557; Loss: 0.6702079176902771\n",
            "Epoch: 2/5; Step: 2520/3557; Loss: 0.604674756526947\n",
            "Epoch: 2/5; Step: 2525/3557; Loss: 0.5891286730766296\n",
            "Epoch: 2/5; Step: 2530/3557; Loss: 0.6892837882041931\n",
            "Epoch: 2/5; Step: 2535/3557; Loss: 0.6707233786582947\n",
            "Epoch: 2/5; Step: 2540/3557; Loss: 0.5768897533416748\n",
            "Epoch: 2/5; Step: 2545/3557; Loss: 0.541725218296051\n",
            "Epoch: 2/5; Step: 2550/3557; Loss: 0.6976206302642822\n",
            "Epoch: 2/5; Step: 2555/3557; Loss: 0.564873218536377\n",
            "Epoch: 2/5; Step: 2560/3557; Loss: 0.606869637966156\n",
            "Epoch: 2/5; Step: 2565/3557; Loss: 0.6294810771942139\n",
            "Epoch: 2/5; Step: 2570/3557; Loss: 0.6635602116584778\n",
            "Epoch: 2/5; Step: 2575/3557; Loss: 0.6221038103103638\n",
            "Epoch: 2/5; Step: 2580/3557; Loss: 0.5342010855674744\n",
            "Epoch: 2/5; Step: 2585/3557; Loss: 0.6749613285064697\n",
            "Epoch: 2/5; Step: 2590/3557; Loss: 0.626372754573822\n",
            "Epoch: 2/5; Step: 2595/3557; Loss: 0.6976714134216309\n",
            "Epoch: 2/5; Step: 2600/3557; Loss: 0.6574660539627075\n",
            "Epoch: 2/5; Step: 2605/3557; Loss: 0.6046203374862671\n",
            "Epoch: 2/5; Step: 2610/3557; Loss: 0.5686395168304443\n",
            "Epoch: 2/5; Step: 2615/3557; Loss: 0.5116269588470459\n",
            "Epoch: 2/5; Step: 2620/3557; Loss: 0.7152053117752075\n",
            "Epoch: 2/5; Step: 2625/3557; Loss: 0.6053475737571716\n",
            "Epoch: 2/5; Step: 2630/3557; Loss: 0.5975286364555359\n",
            "Epoch: 2/5; Step: 2635/3557; Loss: 0.6389349102973938\n",
            "Epoch: 2/5; Step: 2640/3557; Loss: 0.6401757597923279\n",
            "Epoch: 2/5; Step: 2645/3557; Loss: 0.626258373260498\n",
            "Epoch: 2/5; Step: 2650/3557; Loss: 0.6224090456962585\n",
            "Epoch: 2/5; Step: 2655/3557; Loss: 0.6819313168525696\n",
            "Epoch: 2/5; Step: 2660/3557; Loss: 0.6535377502441406\n",
            "Epoch: 2/5; Step: 2665/3557; Loss: 0.6650758981704712\n",
            "Epoch: 2/5; Step: 2670/3557; Loss: 0.5981817841529846\n",
            "Epoch: 2/5; Step: 2675/3557; Loss: 0.614667534828186\n",
            "Epoch: 2/5; Step: 2680/3557; Loss: 0.6385141611099243\n",
            "Epoch: 2/5; Step: 2685/3557; Loss: 0.6498653292655945\n",
            "Epoch: 2/5; Step: 2690/3557; Loss: 0.6149717569351196\n",
            "Epoch: 2/5; Step: 2695/3557; Loss: 0.6749498844146729\n",
            "Epoch: 2/5; Step: 2700/3557; Loss: 0.6292202472686768\n",
            "Epoch: 2/5; Step: 2705/3557; Loss: 0.6585264205932617\n",
            "Epoch: 2/5; Step: 2710/3557; Loss: 0.5753172636032104\n",
            "Epoch: 2/5; Step: 2715/3557; Loss: 0.5733236074447632\n",
            "Epoch: 2/5; Step: 2720/3557; Loss: 0.6613558530807495\n",
            "Epoch: 2/5; Step: 2725/3557; Loss: 0.631811797618866\n",
            "Epoch: 2/5; Step: 2730/3557; Loss: 0.6380575299263\n",
            "Epoch: 2/5; Step: 2735/3557; Loss: 0.6196203231811523\n",
            "Epoch: 2/5; Step: 2740/3557; Loss: 0.5732070803642273\n",
            "Epoch: 2/5; Step: 2745/3557; Loss: 0.6279258131980896\n",
            "Epoch: 2/5; Step: 2750/3557; Loss: 0.5524860620498657\n",
            "Epoch: 2/5; Step: 2755/3557; Loss: 0.6752189993858337\n",
            "Epoch: 2/5; Step: 2760/3557; Loss: 0.648227870464325\n",
            "Epoch: 2/5; Step: 2765/3557; Loss: 0.6200872659683228\n",
            "Epoch: 2/5; Step: 2770/3557; Loss: 0.6947693228721619\n",
            "Epoch: 2/5; Step: 2775/3557; Loss: 0.6327962279319763\n",
            "Epoch: 2/5; Step: 2780/3557; Loss: 0.6491113305091858\n",
            "Epoch: 2/5; Step: 2785/3557; Loss: 0.6112158298492432\n",
            "Epoch: 2/5; Step: 2790/3557; Loss: 0.5503268837928772\n",
            "Epoch: 2/5; Step: 2795/3557; Loss: 0.5568995475769043\n",
            "Epoch: 2/5; Step: 2800/3557; Loss: 0.6677073836326599\n",
            "Epoch: 2/5; Step: 2805/3557; Loss: 0.71589195728302\n",
            "Epoch: 2/5; Step: 2810/3557; Loss: 0.6746208071708679\n",
            "Epoch: 2/5; Step: 2815/3557; Loss: 0.6896914839744568\n",
            "Epoch: 2/5; Step: 2820/3557; Loss: 0.6416659951210022\n",
            "Epoch: 2/5; Step: 2825/3557; Loss: 0.6111415028572083\n",
            "Epoch: 2/5; Step: 2830/3557; Loss: 0.6188637614250183\n",
            "Epoch: 2/5; Step: 2835/3557; Loss: 0.6468490958213806\n",
            "Epoch: 2/5; Step: 2840/3557; Loss: 0.630859375\n",
            "Epoch: 2/5; Step: 2845/3557; Loss: 0.6677833795547485\n",
            "Epoch: 2/5; Step: 2850/3557; Loss: 0.6273436546325684\n",
            "Epoch: 2/5; Step: 2855/3557; Loss: 0.5572384595870972\n",
            "Epoch: 2/5; Step: 2860/3557; Loss: 0.6167037487030029\n",
            "Epoch: 2/5; Step: 2865/3557; Loss: 0.5814216136932373\n",
            "Epoch: 2/5; Step: 2870/3557; Loss: 0.5223391056060791\n",
            "Epoch: 2/5; Step: 2875/3557; Loss: 0.6273790597915649\n",
            "Epoch: 2/5; Step: 2880/3557; Loss: 0.604564368724823\n",
            "Epoch: 2/5; Step: 2885/3557; Loss: 0.6253580451011658\n",
            "Epoch: 2/5; Step: 2890/3557; Loss: 0.6253759264945984\n",
            "Epoch: 2/5; Step: 2895/3557; Loss: 0.5781571269035339\n",
            "Epoch: 2/5; Step: 2900/3557; Loss: 0.4979449212551117\n",
            "Epoch: 2/5; Step: 2905/3557; Loss: 0.6232355833053589\n",
            "Epoch: 2/5; Step: 2910/3557; Loss: 0.5066161155700684\n",
            "Epoch: 2/5; Step: 2915/3557; Loss: 0.5611308813095093\n",
            "Epoch: 2/5; Step: 2920/3557; Loss: 0.5759230852127075\n",
            "Epoch: 2/5; Step: 2925/3557; Loss: 0.5935394167900085\n",
            "Epoch: 2/5; Step: 2930/3557; Loss: 0.551595151424408\n",
            "Epoch: 2/5; Step: 2935/3557; Loss: 0.6378303170204163\n",
            "Epoch: 2/5; Step: 2940/3557; Loss: 0.6070663928985596\n",
            "Epoch: 2/5; Step: 2945/3557; Loss: 0.647512674331665\n",
            "Epoch: 2/5; Step: 2950/3557; Loss: 0.5962128043174744\n",
            "Epoch: 2/5; Step: 2955/3557; Loss: 0.6068305373191833\n",
            "Epoch: 2/5; Step: 2960/3557; Loss: 0.5805439352989197\n",
            "Epoch: 2/5; Step: 2965/3557; Loss: 0.5853846073150635\n",
            "Epoch: 2/5; Step: 2970/3557; Loss: 0.6379517316818237\n",
            "Epoch: 2/5; Step: 2975/3557; Loss: 0.549290657043457\n",
            "Epoch: 2/5; Step: 2980/3557; Loss: 0.5426570773124695\n",
            "Epoch: 2/5; Step: 2985/3557; Loss: 0.6046626567840576\n",
            "Epoch: 2/5; Step: 2990/3557; Loss: 0.6690841913223267\n",
            "Epoch: 2/5; Step: 2995/3557; Loss: 0.5903671979904175\n",
            "Epoch: 2/5; Step: 3000/3557; Loss: 0.6481142044067383\n",
            "Epoch: 2/5; Step: 3005/3557; Loss: 0.6107355952262878\n",
            "Epoch: 2/5; Step: 3010/3557; Loss: 0.6877166032791138\n",
            "Epoch: 2/5; Step: 3015/3557; Loss: 0.557072639465332\n",
            "Epoch: 2/5; Step: 3020/3557; Loss: 0.6095401048660278\n",
            "Epoch: 2/5; Step: 3025/3557; Loss: 0.6302924156188965\n",
            "Epoch: 2/5; Step: 3030/3557; Loss: 0.6081650257110596\n",
            "Epoch: 2/5; Step: 3035/3557; Loss: 0.6298266649246216\n",
            "Epoch: 2/5; Step: 3040/3557; Loss: 0.5619391798973083\n",
            "Epoch: 2/5; Step: 3045/3557; Loss: 0.5625520944595337\n",
            "Epoch: 2/5; Step: 3050/3557; Loss: 0.605491578578949\n",
            "Epoch: 2/5; Step: 3055/3557; Loss: 0.6925079822540283\n",
            "Epoch: 2/5; Step: 3060/3557; Loss: 0.592208743095398\n",
            "Epoch: 2/5; Step: 3065/3557; Loss: 0.6667029857635498\n",
            "Epoch: 2/5; Step: 3070/3557; Loss: 0.6036335825920105\n",
            "Epoch: 2/5; Step: 3075/3557; Loss: 0.5244342684745789\n",
            "Epoch: 2/5; Step: 3080/3557; Loss: 0.5940155386924744\n",
            "Epoch: 2/5; Step: 3085/3557; Loss: 0.5694556832313538\n",
            "Epoch: 2/5; Step: 3090/3557; Loss: 0.6597590446472168\n",
            "Epoch: 2/5; Step: 3095/3557; Loss: 0.5583064556121826\n",
            "Epoch: 2/5; Step: 3100/3557; Loss: 0.5816359519958496\n",
            "Epoch: 2/5; Step: 3105/3557; Loss: 0.5294579863548279\n",
            "Epoch: 2/5; Step: 3110/3557; Loss: 0.6554021239280701\n",
            "Epoch: 2/5; Step: 3115/3557; Loss: 0.5410179495811462\n",
            "Epoch: 2/5; Step: 3120/3557; Loss: 0.5572689771652222\n",
            "Epoch: 2/5; Step: 3125/3557; Loss: 0.6114284992218018\n",
            "Epoch: 2/5; Step: 3130/3557; Loss: 0.5833649635314941\n",
            "Epoch: 2/5; Step: 3135/3557; Loss: 0.5663335919380188\n",
            "Epoch: 2/5; Step: 3140/3557; Loss: 0.6494854688644409\n",
            "Epoch: 2/5; Step: 3145/3557; Loss: 0.6116696000099182\n",
            "Epoch: 2/5; Step: 3150/3557; Loss: 0.545988917350769\n",
            "Epoch: 2/5; Step: 3155/3557; Loss: 0.589668333530426\n",
            "Epoch: 2/5; Step: 3160/3557; Loss: 0.5585011839866638\n",
            "Epoch: 2/5; Step: 3165/3557; Loss: 0.6319434642791748\n",
            "Epoch: 2/5; Step: 3170/3557; Loss: 0.5045202374458313\n",
            "Epoch: 2/5; Step: 3175/3557; Loss: 0.6569473743438721\n",
            "Epoch: 2/5; Step: 3180/3557; Loss: 0.6153940558433533\n",
            "Epoch: 2/5; Step: 3185/3557; Loss: 0.5460377335548401\n",
            "Epoch: 2/5; Step: 3190/3557; Loss: 0.5346237421035767\n",
            "Epoch: 2/5; Step: 3195/3557; Loss: 0.5852541923522949\n",
            "Epoch: 2/5; Step: 3200/3557; Loss: 0.5922859311103821\n",
            "Epoch: 2/5; Step: 3205/3557; Loss: 0.6121506094932556\n",
            "Epoch: 2/5; Step: 3210/3557; Loss: 0.5480288863182068\n",
            "Epoch: 2/5; Step: 3215/3557; Loss: 0.6398286819458008\n",
            "Epoch: 2/5; Step: 3220/3557; Loss: 0.6070913076400757\n",
            "Epoch: 2/5; Step: 3225/3557; Loss: 0.5996553897857666\n",
            "Epoch: 2/5; Step: 3230/3557; Loss: 0.5351763367652893\n",
            "Epoch: 2/5; Step: 3235/3557; Loss: 0.5238709449768066\n",
            "Epoch: 2/5; Step: 3240/3557; Loss: 0.5903407335281372\n",
            "Epoch: 2/5; Step: 3245/3557; Loss: 0.6757463216781616\n",
            "Epoch: 2/5; Step: 3250/3557; Loss: 0.5628323554992676\n",
            "Epoch: 2/5; Step: 3255/3557; Loss: 0.5785303115844727\n",
            "Epoch: 2/5; Step: 3260/3557; Loss: 0.5185351371765137\n",
            "Epoch: 2/5; Step: 3265/3557; Loss: 0.5006093978881836\n",
            "Epoch: 2/5; Step: 3270/3557; Loss: 0.5829290151596069\n",
            "Epoch: 2/5; Step: 3275/3557; Loss: 0.6010088324546814\n",
            "Epoch: 2/5; Step: 3280/3557; Loss: 0.5420491099357605\n",
            "Epoch: 2/5; Step: 3285/3557; Loss: 0.5457484126091003\n",
            "Epoch: 2/5; Step: 3290/3557; Loss: 0.5521332621574402\n",
            "Epoch: 2/5; Step: 3295/3557; Loss: 0.5176231861114502\n",
            "Epoch: 2/5; Step: 3300/3557; Loss: 0.7255104780197144\n",
            "Epoch: 2/5; Step: 3305/3557; Loss: 0.5610983967781067\n",
            "Epoch: 2/5; Step: 3310/3557; Loss: 0.6017755270004272\n",
            "Epoch: 2/5; Step: 3315/3557; Loss: 0.6001511216163635\n",
            "Epoch: 2/5; Step: 3320/3557; Loss: 0.6383445262908936\n",
            "Epoch: 2/5; Step: 3325/3557; Loss: 0.5460377335548401\n",
            "Epoch: 2/5; Step: 3330/3557; Loss: 0.5741206407546997\n",
            "Epoch: 2/5; Step: 3335/3557; Loss: 0.57581627368927\n",
            "Epoch: 2/5; Step: 3340/3557; Loss: 0.5538519620895386\n",
            "Epoch: 2/5; Step: 3345/3557; Loss: 0.5961035490036011\n",
            "Epoch: 2/5; Step: 3350/3557; Loss: 0.5975278615951538\n",
            "Epoch: 2/5; Step: 3355/3557; Loss: 0.5400026440620422\n",
            "Epoch: 2/5; Step: 3360/3557; Loss: 0.5683399438858032\n",
            "Epoch: 2/5; Step: 3365/3557; Loss: 0.5721305012702942\n",
            "Epoch: 2/5; Step: 3370/3557; Loss: 0.5843129754066467\n",
            "Epoch: 2/5; Step: 3375/3557; Loss: 0.561846911907196\n",
            "Epoch: 2/5; Step: 3380/3557; Loss: 0.6571232676506042\n",
            "Epoch: 2/5; Step: 3385/3557; Loss: 0.5657892823219299\n",
            "Epoch: 2/5; Step: 3390/3557; Loss: 0.5634500980377197\n",
            "Epoch: 2/5; Step: 3395/3557; Loss: 0.6317720413208008\n",
            "Epoch: 2/5; Step: 3400/3557; Loss: 0.5473979115486145\n",
            "Epoch: 2/5; Step: 3405/3557; Loss: 0.548881471157074\n",
            "Epoch: 2/5; Step: 3410/3557; Loss: 0.5669847726821899\n",
            "Epoch: 2/5; Step: 3415/3557; Loss: 0.6116120219230652\n",
            "Epoch: 2/5; Step: 3420/3557; Loss: 0.5258508920669556\n",
            "Epoch: 2/5; Step: 3425/3557; Loss: 0.593142032623291\n",
            "Epoch: 2/5; Step: 3430/3557; Loss: 0.590838611125946\n",
            "Epoch: 2/5; Step: 3435/3557; Loss: 0.556165337562561\n",
            "Epoch: 2/5; Step: 3440/3557; Loss: 0.5273441076278687\n",
            "Epoch: 2/5; Step: 3445/3557; Loss: 0.5825806260108948\n",
            "Epoch: 2/5; Step: 3450/3557; Loss: 0.6133300065994263\n",
            "Epoch: 2/5; Step: 3455/3557; Loss: 0.47899681329727173\n",
            "Epoch: 2/5; Step: 3460/3557; Loss: 0.5887179970741272\n",
            "Epoch: 2/5; Step: 3465/3557; Loss: 0.5422239303588867\n",
            "Epoch: 2/5; Step: 3470/3557; Loss: 0.6214292645454407\n",
            "Epoch: 2/5; Step: 3475/3557; Loss: 0.614965558052063\n",
            "Epoch: 2/5; Step: 3480/3557; Loss: 0.6438757181167603\n",
            "Epoch: 2/5; Step: 3485/3557; Loss: 0.5995758771896362\n",
            "Epoch: 2/5; Step: 3490/3557; Loss: 0.5594973564147949\n",
            "Epoch: 2/5; Step: 3495/3557; Loss: 0.5673617720603943\n",
            "Epoch: 2/5; Step: 3500/3557; Loss: 0.5085469484329224\n",
            "Epoch: 2/5; Step: 3505/3557; Loss: 0.5833684802055359\n",
            "Epoch: 2/5; Step: 3510/3557; Loss: 0.5899339318275452\n",
            "Epoch: 2/5; Step: 3515/3557; Loss: 0.4867701232433319\n",
            "Epoch: 2/5; Step: 3520/3557; Loss: 0.5669980049133301\n",
            "Epoch: 2/5; Step: 3525/3557; Loss: 0.4591490626335144\n",
            "Epoch: 2/5; Step: 3530/3557; Loss: 0.6396122574806213\n",
            "Epoch: 2/5; Step: 3535/3557; Loss: 0.598450779914856\n",
            "Epoch: 2/5; Step: 3540/3557; Loss: 0.5138724446296692\n",
            "Epoch: 2/5; Step: 3545/3557; Loss: 0.5130717158317566\n",
            "Epoch: 2/5; Step: 3550/3557; Loss: 0.536783754825592\n",
            "Epoch: 2/5; Step: 3555/3557; Loss: 0.578033447265625\n",
            "Epoch 2, Training Loss: 0.4598279297351837\n",
            "Epoch 2, Avg loss: 7.745205386844489, Generated Text: Jam jest Jacek Księga ósma Zajazd Astronomia Wojskiego - Uwaga Podkomorzego nad kometami - Tajemnicza scena w pokoju Sędziego - Tadeusz, chcąc zręcznie wyplątać się, wpada w wielkie kłopoty - Nowa Dydo - Tadeusz, chcąc zręcznie wyplątać się, wpada w wielkie kłopoty - Nowa Dydo - Zajazd - Ostatnia woźnieńska protestacja - Hrabia zdobywa Soplicowo - Zaścianek - Zakład - Dalej w grzyby! Zajazd - Ostatnia woźnieńska - Ostatnia woźnieńska protestacja - Ostatnia woźnieńska protestacja - Zaścianek szlachecki Dobrzyn - Ostatnia woźnieńska protestacja - Ostatnia woź\n",
            "Epoch: 3/5; Step: 0/3557; Loss: 0.4920870363712311\n",
            "Epoch: 3/5; Step: 5/3557; Loss: 0.5709393620491028\n",
            "Epoch: 3/5; Step: 10/3557; Loss: 0.5551660060882568\n",
            "Epoch: 3/5; Step: 15/3557; Loss: 0.5786303877830505\n",
            "Epoch: 3/5; Step: 20/3557; Loss: 0.561248242855072\n",
            "Epoch: 3/5; Step: 25/3557; Loss: 0.6560742855072021\n",
            "Epoch: 3/5; Step: 30/3557; Loss: 0.57976233959198\n",
            "Epoch: 3/5; Step: 35/3557; Loss: 0.5238407850265503\n",
            "Epoch: 3/5; Step: 40/3557; Loss: 0.5128158926963806\n",
            "Epoch: 3/5; Step: 45/3557; Loss: 0.5798744559288025\n",
            "Epoch: 3/5; Step: 50/3557; Loss: 0.5405870676040649\n",
            "Epoch: 3/5; Step: 55/3557; Loss: 0.5446361899375916\n",
            "Epoch: 3/5; Step: 60/3557; Loss: 0.6323108077049255\n",
            "Epoch: 3/5; Step: 65/3557; Loss: 0.6333433389663696\n",
            "Epoch: 3/5; Step: 70/3557; Loss: 0.4650722146034241\n",
            "Epoch: 3/5; Step: 75/3557; Loss: 0.46333733201026917\n",
            "Epoch: 3/5; Step: 80/3557; Loss: 0.5196187496185303\n",
            "Epoch: 3/5; Step: 85/3557; Loss: 0.5409767627716064\n",
            "Epoch: 3/5; Step: 90/3557; Loss: 0.5439354777336121\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-e48f21325b6e>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model.train()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "epoch_num = 1\n",
        "for epoch in range(epoch_num):\n",
        "    # Trening\n",
        "    model.train()\n",
        "    for i, (input_ids, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = input_ids.to(\"cuda\")\n",
        "        labels = labels.to(\"cuda\")\n",
        "\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 5 == 0:\n",
        "            print(f\"Epoch: {epoch + 1}/{epoch_num}; Step: {i}/{len(train_loader)}; Loss: {loss.item()}\")\n",
        "            pass\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Training Loss: {loss.item()}\")\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    num_batches = 0\n",
        "    num_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_ids, labels in valid_loader:\n",
        "            input_ids = input_ids.to(\"cuda\")\n",
        "            labels = labels.to(\"cuda\")\n",
        "\n",
        "            outputs = model(input_ids, labels=labels)\n",
        "            val_loss += outputs.loss.item() * input_ids.size(0)  # sum the loss for each batch\n",
        "            num_samples += input_ids.size(0)\n",
        "            num_batches += 1\n",
        "\n",
        "    avg_val_loss = val_loss / num_samples\n",
        "\n",
        "    # Generowanie przykładowego tekstu\n",
        "    input_ids = tokenizer.encode(init_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    temperature = 1\n",
        "    top_p = 0.1\n",
        "    generated_ids = model.generate(\n",
        "        input_ids,\n",
        "        max_length=150,\n",
        "        num_return_sequences=1,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        do_sample=True,\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    print(f\"Epoch {epoch + 1}, Avg loss: {avg_val_loss}, Generated Text: {generated_text}\")\n",
        "\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# Wydrukowanie kształtu tensorów wejściowych\n",
        "ids = torch.tensor(tokenized_datasets[\"train\"][\"input_ids\"])\n",
        "print(ids.shape)  # torch.Size([8960, 137])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}