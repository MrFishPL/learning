{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2tRZ2eUH80A"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Custom Dataset class\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Define LSTM model\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, bert, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.lstm = nn.LSTM(bert.config.hidden_size, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        with torch.no_grad():\n",
        "            embedded = self.bert(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
        "        _, (hidden, _) = self.lstm(embedded)\n",
        "        if self.lstm.bidirectional:\n",
        "            hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
        "        else:\n",
        "            hidden = self.dropout(hidden[-1, :, :])\n",
        "        output = self.fc(hidden)\n",
        "        return output\n",
        "\n",
        "# Load data\n",
        "positive_reviews = open('TrainingDataPositive.txt', 'r').readlines()\n",
        "negative_reviews = open('TrainingDataNegative.txt', 'r').readlines()\n",
        "\n",
        "texts = positive_reviews + negative_reviews\n",
        "labels = [1] * len(positive_reviews) + [0] * len(negative_reviews)\n",
        "\n",
        "# Parameters\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 4\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 2\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.3\n",
        "\n",
        "# Tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "dataset = SentimentDataset(texts, labels, tokenizer, MAX_LEN)\n",
        "\n",
        "train_size = int(1 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Initialize model, loss function, optimizer\n",
        "model = SentimentClassifier(bert_model, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Training loop\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print(f'Epoch {epoch + 1}/{EPOCHS}, Batch {batch_idx + 1}/{len(train_loader)}, Loss: {loss.item()}')\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f'Epoch {epoch + 1}/{EPOCHS}, Average Loss: {avg_loss}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1ZzJParIl_V",
        "outputId": "2e965643-d85d-4815-ec0a-60732ed41953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4, Batch 10/703, Loss: 0.6691676378250122\n",
            "Epoch 1/4, Batch 20/703, Loss: 0.6562215089797974\n",
            "Epoch 1/4, Batch 30/703, Loss: 0.6106436848640442\n",
            "Epoch 1/4, Batch 40/703, Loss: 0.5863316059112549\n",
            "Epoch 1/4, Batch 50/703, Loss: 0.6195782423019409\n",
            "Epoch 1/4, Batch 60/703, Loss: 0.54873126745224\n",
            "Epoch 1/4, Batch 70/703, Loss: 0.5734463930130005\n",
            "Epoch 1/4, Batch 80/703, Loss: 0.5607689023017883\n",
            "Epoch 1/4, Batch 90/703, Loss: 0.5474369525909424\n",
            "Epoch 1/4, Batch 100/703, Loss: 0.5574567317962646\n",
            "Epoch 1/4, Batch 110/703, Loss: 0.5955451726913452\n",
            "Epoch 1/4, Batch 120/703, Loss: 0.40213385224342346\n",
            "Epoch 1/4, Batch 130/703, Loss: 0.41625481843948364\n",
            "Epoch 1/4, Batch 140/703, Loss: 0.6221076846122742\n",
            "Epoch 1/4, Batch 150/703, Loss: 0.5139080286026001\n",
            "Epoch 1/4, Batch 160/703, Loss: 0.6183882355690002\n",
            "Epoch 1/4, Batch 170/703, Loss: 0.5331366658210754\n",
            "Epoch 1/4, Batch 180/703, Loss: 0.41678279638290405\n",
            "Epoch 1/4, Batch 190/703, Loss: 0.4235105812549591\n",
            "Epoch 1/4, Batch 200/703, Loss: 0.5892763733863831\n",
            "Epoch 1/4, Batch 210/703, Loss: 0.4639509916305542\n",
            "Epoch 1/4, Batch 220/703, Loss: 0.4772390127182007\n",
            "Epoch 1/4, Batch 230/703, Loss: 0.6766640543937683\n",
            "Epoch 1/4, Batch 240/703, Loss: 0.5076009035110474\n",
            "Epoch 1/4, Batch 250/703, Loss: 0.6511781215667725\n",
            "Epoch 1/4, Batch 260/703, Loss: 0.5200405120849609\n",
            "Epoch 1/4, Batch 270/703, Loss: 0.4129810035228729\n",
            "Epoch 1/4, Batch 280/703, Loss: 0.3603610396385193\n",
            "Epoch 1/4, Batch 290/703, Loss: 0.34788304567337036\n",
            "Epoch 1/4, Batch 300/703, Loss: 0.10670747607946396\n",
            "Epoch 1/4, Batch 310/703, Loss: 0.22478365898132324\n",
            "Epoch 1/4, Batch 320/703, Loss: 0.41484710574150085\n",
            "Epoch 1/4, Batch 330/703, Loss: 0.16169118881225586\n",
            "Epoch 1/4, Batch 340/703, Loss: 0.2613716125488281\n",
            "Epoch 1/4, Batch 350/703, Loss: 0.27325373888015747\n",
            "Epoch 1/4, Batch 360/703, Loss: 0.464774489402771\n",
            "Epoch 1/4, Batch 370/703, Loss: 0.2135184407234192\n",
            "Epoch 1/4, Batch 380/703, Loss: 0.1119740903377533\n",
            "Epoch 1/4, Batch 390/703, Loss: 0.3229340612888336\n",
            "Epoch 1/4, Batch 400/703, Loss: 0.19778400659561157\n",
            "Epoch 1/4, Batch 410/703, Loss: 0.28948476910591125\n",
            "Epoch 1/4, Batch 420/703, Loss: 0.07856563478708267\n",
            "Epoch 1/4, Batch 430/703, Loss: 0.16746146976947784\n",
            "Epoch 1/4, Batch 440/703, Loss: 0.33846089243888855\n",
            "Epoch 1/4, Batch 450/703, Loss: 0.4677247703075409\n",
            "Epoch 1/4, Batch 460/703, Loss: 0.17345714569091797\n",
            "Epoch 1/4, Batch 470/703, Loss: 0.20246925950050354\n",
            "Epoch 1/4, Batch 480/703, Loss: 0.10429937392473221\n",
            "Epoch 1/4, Batch 490/703, Loss: 0.4445314407348633\n",
            "Epoch 1/4, Batch 500/703, Loss: 0.1479489803314209\n",
            "Epoch 1/4, Batch 510/703, Loss: 0.38598325848579407\n",
            "Epoch 1/4, Batch 520/703, Loss: 0.21266525983810425\n",
            "Epoch 1/4, Batch 530/703, Loss: 0.42015984654426575\n",
            "Epoch 1/4, Batch 540/703, Loss: 0.2541813254356384\n",
            "Epoch 1/4, Batch 550/703, Loss: 0.18949323892593384\n",
            "Epoch 1/4, Batch 560/703, Loss: 0.17437323927879333\n",
            "Epoch 1/4, Batch 570/703, Loss: 0.34141799807548523\n",
            "Epoch 1/4, Batch 580/703, Loss: 0.13154256343841553\n",
            "Epoch 1/4, Batch 590/703, Loss: 0.32564347982406616\n",
            "Epoch 1/4, Batch 600/703, Loss: 0.6342440843582153\n",
            "Epoch 1/4, Batch 610/703, Loss: 0.14326900243759155\n",
            "Epoch 1/4, Batch 620/703, Loss: 0.06161350756883621\n",
            "Epoch 1/4, Batch 630/703, Loss: 0.2990487217903137\n",
            "Epoch 1/4, Batch 640/703, Loss: 0.04374822601675987\n",
            "Epoch 1/4, Batch 650/703, Loss: 0.06453344225883484\n",
            "Epoch 1/4, Batch 660/703, Loss: 0.24099299311637878\n",
            "Epoch 1/4, Batch 670/703, Loss: 0.14965899288654327\n",
            "Epoch 1/4, Batch 680/703, Loss: 0.23069512844085693\n",
            "Epoch 1/4, Batch 690/703, Loss: 0.18149694800376892\n",
            "Epoch 1/4, Batch 700/703, Loss: 0.31739673018455505\n",
            "Epoch 1/4, Average Loss: 0.3622934279266538\n",
            "Epoch 2/4, Batch 10/703, Loss: 0.07011138647794724\n",
            "Epoch 2/4, Batch 20/703, Loss: 0.2632433772087097\n",
            "Epoch 2/4, Batch 30/703, Loss: 0.25003862380981445\n",
            "Epoch 2/4, Batch 40/703, Loss: 0.10206419974565506\n",
            "Epoch 2/4, Batch 50/703, Loss: 0.04815671592950821\n",
            "Epoch 2/4, Batch 60/703, Loss: 0.5134328007698059\n",
            "Epoch 2/4, Batch 70/703, Loss: 0.23025697469711304\n",
            "Epoch 2/4, Batch 80/703, Loss: 0.37008136510849\n",
            "Epoch 2/4, Batch 90/703, Loss: 0.14411836862564087\n",
            "Epoch 2/4, Batch 100/703, Loss: 0.15134866535663605\n",
            "Epoch 2/4, Batch 110/703, Loss: 0.15138916671276093\n",
            "Epoch 2/4, Batch 120/703, Loss: 0.1368558257818222\n",
            "Epoch 2/4, Batch 130/703, Loss: 0.047642119228839874\n",
            "Epoch 2/4, Batch 140/703, Loss: 0.08792197704315186\n",
            "Epoch 2/4, Batch 150/703, Loss: 0.07446151971817017\n",
            "Epoch 2/4, Batch 160/703, Loss: 0.046323511749506\n",
            "Epoch 2/4, Batch 170/703, Loss: 0.3238888084888458\n",
            "Epoch 2/4, Batch 180/703, Loss: 0.10385383665561676\n",
            "Epoch 2/4, Batch 190/703, Loss: 0.3035467267036438\n",
            "Epoch 2/4, Batch 200/703, Loss: 0.36421602964401245\n",
            "Epoch 2/4, Batch 210/703, Loss: 0.3405384421348572\n",
            "Epoch 2/4, Batch 220/703, Loss: 0.1734427660703659\n",
            "Epoch 2/4, Batch 230/703, Loss: 0.15509209036827087\n",
            "Epoch 2/4, Batch 240/703, Loss: 0.43730342388153076\n",
            "Epoch 2/4, Batch 250/703, Loss: 0.1800936907529831\n",
            "Epoch 2/4, Batch 260/703, Loss: 0.028839221224188805\n",
            "Epoch 2/4, Batch 280/703, Loss: 0.3311779797077179\n",
            "Epoch 2/4, Batch 290/703, Loss: 0.31011396646499634\n",
            "Epoch 2/4, Batch 300/703, Loss: 0.13998597860336304\n",
            "Epoch 2/4, Batch 310/703, Loss: 0.07446814328432083\n",
            "Epoch 2/4, Batch 320/703, Loss: 0.4396086633205414\n",
            "Epoch 2/4, Batch 330/703, Loss: 0.18234777450561523\n",
            "Epoch 2/4, Batch 340/703, Loss: 0.06708680838346481\n",
            "Epoch 2/4, Batch 350/703, Loss: 0.03904237970709801\n",
            "Epoch 2/4, Batch 360/703, Loss: 0.36315101385116577\n",
            "Epoch 2/4, Batch 370/703, Loss: 0.3522714078426361\n",
            "Epoch 2/4, Batch 380/703, Loss: 0.03791549801826477\n",
            "Epoch 2/4, Batch 390/703, Loss: 0.20239004492759705\n",
            "Epoch 2/4, Batch 400/703, Loss: 0.31951215863227844\n",
            "Epoch 2/4, Batch 410/703, Loss: 0.1251823902130127\n",
            "Epoch 2/4, Batch 420/703, Loss: 0.02562928944826126\n",
            "Epoch 2/4, Batch 430/703, Loss: 0.3912006616592407\n",
            "Epoch 2/4, Batch 440/703, Loss: 0.18317566812038422\n",
            "Epoch 2/4, Batch 450/703, Loss: 0.28030604124069214\n",
            "Epoch 2/4, Batch 460/703, Loss: 0.0568581148982048\n",
            "Epoch 2/4, Batch 470/703, Loss: 0.10838354378938675\n",
            "Epoch 2/4, Batch 480/703, Loss: 0.09767244011163712\n",
            "Epoch 2/4, Batch 490/703, Loss: 0.23169168829917908\n",
            "Epoch 2/4, Batch 500/703, Loss: 0.06738625466823578\n",
            "Epoch 2/4, Batch 510/703, Loss: 0.1547158658504486\n",
            "Epoch 2/4, Batch 520/703, Loss: 0.2478671669960022\n",
            "Epoch 2/4, Batch 530/703, Loss: 0.09616309404373169\n",
            "Epoch 2/4, Batch 540/703, Loss: 0.03238542377948761\n",
            "Epoch 2/4, Batch 550/703, Loss: 0.2345706969499588\n",
            "Epoch 2/4, Batch 560/703, Loss: 0.15877018868923187\n",
            "Epoch 2/4, Batch 570/703, Loss: 0.017456283792853355\n",
            "Epoch 2/4, Batch 580/703, Loss: 0.20200829207897186\n",
            "Epoch 2/4, Batch 590/703, Loss: 0.22463993728160858\n",
            "Epoch 2/4, Batch 600/703, Loss: 0.3772408366203308\n",
            "Epoch 2/4, Batch 610/703, Loss: 0.12485016882419586\n",
            "Epoch 2/4, Batch 620/703, Loss: 0.0244604405015707\n",
            "Epoch 2/4, Batch 630/703, Loss: 0.05721784755587578\n",
            "Epoch 2/4, Batch 640/703, Loss: 0.18010249733924866\n",
            "Epoch 2/4, Batch 650/703, Loss: 0.049520544707775116\n",
            "Epoch 2/4, Batch 660/703, Loss: 0.06189259886741638\n",
            "Epoch 2/4, Batch 670/703, Loss: 0.2254088819026947\n",
            "Epoch 2/4, Batch 680/703, Loss: 0.18112242221832275\n",
            "Epoch 2/4, Batch 690/703, Loss: 0.1637764722108841\n",
            "Epoch 2/4, Batch 700/703, Loss: 0.6108468770980835\n",
            "Epoch 2/4, Average Loss: 0.18644935745284874\n",
            "Epoch 3/4, Batch 10/703, Loss: 0.15109510719776154\n",
            "Epoch 3/4, Batch 20/703, Loss: 0.11519831418991089\n",
            "Epoch 3/4, Batch 30/703, Loss: 0.4071587920188904\n",
            "Epoch 3/4, Batch 40/703, Loss: 0.15759800374507904\n",
            "Epoch 3/4, Batch 50/703, Loss: 0.1632416993379593\n",
            "Epoch 3/4, Batch 60/703, Loss: 0.10932694375514984\n",
            "Epoch 3/4, Batch 70/703, Loss: 0.13540227711200714\n",
            "Epoch 3/4, Batch 80/703, Loss: 0.1075545996427536\n",
            "Epoch 3/4, Batch 90/703, Loss: 0.12117403000593185\n",
            "Epoch 3/4, Batch 100/703, Loss: 0.056898340582847595\n",
            "Epoch 3/4, Batch 110/703, Loss: 0.27160510420799255\n",
            "Epoch 3/4, Batch 120/703, Loss: 0.2500477731227875\n",
            "Epoch 3/4, Batch 130/703, Loss: 0.212544247508049\n",
            "Epoch 3/4, Batch 140/703, Loss: 0.31931695342063904\n",
            "Epoch 3/4, Batch 150/703, Loss: 0.19622944295406342\n",
            "Epoch 3/4, Batch 160/703, Loss: 0.06955110281705856\n",
            "Epoch 3/4, Batch 170/703, Loss: 0.09751500189304352\n",
            "Epoch 3/4, Batch 180/703, Loss: 0.16416721045970917\n",
            "Epoch 3/4, Batch 190/703, Loss: 0.0348927341401577\n",
            "Epoch 3/4, Batch 200/703, Loss: 0.014488395303487778\n",
            "Epoch 3/4, Batch 210/703, Loss: 0.3542446196079254\n",
            "Epoch 3/4, Batch 220/703, Loss: 0.07486473768949509\n",
            "Epoch 3/4, Batch 230/703, Loss: 0.5149247646331787\n",
            "Epoch 3/4, Batch 240/703, Loss: 0.048126623034477234\n",
            "Epoch 3/4, Batch 250/703, Loss: 0.25780314207077026\n",
            "Epoch 3/4, Batch 260/703, Loss: 0.08360853791236877\n",
            "Epoch 3/4, Batch 270/703, Loss: 0.2722882926464081\n",
            "Epoch 3/4, Batch 280/703, Loss: 0.339814692735672\n",
            "Epoch 3/4, Batch 290/703, Loss: 0.21590609848499298\n",
            "Epoch 3/4, Batch 300/703, Loss: 0.5349071621894836\n",
            "Epoch 3/4, Batch 310/703, Loss: 0.03011631965637207\n",
            "Epoch 3/4, Batch 320/703, Loss: 0.37821847200393677\n",
            "Epoch 3/4, Batch 330/703, Loss: 0.03960207849740982\n",
            "Epoch 3/4, Batch 340/703, Loss: 0.1360560804605484\n",
            "Epoch 3/4, Batch 350/703, Loss: 0.14690141379833221\n",
            "Epoch 3/4, Batch 360/703, Loss: 0.032077185809612274\n",
            "Epoch 3/4, Batch 370/703, Loss: 0.14730873703956604\n",
            "Epoch 3/4, Batch 380/703, Loss: 0.03267132118344307\n",
            "Epoch 3/4, Batch 390/703, Loss: 0.137025386095047\n",
            "Epoch 3/4, Batch 400/703, Loss: 0.019098252058029175\n",
            "Epoch 3/4, Batch 410/703, Loss: 0.059412773698568344\n",
            "Epoch 3/4, Batch 420/703, Loss: 0.06457435339689255\n",
            "Epoch 3/4, Batch 430/703, Loss: 0.20046892762184143\n",
            "Epoch 3/4, Batch 440/703, Loss: 0.1775171458721161\n",
            "Epoch 3/4, Batch 450/703, Loss: 0.1982015073299408\n",
            "Epoch 3/4, Batch 460/703, Loss: 0.1352314054965973\n",
            "Epoch 3/4, Batch 470/703, Loss: 0.0897625982761383\n",
            "Epoch 3/4, Batch 480/703, Loss: 0.04508301615715027\n",
            "Epoch 3/4, Batch 490/703, Loss: 0.11131514608860016\n",
            "Epoch 3/4, Batch 500/703, Loss: 0.08165385574102402\n",
            "Epoch 3/4, Batch 510/703, Loss: 0.08794116228818893\n",
            "Epoch 3/4, Batch 520/703, Loss: 0.08707185089588165\n",
            "Epoch 3/4, Batch 530/703, Loss: 0.10689957439899445\n",
            "Epoch 3/4, Batch 540/703, Loss: 0.07419510930776596\n",
            "Epoch 3/4, Batch 550/703, Loss: 0.4089461863040924\n",
            "Epoch 3/4, Batch 560/703, Loss: 0.1313646286725998\n",
            "Epoch 3/4, Batch 570/703, Loss: 0.03994964063167572\n",
            "Epoch 3/4, Batch 580/703, Loss: 0.0976480022072792\n",
            "Epoch 3/4, Batch 590/703, Loss: 0.17597465217113495\n",
            "Epoch 3/4, Batch 600/703, Loss: 0.243540421128273\n",
            "Epoch 3/4, Batch 610/703, Loss: 0.2686817944049835\n",
            "Epoch 3/4, Batch 620/703, Loss: 0.07055801153182983\n",
            "Epoch 3/4, Batch 630/703, Loss: 0.16305188834667206\n",
            "Epoch 3/4, Batch 640/703, Loss: 0.2424675077199936\n",
            "Epoch 3/4, Batch 650/703, Loss: 0.25660276412963867\n",
            "Epoch 3/4, Batch 660/703, Loss: 0.15377454459667206\n",
            "Epoch 3/4, Batch 670/703, Loss: 0.061396390199661255\n",
            "Epoch 3/4, Batch 680/703, Loss: 0.11322088539600372\n",
            "Epoch 3/4, Batch 690/703, Loss: 0.11560190469026566\n",
            "Epoch 3/4, Batch 700/703, Loss: 0.0779477059841156\n",
            "Epoch 3/4, Average Loss: 0.16443797580733535\n",
            "Epoch 4/4, Batch 10/703, Loss: 0.07948081195354462\n",
            "Epoch 4/4, Batch 20/703, Loss: 0.03319275379180908\n",
            "Epoch 4/4, Batch 30/703, Loss: 0.09055005013942719\n",
            "Epoch 4/4, Batch 40/703, Loss: 0.2587193250656128\n",
            "Epoch 4/4, Batch 50/703, Loss: 0.07039880007505417\n",
            "Epoch 4/4, Batch 60/703, Loss: 0.17688320577144623\n",
            "Epoch 4/4, Batch 70/703, Loss: 0.25162413716316223\n",
            "Epoch 4/4, Batch 80/703, Loss: 0.0608694851398468\n",
            "Epoch 4/4, Batch 90/703, Loss: 0.0718226507306099\n",
            "Epoch 4/4, Batch 100/703, Loss: 0.045344360172748566\n",
            "Epoch 4/4, Batch 110/703, Loss: 0.20145422220230103\n",
            "Epoch 4/4, Batch 120/703, Loss: 0.07756400853395462\n",
            "Epoch 4/4, Batch 130/703, Loss: 0.16048626601696014\n",
            "Epoch 4/4, Batch 140/703, Loss: 0.01904251240193844\n",
            "Epoch 4/4, Batch 150/703, Loss: 0.06096670776605606\n",
            "Epoch 4/4, Batch 160/703, Loss: 0.07698183506727219\n",
            "Epoch 4/4, Batch 170/703, Loss: 0.07432830333709717\n",
            "Epoch 4/4, Batch 180/703, Loss: 0.36478403210639954\n",
            "Epoch 4/4, Batch 190/703, Loss: 0.05822496861219406\n",
            "Epoch 4/4, Batch 200/703, Loss: 0.13307146728038788\n",
            "Epoch 4/4, Batch 210/703, Loss: 0.13738130033016205\n",
            "Epoch 4/4, Batch 220/703, Loss: 0.03798554092645645\n",
            "Epoch 4/4, Batch 230/703, Loss: 0.24975153803825378\n",
            "Epoch 4/4, Batch 240/703, Loss: 0.2272671014070511\n",
            "Epoch 4/4, Batch 250/703, Loss: 0.12248875200748444\n",
            "Epoch 4/4, Batch 260/703, Loss: 0.09832330048084259\n",
            "Epoch 4/4, Batch 270/703, Loss: 0.3703632354736328\n",
            "Epoch 4/4, Batch 280/703, Loss: 0.137882262468338\n",
            "Epoch 4/4, Batch 290/703, Loss: 0.23736612498760223\n",
            "Epoch 4/4, Batch 300/703, Loss: 0.08731577545404434\n",
            "Epoch 4/4, Batch 310/703, Loss: 0.02151065319776535\n",
            "Epoch 4/4, Batch 320/703, Loss: 0.019739000126719475\n",
            "Epoch 4/4, Batch 330/703, Loss: 0.5178260803222656\n",
            "Epoch 4/4, Batch 340/703, Loss: 0.4752279222011566\n",
            "Epoch 4/4, Batch 350/703, Loss: 0.18243594467639923\n",
            "Epoch 4/4, Batch 360/703, Loss: 0.2381824553012848\n",
            "Epoch 4/4, Batch 370/703, Loss: 0.13552546501159668\n",
            "Epoch 4/4, Batch 380/703, Loss: 0.04755636677145958\n",
            "Epoch 4/4, Batch 390/703, Loss: 0.10898184776306152\n",
            "Epoch 4/4, Batch 400/703, Loss: 0.1346261352300644\n",
            "Epoch 4/4, Batch 410/703, Loss: 0.3127613663673401\n",
            "Epoch 4/4, Batch 420/703, Loss: 0.1820174753665924\n",
            "Epoch 4/4, Batch 430/703, Loss: 0.07038375735282898\n",
            "Epoch 4/4, Batch 440/703, Loss: 0.11612311750650406\n",
            "Epoch 4/4, Batch 450/703, Loss: 0.07065342366695404\n",
            "Epoch 4/4, Batch 460/703, Loss: 0.18674236536026\n",
            "Epoch 4/4, Batch 470/703, Loss: 0.29625266790390015\n",
            "Epoch 4/4, Batch 480/703, Loss: 0.27297890186309814\n",
            "Epoch 4/4, Batch 490/703, Loss: 0.03723079338669777\n",
            "Epoch 4/4, Batch 500/703, Loss: 0.22783014178276062\n",
            "Epoch 4/4, Batch 510/703, Loss: 0.03656802698969841\n",
            "Epoch 4/4, Batch 520/703, Loss: 0.14928938448429108\n",
            "Epoch 4/4, Batch 530/703, Loss: 0.3653435707092285\n",
            "Epoch 4/4, Batch 540/703, Loss: 0.6096981763839722\n",
            "Epoch 4/4, Batch 550/703, Loss: 0.06943441927433014\n",
            "Epoch 4/4, Batch 560/703, Loss: 0.06294599175453186\n",
            "Epoch 4/4, Batch 570/703, Loss: 0.1598624289035797\n",
            "Epoch 4/4, Batch 580/703, Loss: 0.09630492329597473\n",
            "Epoch 4/4, Batch 590/703, Loss: 0.03420059755444527\n",
            "Epoch 4/4, Batch 600/703, Loss: 0.04939093813300133\n",
            "Epoch 4/4, Batch 610/703, Loss: 0.27926871180534363\n",
            "Epoch 4/4, Batch 620/703, Loss: 0.19226567447185516\n",
            "Epoch 4/4, Batch 630/703, Loss: 0.053170956671237946\n",
            "Epoch 4/4, Batch 640/703, Loss: 0.16388951241970062\n",
            "Epoch 4/4, Batch 650/703, Loss: 0.2411504089832306\n",
            "Epoch 4/4, Batch 660/703, Loss: 0.12602151930332184\n",
            "Epoch 4/4, Batch 670/703, Loss: 0.04591439664363861\n",
            "Epoch 4/4, Batch 680/703, Loss: 0.12851029634475708\n",
            "Epoch 4/4, Batch 690/703, Loss: 0.12098938226699829\n",
            "Epoch 4/4, Batch 700/703, Loss: 0.13226318359375\n",
            "Epoch 4/4, Average Loss: 0.15318678462133342\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluation on test set\n",
        "test_df = pd.read_csv('TestReviews.csv')\n",
        "test_texts = test_df['review'].tolist()\n",
        "test_labels = test_df['class'].tolist()\n",
        "\n",
        "test_dataset = SentimentDataset(test_texts, test_labels, tokenizer, MAX_LEN)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        correct += torch.sum(preds == labels)\n",
        "        total += labels.size(0)\n",
        "\n",
        "accuracy = correct.double() / total\n",
        "print(f'Test Accuracy: {accuracy.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RA74ttTXIocd",
        "outputId": "4fa9a69e-b1f6-445b-ca08-df6c4366e6bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9409858828974774\n"
          ]
        }
      ]
    }
  ]
}